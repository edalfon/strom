---
title: "Gradient Boost"
execute:
  echo: false
  eval: true
---

```{python}
import pandas as pd
import numpy as np
import seaborn as sns
import epyfun

import strom

strom_climate = strom.read_result("merge_strom_climate_data")
strom_climate["ds"] = strom_climate["date"].dt.date
strom_climate = strom_climate.set_index("ds")

cutoff = strom_climate["date"].max() - pd.DateOffset(years=1)

X = strom_climate.drop(columns="wd")
y = strom_climate["wd"]

train_set = strom_climate[strom_climate["date"] <= cutoff]
test_set = strom_climate[strom_climate["date"] > cutoff]

X_train = train_set.drop(columns="wd")
y_train = train_set["wd"]
X_test = test_set.drop(columns="wd")
y_test = test_set["wd"]
```

Before moving forward with the to-do list, let's throw a Random Forest to it.

# Gradient boost

For many reasons, Random Forest is usually a very good baseline model. In this particular
case I started with the polynomial OLS as baseline model, just because it was so evident
from the correlations that the relationship between temperature and consumption
follows a polynomial shape. But let's go back to a beloved RF.

```{python}
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import LinearSVR

from sklego.preprocessing import ColumnSelector

# import unifiedbooster as ub
# regressor1 = ub.GBDTRegressor(model_type='xgboost')

vars = ["tt_tu_mean", "rf_tu_mean", "td_mean", "vp_std_mean", "tf_std_mean"]
pipe = Pipeline(
    [
        ("vars", ColumnSelector(columns=vars)),
        ("model", GradientBoostingRegressor(random_state=7)),
    ]
)
pipe.fit(X_train, y_train)
strom.log_vetiver(
    pipe,
    "wd-gb",
    description="Gradient Boost",
)
```

{{< include _assess_model.qmd >}}

Again, overfits a lot.




```{python}
# | include: false
from sklearn.model_selection import GridSearchCV

from itertools import combinations

vars = [
    "tt_tu_mean",
    "rf_tu_mean",
    "td_mean",
    "vp_std_mean",
    "tf_std_mean",
    # "p_std_mean",
]
combs = [
    list(combo) for r in range(1, len(vars) + 1) for combo in combinations(vars, r)
]
param_grid = [
    {
        "vars__columns": combs,  # combs, vars, ["tt_tu_mean"]
        "model__learning_rate": [0.1],  # [0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3],
        "model__n_estimators": [60],  # range(10, 200, 10),
        "model__max_depth": [5],  # range(1, 7, 1),
        "model__min_samples_split": [48],  # [48],  # range(2, 50, 2),
        "model__min_samples_leaf": [5],  # [5],  # range(1, 30, 1),
        # "model__max_features": range(7, 20, 2),
        "model__subsample": [1],  # [0.6, 0.7, 0.75, 0.8, 0.85, 0.9, 1],
    },
]

grid = GridSearchCV(
    pipe,
    param_grid=param_grid,
    scoring=strom.get_scoring(),
    n_jobs=-1,
    cv=strom.get_cv(),
    refit="RMSE - Root Mean Squared Error",
    return_train_score=True,
)
grid = grid.fit(X_train, y_train)

pipe = grid.best_estimator_

strom.log_vetiver(pipe, "wd-gb", description="Gradient Boost-tuned")
# strom.reload_all()

grid_plots = strom.plot_grid_search_results(
    grid, "param_model__min_samples_leaf", False
)
fig = grid_plots["MAE - Mean Absolute Error"].show()
fig = grid_plots["RMSE - Root Mean Squared Error"].show()

# grid_plots = strom.plot_grid_search_results(grid)
# fig = grid_plots["MAE - Mean Absolute Error"].show()
# fig = grid_plots["RMSE - Root Mean Squared Error"].show()
# grid.best_params_
```

```{python}
# | output: asis
cols = pd.DataFrame(grid.cv_results_).filter(like="param_").columns.to_list()
for col in cols:
    print(f"## Parameter: {col}")
    grid_plots = strom.plot_grid_search_results(grid, col, False)
    print("::: {.panel-tabset}")
    for key in grid_plots:
        print(f"####### {key.split('-', 1)[0].strip()}")
        grid_plots[key].show()
    print(":::")
```


## Best model
```{python}
grid.best_params_
```
```{python}
pipe
```

{{< include _assess_model.qmd >}}


# Compare vanilla vs. tuned

```{python}
import strom

versions = strom.get_board().pin_versions("wd-gb")
vetivers = [strom.get_vetiver("wd-gb", version=v) for v in versions.version]
```

{{< include _compare_models.qmd >}}

# TODOs


