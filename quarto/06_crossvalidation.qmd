---
title: "Cross-validation"
execute:
  echo: false
  eval: false
---

```{python}
import pandas as pd
import numpy as np
import epyfun
import strom

import strom
strom_climate = strom.read_result("merge_strom_climate_data")
#strom_climate = pd.read_parquet("interim/strom_climate.parquet")
X = strom_climate.drop(columns="wd")
y = strom_climate["wd"]
```

# scikit-learn pipeline

Let's run the models using a scikit-learn pipeline, so we can bundle variable selection, 
transformations and model estimation in one pipeline, that will be stored in a pin as
a vetiver model

```{python}
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score

from sklego.preprocessing import ColumnSelector

wd_pipeline = Pipeline([
    ("vars", ColumnSelector(columns=['tt_tu_mean', 'rf_tu_mean'])),
    ('polynomial', PolynomialFeatures(degree=3)),
    ('model', LinearRegression())
])

strom.log_vetiver(wd_pipeline, "waermestrom")

# Train the model on the training data
wd_pipeline.fit(X, y)

# Make predictions on the test data
y_pred = wd_pipeline.predict(X)

# Evaluate the model's performance on the test data
mse = r2_score(y, y_pred)
print("R2:", mse)
```


# cross validate, the bundled model in the pipeline

https://scikit-learn.org/stable/modules/cross_validation.show_html

using cross_val_score, that only allows you to get one scoring metric.

Make sure to use shuffle=True, otherwise, since the data are ordered by date and 
we have only one year and it has a strong seasonal pattern, you would end up basically
we a bunch of very bad models, because january is a very bad data to predict july's 
energy consumption

```{python}
#| include: false
from sklearn.model_selection import cross_val_score

from sklearn.model_selection import TimeSeriesSplit
tscv = TimeSeriesSplit(n_splits=5)

from sklearn.model_selection import RepeatedKFold
rkf = RepeatedKFold(n_splits=10, n_repeats=100, random_state=7)

from sklearn.model_selection import KFold
kf = KFold(n_splits=10, shuffle=True)

scores = cross_val_score(
    wd_pipeline, X, y, scoring="explained_variance", cv=rkf, n_jobs=-1, verbose=1
)
scores
```

and using cross_validate that receives a bunch of scoring metrics

```{python}
#| include: false
from sklearn.model_selection import cross_validate

scoring = ["r2", "explained_variance", "neg_median_absolute_error", "neg_root_mean_squared_error", "neg_mean_absolute_error"]

scores = cross_validate(
    wd_pipeline, X, y, scoring=scoring, cv=rkf, n_jobs=-1, verbose=0, return_train_score=True
)
scores["test_r2"]
```


## Plotting Cross-Validated predictions

https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_predict.html#sphx-glr-auto-examples-model-selection-plot-cv-predict-py

basically use cross_val_predict to lump together the predictions of all different models fitted in for the cross-validation

```{python}
from sklearn.model_selection import cross_val_predict

y_pred = cross_val_predict(wd_pipeline, X, y, cv=kf)
```


```{python}
from sklearn.metrics import PredictionErrorDisplay
import matplotlib.pyplot as plt

fig, axs = plt.subplots(ncols=2, figsize=(8, 4))
PredictionErrorDisplay.from_predictions(
    y,
    y_pred=y_pred,
    kind="actual_vs_predicted",
    #subsample=100,
    ax=axs[0],
    random_state=0,
)
axs[0].set_title("Actual vs. Predicted values")
PredictionErrorDisplay.from_predictions(
    y,
    y_pred=y_pred,
    kind="residual_vs_predicted",
    #subsample=100,
    ax=axs[1],
    random_state=0,
)
axs[1].set_title("Residuals vs. Predicted Values")
fig.suptitle("Plotting cross-validated predictions")
plt.tight_layout()
plt.show()
```



```{python}
#| eval: false
import vetiver
import pins

waermestrom_versions = strom.get_board().pin_versions("waermestrom")

scores = pd.DataFrame()
for index, row in waermestrom_versions.iterrows():
    v = strom.get_vetiver("waermestrom", version=row["version"])
    scores_i = cross_validate(
        v.model,
        X,
        y,
        scoring=scoring,
        cv=kf,
        n_jobs=-1,
        verbose=0,
        return_train_score=True,
    )
    scores_i["version"] = row["version"]
    scores = pd.concat([scores, pd.DataFrame(scores_i)], ignore_index=True)
```

```{python}
#| eval: false
from joypy import joyplot
joyplot(data=scores, by='version', column='test_r2', kind='kde', fill=True, ylim='own')

group_stats = scores.groupby('version')['test_r2'].agg(['mean', 'median']).reset_index()
group_stats

#sns.kdeplot(data=scores, x="test_r2", hue="version", common_norm=False, fill=True)
```


# Grid search

```{python}
wd_pipeline.set_params(polynomial__degree=9).fit(X, y).score(X, y)
```


```{python}

from sklearn.model_selection import GridSearchCV

param_grid = [
    {
        "polynomial__degree": [2, 3, 4, 5, 6, 7],
        "vars__columns": [
            ["tt_tu_mean"],
            ["tt_tu_mean", "rf_tu_mean"],
            ["tt_tu_mean", "tt_tu_min", "tt_tu_max", "rf_tu_mean"],
            ["tt_tu_mean", "rf_tu_min", "rf_tu_max", "rf_tu_mean"],
            ["tt_tu_min", "tt_tu_max", "rf_tu_min", "rf_tu_max"],
        ],
    },
]
# reducer_labels = ["PCA", "NMF", "KBest(mutual_info_classif)"]

scoring = [
    "r2",
    "explained_variance",
    "neg_median_absolute_error",
    "neg_root_mean_squared_error",
    "neg_mean_absolute_error",
]


grid = GridSearchCV(
    wd_pipeline, param_grid=param_grid, scoring=scoring, n_jobs=-1, cv=rkf, refit="explained_variance"
)
grid.fit(X, y)
pd.DataFrame(grid.cv_results_)

grid.best_params_
```


```{python}
# Convert the cv_results_ to a DataFrame
df = pd.DataFrame(grid.cv_results_)

value_vars = [
    col for col in df.columns if col.endswith("test_r2") and col.startswith("split")
]

# Melt the DataFrame to a long format
df_long = pd.melt(
    df,
    id_vars=["params", "param_polynomial__degree", "param_vars__columns"],
    value_vars=value_vars,
    var_name="scoring",
    value_name="score",
)

df_long["params"] = df_long["params"].astype(str)

# Print the long format DataFrame
#print(df_long)

#from joypy import joyplot

#joyplot(data=df_long, by="params", column="score", kind="kde", fill=True, ylim="own")


# sns.kdeplot(data=scores, x="test_r2", hue="version", common_norm=False, fill=True)

```


```{python}
import plotly.graph_objects as go

# Create a list of unique params
params = df_long['params'].unique()

# Create an empty list to store the traces
traces = []

# Loop through the params and create a histogram for each one
for i, param in enumerate(params):
    scores = df_long[df_long['params'] == param]['score']
    traces.append(
        go.Violin(x=[i]*len(scores), y=scores, name=str(i), box_visible=True, meanline_visible=True, hovertemplate=param)
    )
 
# Create the figure
fig = go.Figure(data=traces)

fig
```

```{python}
#| eval: false
fig.show(renderer="browser") 
```


```{python}
from sklearn.metrics import PredictionErrorDisplay
import matplotlib.pyplot as plt

y_pred = wd_pipeline.predict(strom_climate)

fig, axs = plt.subplots(ncols=2, figsize=(8, 4))
PredictionErrorDisplay.from_predictions(
    y,
    y_pred=y_pred,
    kind="actual_vs_predicted",
    #subsample=100,
    ax=axs[0],
    random_state=0,
)
axs[0].set_title("Actual vs. Predicted values")
PredictionErrorDisplay.from_predictions(
    y,
    y_pred=y_pred,
    kind="residual_vs_predicted",
    #subsample=100,
    ax=axs[1],
    random_state=0,
)
axs[1].set_title("Residuals vs. Predicted Values")
fig.suptitle("Plotting cross-validated predictions")
plt.tight_layout()
plt.show()
```

```{python}
strom.splom_fitted_observed(y, y_pred, strom_climate)
```

```{python}
strom.residuals_fitted(y, y_pred)
```

```{python}
strom.residuals_hist(y, y_pred)
```

```{python}
strom.residuals_qq(y, y_pred)
```

```{python}
strom.leverage(y, y_pred, wd_pipeline, strom_climate)
```

```{python}
strom.scale_location(y, y_pred, wd_pipeline, strom_climate)
```


```{python}
#import strom
#epyfun.reload_all()
```


# wrap-up

- cross-validating the model suggests that the best model should have 4 degree 
polinomial. 
- Relative humidity seems to be relevant and consistently improve the 
predictions, although marginally. So a proper mediation analysis is still needed.
- max and min temperatures and humidity, do not seem to improve much. So, everything already collected in the mean explains better the data, as the max 
and mins

## to-dos

- [x] It needs a polinomial with the temperature (second order perhaps)
- [ ] check if the association with relative humidity, if it is just an artifact of the correlation with temperature, or if there might something meaningful there going on
- [ ] and bring other climatic data, rainfall and snowfall might be relevant
- [ ] depite the much better fit, there are still some -although minor- heteroscedaticity, mostly associated to the lowest temperatures where the variance of strom use is higher
- [ ] there might be also a seasonal pattern, because we are not accounting for, for example, weekends and so on
- [ ] finally, there is still some evidence of autocorrelation. Which kinda makes sense: it is not the same to have one day with a very low temperature compared to a whole week with very low temperatures: while the former could be wintered with the heat already produced, as long as there is not much energy lose, the latter would certainly need much more work from the w√§rmepumpe to keep fighting the cold. In that sense, it seems unrealistic to just assume that the increase energy consumption of several days of extreme cold is just the sum of the daily increases. So if the one-day-increase due to extreme cold is 10, five consecutve days would probably result in an increase greater than 10*5=50, probably much greater. But let's explore such hipothesis in the data and try to accomodate that in the model using lags, or moving average of the last few days.
- [ ] influence plot and leverage plots should be interactive with tooltips with all the data associated to the points. Cook's distance perhaps as well.
- [ ] run some PCA, with min, max mean and so on and think about a Cumulative Explained Variance plot https://archive.is/X1wrZ#selection-1059.8-1059.37
- [X] and use cross validation
- [ ] perhaps experiment with feature selection
- [ ] and consider log-transforming the target, after all, the distribution of it is right-skewed with relatively long right tail


