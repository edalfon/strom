---
title: "H20 AutoML"
execute:
  echo: true
  eval: false
---


So let's start from the pipeline with the best model cross-validated before
```{python}
import strom
import epyfun
import pandas as pd

import strom
strom_climate = strom.read_result("merge_strom_climate_data")
#strom_climate = pd.read_parquet("interim/strom_climate.parquet")
X, y = strom.get_model_data()

import numpy as np

from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import PolynomialFeatures
from sklearn.preprocessing import FunctionTransformer
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_absolute_percentage_error
from sklearn.metrics import median_absolute_error
from sklearn.metrics import r2_score

from sklego.preprocessing import ColumnSelector

import matplotlib.pyplot as plt


def lag_predictor(X, predictor, lag=1):
    lagged = pd.Series(
        X[predictor].shift(lag).bfill(), name=predictor + "_lag" + str(lag)
    )
    X = pd.concat([X, lagged], axis=1)
    return X


def average_ndays(X, predictor, n=3):
    lagged = pd.Series(
        X[predictor].shift(1).rolling(n).mean().bfill(), name=predictor + "_ma" + str(n)
    )
    X = pd.concat([X, lagged], axis=1)
    return X


# {'columns__columns': ('tt_tu_mean', 'tt_tu_min', 'rf_tu_min'), 'preprocessor__degree': 3}
wd_pipeline = Pipeline(
    [
        ("columns", ColumnSelector(columns=["tt_tu_mean", "rf_tu_mean"])),
        # (
        #     "lag1",
        #     FunctionTransformer(average_ndays, kw_args={"predictor": "tt_tu_mean", "n": 5}),
        # ),
        (
            "preprocessor",
            ColumnTransformer(
                transformers=[
                    (
                        "polynomials",
                        PolynomialFeatures(degree=4, include_bias=False),
                        ["tt_tu_mean", "rf_tu_mean"],
                    ),
                ],
                remainder="passthrough",
            ),
        ),
        # ("preprocessor", PolynomialFeatures(degree=4)),
        ("model", LinearRegression()),
    ]
)

# X["tt_tu_mean"].shift(1).fillna(method="bfill")

# Train the model on the training data
wd_pipeline.fit(X, y)
strom.log_vetiver(wd_pipeline, "waermestrom")

# Make predictions on the test data
y_pred = wd_pipeline.predict(X)
```

I am new!

```{python}
dimensions = [
    "tt_tu_mean",
    "tt_tu_min",
    "tt_tu_max",
    "rf_tu_mean",
    "rf_tu_min",
    "rf_tu_max",
]
x = strom.assess_regression(wd_pipeline, X, y, dimensions)
```




```{python}
#| output: asis

# from IPython.display import display, Markdown
# for key, value in yyy.items():
#     print(key)
#     display(Markdown(key))
#     display(value)
```



## coefficients

```{python}
#feature_names = wd_pipeline[:-1].get_feature_names_out()

feature_names = wd_pipeline.named_steps['preprocessor'].get_feature_names_out()
print(feature_names)

coefs = pd.DataFrame(
    wd_pipeline[-1].coef_,
    columns=["Coefficients"],
    index=feature_names,
)

print(coefs)
```

```{python}
X_train_preprocessed = pd.DataFrame(
    wd_pipeline[:-1].transform(X), columns=feature_names
)

coefs = pd.DataFrame(
    wd_pipeline[-1].coef_ * X_train_preprocessed.std(axis=0),
    columns=["Coefficient importance"],
    index=feature_names,
)
coefs.plot(kind="barh", figsize=(9, 7))
plt.xlabel("Coefficient values corrected by the feature's std. dev.")
plt.title("Ridge model, small regularization")
plt.axvline(x=0, color=".5")
plt.subplots_adjust(left=0.3)
```



check coeficient variability / stability using cross validation


```{python}
from sklearn.model_selection import RepeatedKFold, cross_validate

cv = RepeatedKFold(n_splits=10, n_repeats=100, random_state=0)
cv_model = cross_validate(
    wd_pipeline,
    X,
    y,
    cv=cv,
    return_estimator=True,
    n_jobs=-1,
)

```

```{python}
import seaborn as sns
coefs = pd.DataFrame(
    [
        est[-1].coef_ * est[:-1].transform(X.iloc[train_idx]).std(axis=0)
        for est, (train_idx, _) in zip(cv_model["estimator"], cv.split(X, y))
    ],
    columns=feature_names,
)
plt.figure(figsize=(9, 7))
sns.stripplot(data=coefs, orient="h", palette="dark:k", alpha=0.5)
sns.boxplot(data=coefs, orient="h", color="cyan", saturation=0.5, whis=10)
plt.axvline(x=0, color=".5")
plt.xlabel("Coefficient importance")
plt.title("Coefficient importance and its variability")
plt.suptitle("Ridge model, small regularization")
plt.subplots_adjust(left=0.3)
```





# with mins

```{python}
import pandas as pd
import numpy as np
import epyfun
import strom

import strom
strom_climate = strom.read_result("merge_strom_climate_data")
# strom_climate = pd.read_parquet("interim/strom_climate.parquet")
X = strom_climate.drop(columns="wd")
y = strom_climate["wd"]

from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_absolute_percentage_error
from sklearn.metrics import median_absolute_error
from sklearn.metrics import r2_score

from sklego.preprocessing import ColumnSelector
#{'columns__columns': ('tt_tu_mean', 'tt_tu_min', 'rf_tu_min'), 'preprocessor__degree': 3}
wd_pipeline = Pipeline([
    ("columns", ColumnSelector(columns=['tt_tu_mean', 'tt_tu_min', 'rf_tu_min'])),
    ('preprocessor', PolynomialFeatures(degree=3)),
    ('model', LinearRegression())
])

wd_pipeline.fit(X, y)
y_pred = wd_pipeline.predict(X)
```

```{python}
x = strom.assess_regression(wd_pipeline, X, y, dimensions)
```

## coefficients

```{python}
#feature_names = wd_pipeline[:-1].get_feature_names_out()

feature_names = wd_pipeline.named_steps['preprocessor'].get_feature_names_out()
print(feature_names)

coefs = pd.DataFrame(
    wd_pipeline[-1].coef_,
    columns=["Coefficients"],
    index=feature_names,
)

print(coefs)
```

```{python}
X_train_preprocessed = pd.DataFrame(
    wd_pipeline[:-1].transform(X), columns=feature_names
)

coefs = pd.DataFrame(
    wd_pipeline[-1].coef_ * X_train_preprocessed.std(axis=0),
    columns=["Coefficient importance"],
    index=feature_names,
)
coefs.plot(kind="barh", figsize=(9, 7))
plt.xlabel("Coefficient values corrected by the feature's std. dev.")
plt.title("Ridge model, small regularization")
plt.axvline(x=0, color=".5")
plt.subplots_adjust(left=0.3)
```


check coeficient variability / stability using cross validation

```{python}
from sklearn.model_selection import RepeatedKFold, cross_validate

cv = RepeatedKFold(n_splits=10, n_repeats=100, random_state=0)
cv_model = cross_validate(
    wd_pipeline,
    X,
    y,
    cv=cv,
    return_estimator=True,
    n_jobs=-1,
)

```

```{python}
import seaborn as sns
coefs = pd.DataFrame(
    [
        est[-1].coef_ * est[:-1].transform(X.iloc[train_idx]).std(axis=0)
        for est, (train_idx, _) in zip(cv_model["estimator"], cv.split(X, y))
    ],
    columns=feature_names,
)
plt.figure(figsize=(9, 7))
sns.stripplot(data=coefs, orient="h", palette="dark:k", alpha=0.5)
sns.boxplot(data=coefs, orient="h", color="cyan", saturation=0.5, whis=10)
plt.axvline(x=0, color=".5")
plt.xlabel("Coefficient importance")
plt.title("Coefficient importance and its variability")
plt.suptitle("Ridge model, small regularization")
plt.subplots_adjust(left=0.3)
```





# statmodels

```{python}
xstatsmodels = strom.get_design_matrix(wd_pipeline, X)

from statsmodels.compat import lzip

import numpy as np
import pandas as pd
import statsmodels.formula.api as smf
import statsmodels.stats.api as sms
import matplotlib.pyplot as plt

import statsmodels.api as sm
import pandas as pd
results = sm.OLS(y, xstatsmodels).fit()
print(results.summary())
```


```{python}
name = ["Jarque-Bera", "Chi^2 two-tail prob.", "Skew", "Kurtosis"]
test = sms.jarque_bera(results.resid)
lzip(name, test)
```


```{python}
name = ["Chi^2", "Two-tail probability"]
test = sms.omni_normtest(results.resid)
lzip(name, test)
```

```{python}
from statsmodels.stats.outliers_influence import OLSInfluence

test_class = OLSInfluence(results)
test_class.dfbetas[:5, :]
```

```{python}
from statsmodels.graphics.regressionplots import plot_leverage_resid2

fig, ax = plt.subplots(figsize=(8, 6))
fig = plot_leverage_resid2(results, ax=ax)
```

```{python}
from statsmodels.graphics.regressionplots import influence_plot
influence_plot(results)
```

```{python}
from statsmodels.graphics.regressionplots import plot_leverage_resid2
plot_leverage_resid2(results)
```



```{python}
from sklearn.preprocessing import PolynomialFeatures
import matplotlib.pyplot as plt

import pandas as pd
from sklego.preprocessing import FormulaicTransformer

wd_pipeline = Pipeline(
    [
        ("columns", ColumnSelector(columns=["tt_tu_mean", "rf_tu_mean", "rf_tu_min", "rf_tu_max"])),
        ("parsy", FormulaicTransformer("""
        tt_tu_mean + I(tt_tu_mean ** 2) + I(tt_tu_mean ** 3) + 
           I(tt_tu_mean ** 4) + I(tt_tu_mean ** 5) +
           rf_tu_mean + rf_tu_min + rf_tu_max +
           tt_tu_mean.shift(1, fill_value=-1.3)
        """)),
        ("model", LinearRegression()),
    ]
)

#xxx = X["tt_tu_mean"]
#xxx.shift(1, fill_value=1)

# Train the model on the training data
wd_pipeline.fit(X, y)

# Make predictions on the test data
y_pred = wd_pipeline.predict(X)
```

import statsmodels.formula.api as smf
model = smf.ols(formula="""
      wd ~ tt_tu_mean + I(tt_tu_mean ** 2) + I(tt_tu_mean ** 3) + 
           I(tt_tu_mean ** 4) + I(tt_tu_mean ** 5) +
           rf_tu_mean + rf_tu_min + rf_tu_max +
           tt_tu_mean.shift(1) + tt_tu_mean.shift(2) + tt_tu_mean.shift(3)
  """, data=strom_climate).fit()


```{python}
dimensions = [
    "tt_tu_mean",
    "tt_tu_min",
    "tt_tu_max",
    "rf_tu_mean",
    "rf_tu_min",
    "rf_tu_max",
]
x = strom.assess_regression(wd_pipeline, X, y, dimensions)
```




```{python}
#| eval: false
from sklearn.metrics import get_scorer_names
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import HistGradientBoostingRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
import itertools

from sklearn.model_selection import cross_val_score

from sklearn.model_selection import TimeSeriesSplit
tscv = TimeSeriesSplit(n_splits=5)

from sklearn.model_selection import RepeatedKFold
rkf = RepeatedKFold(n_splits=10, n_repeats=100, random_state=7)

from sklearn.model_selection import KFold
kf = KFold(n_splits=10, shuffle=True)


column_candidates = ["tt_tu_mean", "tt_tu_min", "tt_tu_max", 
                    "rf_tu_mean", "rf_tu_min", "rf_tu_max"]
all_columns_combinations = []
# Generate combinations of all sizes from 1 to the length of the list
for r in range(1, len(column_candidates) + 1):
    combinations_of_size_r = list(itertools.combinations(column_candidates, r))
    all_columns_combinations.extend(combinations_of_size_r)


param_grid = [
    {
        "columns__columns": all_columns_combinations,
        "preprocessor__degree": [3, 4, 5, 6, 7],
    },
    {
        "columns__columns": all_columns_combinations,
        "preprocessor": [
            ColumnTransformer(
                transformers=[
                    (
                        "transformer",
                        PolynomialFeatures(degree=4, include_bias=False),
                        ["tt_tu_mean"],
                    ),
                ],
                remainder="passthrough",
            )
        ],
    },
    {
        "columns__columns": all_columns_combinations,
        "preprocessor": ["passthrough"],
        "model": [HistGradientBoostingRegressor(), RandomForestRegressor(), SVR(kernel="poly")]
    },
]
# reducer_labels = ["PCA", "NMF", "KBest(mutual_info_classif)"]

scoring = [
    "r2",
    "explained_variance",
    "neg_median_absolute_error",
    "neg_root_mean_squared_error",
    "neg_mean_absolute_error",
]


grid = GridSearchCV(
    wd_pipeline,
    param_grid=param_grid,
    scoring=scoring,
    n_jobs=-1,
    cv=rkf,
    refit="explained_variance",
)
grid.fit(X, y)
pd.DataFrame(grid.cv_results_)

grid.best_params_
grid.best_score_
```





sdf


#import strom

#model = strom.get_vetiver("waermestrom")

#model.handler_predict(input_data=strom_climate, check_prototype=True)

#strom.assess_vetiver_regression(model, strom_climate, strom_climate["wd"])




from sklearn.model_selection import RepeatedKFold
from sklearn.model_selection import cross_val_score

cv = RepeatedKFold(n_splits=10, n_repeats= 100, random_state=1)

 getting misclassification rate
scores = 1 - cross_val_score(SMWrapper(sm.OLS), independent_variables, dependent_variable, scoring='neg_median_absolute_error', cv=cv, n_jobs=-1)
import seaborn as sns
ax = sns.histplot(x=scores, kde=True)





import statsmodels.formula.api as smf
model = smf.ols(formula="""wd ~ tt_tu_mean + rf_tu_mean""", data=strom_climate).fit()

print(model.summary())

model = strom.log_vetiver(model, "waermestrom")






import statsmodels.api as sm
from sklearn.base import BaseEstimator, RegressorMixin


class SMWrapper(BaseEstimator, RegressorMixin):
    """A universal sklearn-style wrapper for statsmodels regressors"""

    def __init__(self, model_class, fit_intercept=True):
        self.model_class = model_class
        self.fit_intercept = fit_intercept

    def fit(self, X, y):
        if self.fit_intercept:
            X = sm.add_constant(X)
        self.model_ = self.model_class(y, X)
        self.results_ = self.model_.fit()
        return self

    def predict(self, X):
        if self.fit_intercept:
            X = sm.add_constant(X)
        return self.results_.predict(X)




from sklearn.model_selection import RepeatedKFold
from sklearn.model_selection import cross_val_score

cv = RepeatedKFold(n_splits=10, n_repeats= 100, random_state=1)
model = LogisticRegression()

getting misclassification rate
scores = 1 - cross_val_score(model, X, y, scoring='r2_score', cv=cv, n_jobs=-1)
import seaborn as sns
ax = sns.histplot(x=scores, kde=True)





import strom
model = strom.get_vetiver("waermestrom")

strom.assess_vetiver_regression(model, strom_climate, "wd")






# Vetiver

import strom
model = strom.get_vetiver("waermestrom")
model.handler_predict(X, False)


from vetiver import mock, VetiverModel
mm = VetiverModel.from_pin(strom.get_board(), "waermestrom")
mm.handler_predict(strom_climate, False)

import vetiver
vetiver.compute_metrics()

import pandas as pd
import numpy as np
import seaborn as sns
import epyfun

strom_climate = pd.read_parquet("interim/strom_climate.parquet")

import strom
vetiver_model = strom.get_vetiver("waermestrom")
y_pred = strom.assess_vetiver_regression(vetiver_model, strom_climate, strom_climate["wd"])
y_pred


from vetiver import mock, VetiverModel
X, y = mock.get_mock_data()
model = mock.get_mock_model().fit(X, y)
v = VetiverModel(model = model, model_name = "my_model", prototype_data = X)

import strom
strom.assess_vetiver_regression(v, X, y)




























```{python}
wd_pipeline.set_params(polynomial__degree=9).fit(X, y).score(X, y)
```

```{python}
import pandas as pd
penguins = pd.read_csv("https://gist.githubusercontent.com/slopp/ce3b90b9168f2f921784de84fa445651/raw/4ecf3041f0ed4913e7c230758733948bc561f434/penguins.csv")
ojs_define(data = penguins)
```


```{ojs}
filtered = transpose(data).filter(function(penguin) {
  return bill_length_min < penguin.bill_length_mm &&
         islands.includes(penguin.island);
})
```


```{ojs}
viewof bill_length_min = Inputs.range(
  [32, 50], 
  {value: 35, step: 1, label: "Bill length (min):"}
)
viewof islands = Inputs.checkbox(
  ["Torgersen", "Biscoe", "Dream"], 
  { value: ["Torgersen", "Biscoe"], 
    label: "Islands:"
  }
)
```


```{ojs}
Plot.rectY(filtered, 
  Plot.binX(
    {y: "count"}, 
    {x: "body_mass_g", fill: "species", thresholds: 20}
  ))
  .plot({
    facet: {
      data: filtered,
      x: "sex",
      y: "species",
      marginRight: 80
    },
    marks: [
      Plot.frame(),
    ]
  }
)
```









```{ojs}
transpose(metrics_scores)
```

```{ojs}
Plot.plot({
  x: {axis: null},
  y: {tickFormat: "s", grid: true},
  color: {scheme: "spectral", legend: true},
  marks: [
    Plot.barY(transpose(metrics_scores), {
      x: "mode",
      y: "value",
      fill: "traintest",
      fx: "metric",
    }),
    Plot.ruleY([0])
  ]
})
```

```{ojs}
Plot.plot((() => {
  const n = 3; // number of facet columns
  const keys = Array.from(d3.union(transpose(metrics_scores).map((d) => d.metric)));
  const index = new Map(keys.map((key, i) => [key, i]));
  const fx = (key) => index.get(key) % n;
  const fy = (key) => Math.floor(index.get(key) / n);
  return {
    height: 300,
    axis: null,
    y: {insetTop: 10},
    fx: {padding: 0.03},
    marks: [
        Plot.barY(transpose(metrics_scores), {
        x: "mode",
        y: "value",
        fill: "traintest",
        fx: (d) => fx(d.metric),
        fy: (d) => fy(d.metric),
        }),
        Plot.ruleY([0])
    ]
  };
})())
```

not going this way to make the plot anymore because
observable plot does not yet have automatic facet wrap https://observablehq.com/plot/features/facets
and although it is not so hard to do, as in the example above,
what is not yet possible is to have something like scales free_y









9. Influence Plot:
7. Partial Regression (Added Variable) Plot:
5. Cook’s Distance Plot:
4. Residuals vs. Leverage Plot:



```{python}
from statsmodels.graphics.regressionplots import plot_partregress

# Assuming 'model' is your fitted regression model and 'x' is the predictor variable
plot_partregress('wd', 'tt_tu_mean', ['rf_tu_mean'], data=train_set)
plt.title("Partial Regression (Added Variable) Plot")
plt.show()
```

### 4. Influence and Outliers:

```{python}
#| eval: false

# Influence plot
sm.graphics.influence_plot(model)
plt.show()
```

This plot helps identify influential points or outliers.

### 5. Leverage-Residuals plot:

```{python}
#| eval: false

# Leverage-Residuals plot
sm.graphics.plot_leverage_resid2(model)
plt.show()
```

```{python}
#| eval: false

from statsmodels.graphics.regressionplots import plot_leverage_resid2

# Assuming 'model' is your fitted regression model
plot_leverage_resid2(model)
plt.title("Residuals vs. Leverage Plot")
plt.show()
```
This plot helps identify points with high leverage.

### 6. Cook's Distance:

```{python}
#| eval: false
# Cook's Distance plot
sm.graphics.plot_cooks_distance(model)
plt.show()
```

This plot helps identify influential observations.


```{python}
#| eval: false

import scipy as sp
import numpy as np
import pandas as pd 
import matplotlib.pyplot as plt 

# Note: statsmodels requires scipy 1.2
import statsmodels.formula.api as sm

from sklearn.datasets import make_regression
from sklearn.linear_model import LinearRegression
from statsmodels.stats.outliers_influence import OLSInfluence as influence

```

```{python}
#| eval: false

inf = influence(model)
C, P = inf.cooks_distance
def plot_cooks_distance(c):
    _, ax = plt.subplots(figsize=(9,6))
    ax.stem(c, markerfmt=",")
    ax.set_xlabel("instance")
    ax.set_ylabel("distance")
    ax.set_title("Cook's Distance Outlier Detection")
    return ax


plot_cooks_distance(C)
```


### 7. Partial Regression Plots:

```{python}
#| eval: false

# Partial regression plots
import statsmodels.api as sm
sm.graphics.plot_partregress_grid(model)
plt.show()
```

These plots help identify the influence of each independent variable on the dependent variable while holding other variables constant.

These diagnostic plots can provide insights into the performance of your regression model and help you identify any violations of the assumptions of linear regression. Adjustments to the model or data transformation may be necessary based on the diagnostic results.



### 7. Partial Regression (Added Variable) Plot:
This plot helps visualize the relationship between an individual predictor and the response variable while accounting for the other predictors in the model.


```{python}
#| eval: false

from statsmodels.graphics.regressionplots import plot_partregress
# Assuming 'model' is your fitted regression model and 'x' is the predictor variable
plot_partregress('wd', 'tt_tu_mean', ['rf_tu_mean'], data=strom_climate)
plt.title("Partial Regression (Added Variable) Plot")
plt.show()

```

- [ ] influence plot and leverage plots should be interactive with tooltips with all the data associated to the points. Cook's distance perhaps as well.


Time series models ala auto-ml
- Cassandra
- AutoTS
- Prophet
- Orbit


# TODO plot param vs. metric, and to the side, param vs. time


# TODO: PCA https://medium.com/geekculture/principal-component-analysis-pca-in-feature-engineering-472afa39c27d

https://scikit-learn.org/stable/auto_examples/compose/plot_digits_pipe.html#sphx-glr-auto-examples-compose-plot-digits-pipe-py


# todo through
https://scikit-learn.org/stable/modules/cross_validation.html


# todo
out of bag estimates
https://scikit-learn.org/stable/modules/grid_search.html


# todo
yellow brick 
https://www.scikit-yb.org/en/latest/matplotlib.html

# model persistence 
https://scikit-learn.org/stable/model_persistence.html



wrappers including prophet wrappers fr scikit learn api
https://hcrystalball.readthedocs.io/en/latest/wrappers.html


