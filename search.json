[
  {
    "objectID": "Setup/02_duckdb.html",
    "href": "Setup/02_duckdb.html",
    "title": "B. DuckDB",
    "section": "",
    "text": "We will be setting-up DuckDB to work in Jupyter-powered Quarto Documents.\n\nSet-up jupysql to be able to write sql queries directly\nTo do it with DuckDB, basically following this guide, just need to make sure jupysql, SQLAlchemy and duckdb-engine are installed, besides the core libraries (notebook, pandas, duckdb). If any of them mssing, simply pip install them.\nStep 1 is then to import extension. It enables SQL cells in Jupyter. It supports inline SQL using %sql and a whole SQL cell starting it with %%sql.\n\nimport duckdb\nimport pandas as pd\nimport sqlalchemy # No need to import duckdb_engine, \n                  # SQLAlchemy will auto-detect \n\n%load_ext sql\n%config SqlMagic.autopandas = True\n%config SqlMagic.feedback = False\n%config SqlMagic.displaycon = False\n\nLoading configurations from /home/runner/work/strom/strom/pyproject.toml.\n\n\nSettings changed:\n\n\n\n\n\nConfig\nvalue\n\n\n\n\nfeedback\nTrue\n\n\nautopandas\nTrue\n\n\ndisplaycon\nFalse\n\n\ndsn_filename\n./connections.ini\n\n\n\n\n\nI prefer Quarto to edit my notebooks, and the above still works. However, it seems Quarto’s SQL engine is still only for R since it requires knitr and does not seem to support the combo ipython-sql-SQLAlchemy. So you cannot simply use an SQl chunk like this\n```{sql}\nSELECT * FROM test;\n```\nBut you have to use a standard python chunk and use the %sql or %%sql to be able to write SQL direcly.\nStep 2 is to fire-up DuckDB, either in memory or pointing to a file.\n\n%sql duckdb:///:memory:\n# %sql duckdb:///path/to/file.db\n\nTest it’s working\n\n%sql SELECT 'Off and flying!' as a_duckdb_column\n\n\n\n\n\n\n\n\na_duckdb_column\n\n\n\n\n0\nOff and flying!\n\n\n\n\n\n\n\n\n%sql SELECT * FROM duckdb_settings();\n\n\n\n\n\n\n\n\nname\nvalue\ndescription\ninput_type\nscope\naliases\n\n\n\n\n0\nCalendar\ngregorian\nThe current calendar\nVARCHAR\nGLOBAL\n[]\n\n\n1\nTimeZone\nEtc/UTC\nThe current time zone\nVARCHAR\nGLOBAL\n[]\n\n\n2\naccess_mode\nautomatic\nAccess mode of the database (AUTOMATIC, READ_O...\nVARCHAR\nGLOBAL\n[]\n\n\n3\nallocator_background_threads\nfalse\nWhether to enable the allocator background thr...\nBOOLEAN\nGLOBAL\n[]\n\n\n4\nallocator_bulk_deallocation_flush_threshold\n512.0 MiB\nIf a bulk deallocation larger than this occurs...\nVARCHAR\nGLOBAL\n[]\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n131\nusername\nNone\nThe username to use. Ignored for legacy compat...\nVARCHAR\nGLOBAL\n[user]\n\n\n132\nvariant_legacy_encoding\nfalse\nEnables the Parquet reader to identify a Varia...\nBOOLEAN\nGLOBAL\n[]\n\n\n133\nwal_autocheckpoint\n16.0 MiB\nThe WAL size threshold at which to automatical...\nVARCHAR\nGLOBAL\n[]\n\n\n134\nworker_threads\n4\nThe number of total threads used by the system.\nBIGINT\nGLOBAL\n[]\n\n\n135\nzstd_min_string_length\n4096\nThe (average) length at which to enable ZSTD c...\nUBIGINT\nGLOBAL\n[]\n\n\n\n\n136 rows × 6 columns\n\n\n\nWe will need to make sure we load the extension for all Quarto documents, as they will all be different jupyter notebooks (TODO: double-check this). So we will use Quarto includes to keep the source files not-so-verbose.\n\nNote that we use an underscore (_) prefix for the included file. You should always use an underscore prefix with included files so that they are automatically ignored (i.e. not treated as standalone files) by a quarto render of a project).\n\nAlso, remember the “trick” here\n\nIn Quarto projects, an include path that begins with a leading slash will be interpreted as project relative, meaning that you should be able to use an include such as:\n\n\"{{&lt; include /_codebit.qmd &gt;}}\"\n\nto include from the project root no matter the subdirectory of the file itself.",
    "crumbs": [
      "Dashboard",
      "Setup",
      "B. DuckDB"
    ]
  },
  {
    "objectID": "Process/03_dwd_data.html",
    "href": "Process/03_dwd_data.html",
    "title": "DWD Daten",
    "section": "",
    "text": "Deutscher Wetterdienst\nhttps://www.dwd.de/DE/leistungen/cdc/cdc_ueberblick-klimadaten.html\n\n\nstundliche Auflösung Wetter Stationen\nladen wir die Stationslisten herunter mit allen Klimastationen, die Lufttemperatur - stündliche Auflösung haben.\n\nimport epyfun\nstations_file = \"interim/climate/latest/TU_Stundenwerte_Beschreibung_Stationen.txt\"\n\nepyfun.web.download_file(\"https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/air_temperature/historical/TU_Stundenwerte_Beschreibung_Stationen.txt\", stations_file)\n\nepyfun.fs.convert_to_utf8(stations_file)\n\nZeige die Stationen kurz auf einer Karte an, wobei sich Brannenburg in der Mitte befindet, so dass es leicht ersichtlich ist, welche Stationen näher liegen.\n\nimport pandas as pd\n\ndf = pd.read_fwf(\n    stations_file,\n    colspecs=\"infer\",\n    skiprows=3,\n    names=[\n        \"Stations_id\",\n        \"von_datum\",\n        \"bis_datum\",\n        \"Stationshoehe\",\n        \"geoBreite\",\n        \"geoLaenge\",\n        \"Stationsname\",\n        \"Bundesland\",\n    ],\n)\n\n\nimport folium\nfrom folium.plugins import MarkerCluster\nimport pandas as pd\n\nbrannenburg_coords = [47.7424, 12.1041]  # source wikipedia\nmy_map = folium.Map(location=brannenburg_coords, zoom_start=10)\n\nfor index, row in df.iterrows():\n    folium.Marker(\n        [row[\"geoBreite\"], row[\"geoLaenge\"]],\n        popup=f\"\"\"\n            {row[\"Stationsname\"]} ({str(row[\"Stations_id\"])})\n            &lt;br/&gt;\n            {str(row[\"Stationshoehe\"])} m2\n            &lt;br/&gt;\n            Von: {str(row[\"von_datum\"])}\n            &lt;br/&gt;\n            Bis: {str(row[\"bis_datum\"])}\n        \"\"\",\n    ).add_to(my_map)\n\nmy_map\n\nAlso, wir sollten entweder Kiefersfelden-Gach (3679) oder Rosenheim (4261) verwenden. Wendelstein (5467) ist ebenfalls näher, aber höher (832 m²). Daher wäre es möglicherweise besser, Kiefersfelden oder Rosenheim zu wählen, da deren Höhen (518 m² bzw. 442 m²) viel ähnlicher sind als die Höhe von Brannenburg (509 m² laut Wikipedia).\nDann schreiben wir eine kleine Funktion, die nach Dateien für solche Stationen sucht und sie herunterlädt. (Eine Funktion, da wir dies mindestens zweimal machen müssen, weil es Dateien mit historischen Daten und Dateien mit aktuellen Daten gibt.)\n\n\nfns to support download data\n\nimport requests\nfrom bs4 import BeautifulSoup\n\n\ndef download_climate_data(listing_url):\n    # Fetch HTML content\n    response = requests.get(listing_url)\n    html_content = response.text\n\n    # Parse HTML content\n    soup = BeautifulSoup(html_content, \"html.parser\")\n\n    # Extract links that contains the stations ids, with the codes\n    # for the stations of kiefersfelden or rosenheim\n    target_links = [\n        a[\"href\"]\n        for a in soup.find_all(\"a\", href=True)\n        if \"_03679_\" in a[\"href\"] or \"_04261_\" in a[\"href\"]\n    ]\n\n    downloaded_files = []\n    for link in target_links:\n        print(\"Downloading: \", link)\n        epyfun.web.download_file(\n            listing_url + \"/\" + link, \"interim/climate/latest/\" + link\n        )\n        downloaded_files.append(\"interim/climate/latest/\" + link)\n\n    return downloaded_files\n\n\nimport pandas as pd\nimport zipfile\nimport io\n\ndef read_csv_from_zip(zip_file_path):\n    with zipfile.ZipFile(zip_file_path, 'r') as zip_file:\n        # Get the list of file names in the zip file\n        file_names = zip_file.namelist()\n\n        # Filter the file names that start with the specified prefix\n        target_files = [file_name for file_name in file_names if file_name.startswith(\"produkt\")]\n\n        print(target_files)\n        if not target_files:\n            raise Exception(\"No files start with produkt\")\n        if len(target_files) &gt; 1:\n            raise Exception(\"There are more files that start with produkt. Double check\")\n        \n        target_file_name = target_files[0]\n\n        # Read the CSV file directly from the zip archive using pandas\n        with zip_file.open(target_file_name) as file_in_zip:\n            df = pd.read_csv(io.TextIOWrapper(file_in_zip), sep=';')  # Adjust the separator based on the file format\n\n        return df\n\n\ndef bind_rows_files(all_files):\n    # all_files = [\"interim/climate/latest/stundenwerte_TU_04261_akt.zip\",\n    # \"./interim/climate/latest/stundenwerte_TU_04261_20060301_20221231_hist.zip\"]\n    #read_csv_from_zip(\"interim/climate/latest/stundenwerte_TU_04261_akt.zip\")\n    all_dfs = [read_csv_from_zip(file) for file in all_files]\n    result_df = pd.concat(all_dfs, ignore_index=True)\n    result_df['MESS_DATUM'] = pd.to_datetime(result_df['MESS_DATUM'], format='%Y%m%d%H')\n\n    return result_df\n\n\n\nLufttemperatur - stündliche Auflösung\n\nhistorical_files = download_climate_data(\"https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/air_temperature/historical/\")\n\nrecent_files = download_climate_data(\"https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/air_temperature/recent/\")\n\nair = bind_rows_files(historical_files + recent_files)\naircols = [\"TT_TU\", \"RF_TU\"]\n\nEs gibt komische Werte hier. Es scheint, dass -999 als Missing Value-Indikator verwendet wird (für RF_TU, relative Feuchte).\n\nimport numpy as np\nair.replace(-999, np.nan, inplace=True)\n\n\nimport plotly.express as px\n\n# Filter rows where the date is greater than 2020\nfiltered_df = air[air['MESS_DATUM'] &gt; '2020-01-01']\n\nfig = px.line(\n    filtered_df,\n    x=\"MESS_DATUM\",\n    y=aircols,\n    labels={\"value\": \"Values\"},\n    line_shape=\"linear\"\n)\nfig.show()\n\nepyfun.splom(filtered_df, aircols + [\"MESS_DATUM\"]).show()\n\n\n\nTaupunkttemperatur - stündliche Auflösung\n\nhistorical_files = download_climate_data(\"https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/dew_point/historical/\")\n\nrecent_files = download_climate_data(\"https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/dew_point/recent/\")\n\ndew = bind_rows_files(historical_files + recent_files)\ndewcols = [\"  TT\", \"  TD\"]\n\nEs gibt komische Werte hier. Es scheint, dass -999 als Missing Value-Indikator verwendet wird (für RF_TU, relative Feuchte).\n\nimport numpy as np\ndew.replace(-999, np.nan, inplace=True)\n\n\nimport plotly.express as px\n\n# Filter rows where the date is greater than 2020\nfiltered_df = dew[dew['MESS_DATUM'] &gt; '2020-01-01']\n\nfig = px.line(\n    filtered_df,\n    x=\"MESS_DATUM\",\n    y=dewcols,\n    labels={\"value\": \"Values\"},\n    line_shape=\"linear\"\n)\nfig.show()\n\nepyfun.splom(filtered_df, dewcols + [\"MESS_DATUM\"]).show()\n\n\n\nMoisture - stündliche Auflösung\n\nhistorical_files = download_climate_data(\"https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/moisture/historical/\")\n\nrecent_files = download_climate_data(\"https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/moisture/recent/\")\n\nmoist = bind_rows_files(historical_files + recent_files)\nmoistcols = ['VP_STD', 'TF_STD', 'P_STD', 'TT_STD', 'RF_STD', 'TD_STD']\n\n\nimport plotly.express as px\n\n# Filter rows where the date is greater than 2020\nfiltered_df = moist[moist['MESS_DATUM'] &gt; '2020-01-01']\n\nfig = px.line(\n    filtered_df,\n    x=\"MESS_DATUM\",\n    y=moistcols,\n    labels={\"value\": \"Values\"},\n    line_shape=\"linear\"\n)\nfig.show()\n\nepyfun.splom(filtered_df, moistcols + [\"MESS_DATUM\"]).show()\n\n\n\nPrecipitaci - stündliche Auflösung\n\nhistorical_files = download_climate_data(\"https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/precipitation/historical/\")\n\nrecent_files = download_climate_data(\"https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/precipitation/recent/\")\n\nprecip = bind_rows_files(historical_files + recent_files)\nprecipcols = ['  R1', 'RS_IND', 'WRTR']\n\nEs gibt komische Werte hier. Es scheint, dass -999 als Missing Value-Indikator verwendet wird.\n\nimport numpy as np\nprecip.replace(-999, np.nan, inplace=True)\n\n\nimport plotly.express as px\n\n# Filter rows where the date is greater than 2020\nfiltered_df = precip[precip['MESS_DATUM'] &gt; '2020-01-01']\n\nfig = px.line(\n    filtered_df,\n    x=\"MESS_DATUM\",\n    y=precipcols,\n    labels={\"value\": \"Values\"},\n    line_shape=\"linear\"\n)\nfig.show()\n\nepyfun.splom(filtered_df, precipcols + [\"MESS_DATUM\"]).show()\n\n\n\njoin in a single table and mach es auf täglicheauslösung\n\noncols = [\"MESS_DATUM\", \"STATIONS_ID\"]\nagg_df = air.merge(dew[oncols + dewcols], on=oncols)\nagg_df = agg_df.merge(moist[oncols + moistcols], on=oncols)\nagg_df = agg_df.merge(precip[oncols + precipcols], on=oncols)\n\n\ndatacols = aircols + dewcols + moistcols + precipcols\nagg_df[\"date\"] = agg_df[\"MESS_DATUM\"].dt.normalize()\nagg_df = (\n    agg_df.groupby(\"date\")\n    .agg({key: [\"min\", \"mean\", \"max\"] for key in datacols})\n    .reset_index()\n)\nagg_df = epyfun.pandas.clean_names(agg_df)\n\n\n#epyfun.splom(agg_df).show(\"browser\")\n\n\n\nmerge everything and save the file\n\n# agg_df.to_parquet('./interim/climate/climate_daily.parquet', index=False)",
    "crumbs": [
      "Dashboard",
      "Process",
      "DWD Daten"
    ]
  },
  {
    "objectID": "Process/01_strom_process.html",
    "href": "Process/01_strom_process.html",
    "title": "Strom",
    "section": "",
    "text": "%%sql\nCREATE OR REPLACE VIEW fail AS\nSELECT * FROM sqlite_scan('{{latest_file}}', 'meter');\nSELECT * FROM fail;\n\n\nGet strom data\nBy querying directly the SQLite table and filtering by meter id.\n\n%%sql \n\nCREATE OR REPLACE TABLE strom AS \nWITH strom_sqlite AS (\n  SELECT \n    meterid, \n    -- Blob Functions, because most columns get read as blob\n    -- https://duckdb.org/docs/sql/functions/blob\n    decode(date)::DATETIME AS date, \n    decode(value)::INT AS value\n  FROM sqlite_scan('{{latest_file}}', 'reading') \n  WHERE meterid = 1\n)\nSELECT \n  *,\n  -- add default values to lag(), to prevent null in the first row\n  date_sub('minute', lag(date, 1, '2020-11-30 00:00:00') over(order by date), date) AS minutes, \n  -- add default values to lag(), to prevent null in the first row\n  value - lag(value, 1, 12160) over(order by date) AS consumption,\n  1.0 * consumption / minutes AS cm,\n  24.0 * 60.0 * consumption / minutes AS consumption_day_equivalent\nFROM strom_sqlite\nORDER BY date\n;\n\n\n\nVisualize the data\n\n%%sql \nstrom &lt;&lt; SELECT * FROM strom;\n\nOf course noisy data, with substantial variation in the consumption day equivalent and there is 1.5 years without data.\n\nimport plotly.express as px\nfig = px.line(strom, x='date', y=\"consumption_day_equivalent\")\nfig.show()\n\n\n#import pandas as pd\n#from pandas_profiling import ProfileReport\n\n#EDA using pandas-profiling\n#profile = ProfileReport(strom, explorative=True)\n#profile.to_file(\"output.html\")\n\nWith the exception of the long period without data, the number of minutes shows there are measurements from a few hours away, to a few days away. Most measurements are between 7 and 16 hours apart. That’s worrisome, as the periods are relatively long. In addition, the measurements are scattered and do not follow a systematic pattern.\n\nimport plotly.express as px\nfig = px.histogram(strom.query(\"minutes &lt; 10000\"), x=\"minutes\", marginal=\"box\")\nfig.show()\n\nThe consumption day equivalent varies also substantially. Median 8.8, which is consistent with the long-run consumption (equivalent to python 8.8*365 per year.). The distribution has a long right tail, with very high consumptions, presumably, associated to very short measurements periods.\n\nimport plotly.express as px\nfig = px.histogram(strom.query(\"minutes &lt; 10000\"), x=\"consumption_day_equivalent\", marginal=\"box\")\nfig.show()\n\nWell, yeah, as expected, short measurement periods (few minutes) are associated with higher variability, and with the highest and lowest consumptions.\n\nfrom matplotlib import pyplot\npyplot.scatter(\n    strom.query(\"minutes &lt; 10000\")[\"minutes\"], \n    strom.query(\"minutes &lt; 10000\")[\"consumption_day_equivalent\"]\n)\n\n\nimport plotly.express as px\nfig = px.scatter(\n    data_frame=strom.query(\"minutes &lt; 10000\"), \n    x=\"minutes\", \n    y=\"consumption_day_equivalent\", hover_data=['date'],\n    marginal_x=\"histogram\", \n    marginal_y=\"histogram\"\n)\nfig.show()\n\n\n\nConsumption by hour\nLet’s try to see what hours have the highest consumption. That’s tricky given this messy data. One approach is to just interpolate between data points and assume a constant consumption. That’s of course not realistic (specially during the day), but it would get us closer.\n\n%%sql\nSELECT MIN(date), MAX(DATE) FROM strom;\n\nThis is pretty inefficient, as it will create a table with as many rows as minutes there are. So more than a million, and then left join the actual data to that huge table. We end up with a table with a bunch of nulls, and only observations where there are actual measurements. But let’s move on; this is quick-and-dirty.\n\n%%sql\n\nCREATE OR REPLACE TABLE strom_minute_nulls AS\nWITH minutes_table AS (\n  SELECT UNNEST(generate_series(ts[1], ts[2], interval 1 minute)) as minute\n  FROM (VALUES (\n    [(SELECT MIN(date) FROM strom), (SELECT MAX(DATE) FROM strom)]\n  )) t(ts)\n)\nSELECT * \nFROM minutes_table\nLEFT JOIN strom\nON minutes_table.minute = strom.date\n;\nSELECT * FROM strom_minute_nulls ORDER BY minute LIMIT 100;\n\nAnd now we just interpolate the consumption per minute, filling the nulls with the next non-null value (i.e. the consumption is constant in all the measurement period -all the minutes between one measurement and the other-). TODO: this uses a correlated subquery. Look for a better solution https://dba.stackexchange.com/questions/279039/how-to-get-the-last-non-null-value-that-came-before-the-current-row\n\n%%sql\n\nCREATE OR REPLACE TABLE strom_minute AS\nSELECT\n  minute,\n  date,\n  value,\n  minutes,\n  consumption,\n  CASE \n    WHEN cm IS NULL THEN\n      (SELECT cm \n      FROM strom_minute_nulls t2 \n      WHERE t2.minute &gt; t1.minute and cm is not null \n      ORDER BY minute\n      LIMIT 1)\n    else cm \n  END AS cm\nFROM strom_minute_nulls t1\nORDER BY t1.minute\n;\nSELECT * FROM strom_minute ORDER BY minute LIMIT 100;\n\nIt turns out DuckDB already implements ignore nulls, so we can rewrite the query above, to run much more efficiently, like this\n\n%%sql\n\nCREATE OR REPLACE TABLE strom_minute AS\nSELECT\n  minute,\n  date,\n  value,\n  minutes,\n  consumption,\n  FIRST_VALUE(cm IGNORE NULLS) OVER(\n    ORDER BY minute ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING \n  ) AS cm\nFROM strom_minute_nulls t1\nORDER BY t1.minute\n;\nSELECT * FROM strom_minute ORDER BY minute LIMIT 100;\n\n\n%%sql\n\ntoy &lt;&lt; SELECT * FROM strom_minute ORDER BY minute LIMIT 7000;\n\nNow we can simply aggregate per day and hour, and the average will be correct, as all the rows have comparable units (consumption for one minute, with equal weight).\n\n%%sql\n\nconsumption_hour_avg &lt;&lt; SELECT \n  COUNT(*) AS cnt,\n  hour(minute) AS hour, \n  AVG(cm)*60*24*365 AS consumption\nFROM strom_minute\nGROUP BY hour(minute)\n;\n\n\nimport plotly.express as px\nfig = px.bar(consumption_hour_avg, y='consumption', x='hour')\nfig.show()\n\nOk, good enough. But this includes a very long period without measurements, which would have the effect to smooth everything. Let’s take that chunk out to see how it looks.\n\n%%sql\n\nconsumption_hour_avg &lt;&lt; SELECT \n  hour(minute) AS hour, \n  AVG(cm)*60*24*365 AS consumption\nFROM strom_minute\nWHERE minute &lt;= '2021-05-25' OR minute &gt;= '2022-11-30'\nGROUP BY hour(minute)\n;\n\nThis looks more accurate. It still should have some smoothing going on, giving that there are still long-ish periods without measurements (a few days) and the non-systematic measurement pattern, than frequently spans more than one hour.\n\nimport plotly.express as px\nfig = px.bar(consumption_hour_avg, y='consumption', x='hour')\nfig.show()\n\n\n%%sql\n\nselect * from consumption_hour_avg;\n\n\n\nTraces plot\n\n%%sql\n\nCREATE OR REPLACE TABLE toy AS\nWITH hourly_average AS (\n  SELECT\n    hour(minute) AS hour, \n    AVG(1.0 * 60 * 24 * cm) AS cmha\n  FROM strom_minute\n  -- This was originally here, because we wanted to see the hourly variation\n  -- and keeping this long period without measurements just smoothed things\n  -- but for waermestrom, it has a misleading implication: since we have \n  -- measurements mostly in the wintertime, the average without this period\n  -- is high, reflecting the higher energy consumption during winter\n  WHERE minute &lt;= '2021-05-25' OR minute &gt;= '2022-11-30'\n  GROUP BY hour(minute)\n),\nlast_measurements AS (\n  SELECT \n    minute,\n    date,\n    1.0 * 60 * 24 * cm AS cm,\n    AVG(1.0 * 60 * 24 * cm) OVER(\n      ORDER BY minute ROWS BETWEEN 60*4 PRECEDING AND CURRENT ROW\n    ) AS cmma\n  FROM strom_minute\n  WHERE minute &gt;= '2022-11-30'\n)\nSELECT *, CASE WHEN date IS NOT NULL THEN cm ELSE NULL END AS cmdate\nFROM last_measurements\nLEFT JOIN hourly_average\nON hour(last_measurements.minute) = hourly_average.hour\n;\n\n\n%%sql \n\ntoy &lt;&lt; SELECT * FROM toy;\n\n\nimport plotly.graph_objects as go\nfig = px.area(toy, x='minute', y='cmha')\nfig.add_trace(go.Scatter(\n  x=toy['minute'], y=toy['cmma'], mode='lines', showlegend=False\n))\nfig.add_trace(go.Scatter(\n  x=toy['date'], y=toy['cmdate'], mode='markers', showlegend=False\n))\n\n# Add range slider\nfig.update_layout(\n    xaxis=dict(\n        rangeselector=dict(\n            buttons=list([\n                dict(count=1,\n                     label=\"1m\",\n                     step=\"month\",\n                     stepmode=\"backward\"),\n                dict(count=7,\n                     label=\"7d\",\n                     step=\"day\",\n                     stepmode=\"backward\"),\n                dict(count=15,\n                     label=\"15d\",\n                     step=\"day\",\n                     stepmode=\"backward\"),\n                dict(step=\"all\")\n            ])\n        ),\n        rangeslider=dict(\n            visible=True\n        ),\n        type=\"date\"\n    )\n)\n\nfig.show()\n\nTODO: fix the range\n\n%%sql\n\n--SELECT * FROM strom_minute LIMIT 10;\nSELECT COUNT(*) FROM strom_minute;\n\n%sql toy &lt;&lt; SELECT * FROM strom_minute WHERE year(minute) &gt;= 2022 AND month(minute) &gt; 11;\n%%sql\ntoy &lt;&lt; SELECT *, ‘H’||hour(minute) AS hour FROM strom_minute WHERE minute &lt;= ‘2021-05-25’ OR minute &gt;= ‘2022-11-30’ ;\npx.line(toy, x=‘minute’, y=‘cm’)\npx.histogram(toy, x=‘cm’)\n%%sql SELECT COUNT(*) FROM strom WHERE date IS NOT NULL ;\nSELECT COUNT(*) FROM strom_minute WHERE date IS NOT NULL ;\nSELECT * FROM strom_minute LIMIT 10 OFFSET 1000;\nimport pandas as pd\nminute = pd.date_range( start=min(strom_df[‘date’]), end=max(strom_df[‘date’]), freq=‘min’ ) minute_df = pd.DataFrame(dict(date = minute)) minute_df = minute_df.merge(strom_df, on=‘date’, how=‘left’) minute_df[‘day’] = minute_df[‘date’].dt.date minute_df[‘hour’] = minute_df[‘date’].dt.hour minute_df[‘minute’] = minute_df[‘date’].dt.minute\nhour_df = minute_df.groupby([‘day’, ‘hour’]).agg({‘value’: [‘max’]})\nhour_df = minute_df.groupby([‘day’, ‘hour’]).agg({‘value’: [‘max’], ‘minutes’: ‘sum’}) hour_df = minute_df.groupby([‘day’, ‘hour’]).agg({‘value’: [‘max’], ‘minutes’: ‘sum’})\nfig = px.scatter(hour_df, x=‘index’, y=‘consumption_per_day’) fig.show()\nhttps://www.rstudio.com/blog/6-productivity-hacks-for-quarto/#write-verbatim-code-chunks-with-echo-fenced\nhttps://quarto.org/docs/computations/execution-options.html",
    "crumbs": [
      "Dashboard",
      "Process",
      "Strom"
    ]
  },
  {
    "objectID": "Pool/pool_temp.html",
    "href": "Pool/pool_temp.html",
    "title": "Pool",
    "section": "",
    "text": "###```{python} import os from epyfun.fs import convert_to_utf8\n\nExample usage:\n#file_path = ‘./data/pooltemp/Outdoor_log_from_20230701_to_20230727.csv’ #encoding = convert_to_utf8(file_path, “interim/pooltemp”) #print(f”The file encoding was: {encoding}“)\ninput_directory = ‘./data/pooltemp’ for filename in os.listdir(input_directory): if filename.endswith(‘.csv’): input_file = os.path.join(input_directory, filename) convert_to_utf8(input_file, ‘interim/pooltemp/’ + filename) ###```\n###{python} %load_ext sql %config SqlMagic.autopandas = True %config SqlMagic.feedback = False %config SqlMagic.displaycon = False ###\n###{python} %sql duckdb:///:default: # %sql duckdb:///:memory: # %sql duckdb:///path/to/file.db ###\n\n\nIngest\n###```{python} %%sql\nCREATE OR REPLACE VIEW pooltemp AS SELECT  FROM read_csv( ’interim/pooltemp/.csv’, columns = {‘time’: ‘TIMESTAMP’, ‘temp’: ‘DOUBLE’}, decimal_separator = ‘,’, delim = ’, filename = True, header = True, skip = 1 ) ; ###```\n###{python} #%sqlcmd profile -t pooltemp; #this does not seem to be working, it does not work on saved queries,  #nor if I create the view, nor if I create the table ###\n###```{python} %%sql\nCREATE OR REPLACE MACRO add_dups_count(_srctbl, _cols) AS TABLE (SELECT , COUNT() OVER (PARTITION BY _cols) AS _cnt FROM pooltemp) –TODO: figure out how to pass a table name to macros in DuckDB –FROM _srctbl ; ###```\n###```{python} %%sql\nSELECT * FROM add_dups_count(pooltemp, time); ###```\n###```{python} %%sql\nCREATE OR REPLACE MACRO count_dups_by(_srctbl, _cols) AS TABLE SELECT COUNT(*) AS _tot, COUNT(*) FILTER(WHERE _cnt &gt; 1) AS _dups, COUNT(DISTINCT _cols) AS _uniq, _dups - (_tot - _uniq) AS _duniq, _dups / _duniq AS _puniq FROM add_dups_count(_srctbl, _cols) ; ###```\n###```{python} %%sql\nSELECT * FROM count_dups_by(pooltemp, (time, temp)); ###```\n###```{python} %%sql –save pooltemp_clean\npooltemp_clean =&lt;&lt; SELECT time, avg(temp) AS temp FROM pooltemp GROUP BY time ORDER BY time ; ###```\n###{python} pooltemp_clean ###\n###```{python} import plotly.express as px\nimport pandas as pd import numpy as np\nfig = px.scatter(pooltemp_clean, x=“time”, y=“temp”, render_mode=‘webgl’) fig.update_traces(marker_line=dict(width=1, color=‘DarkSlateGray’)) fig.show() ###```\n%%timeit %%sql SELECT COUNT(DISTINCT (time, temp)), COUNT(*) FROM pooltemp ;\n%%timeit %%sql SELECT COUNT() FROM ( SELECT time, COUNT() FROM pooltemp GROUP BY time HAVING COUNT(*) &gt; 1\n) ;\n%%timeit %%sql –save duplicates_detail\nSELECT  FROM ( SELECT , COUNT() OVER (PARTITION BY time) AS cnt COUNT() OVER (PARTITION BY time, temp) AS cnt2 FROM pooltemp ) WHERE cnt &gt; 1 – AND cnt &lt;&gt; cnt2 ORDER BY -cnt, time ;\n%%sql\nSELECT COUNT(*), COUNT(DISTINCT time),\nCOUNT(DISTINCT (time, temp))\nFROM duplicates_detail ;\n%%timeit %%sql\nSELECT time, avg(temp) AS temp FROM pooltemp GROUP BY time ORDER BY time\n%%sql\nCREATE OR REPLACE MACRO count_uniq_by(cols) AS TABLE SELECT COUNT(DISTINCT cols) FROM pooltemp",
    "crumbs": [
      "Dashboard",
      "Pool",
      "Pool"
    ]
  },
  {
    "objectID": "05_model_waermestrom/05_99_compare_models.html",
    "href": "05_model_waermestrom/05_99_compare_models.html",
    "title": "Compare Models",
    "section": "",
    "text": "Cross-validation messages\n\n\n\n♻️  stepit 'cross_validate_pipe': is up-to-date. Using cached result for `strom.modelling.cross_validate_pipe()` 2025-11-22 16:20:43\n\n♻️  stepit 'cross_validate_pipe': is up-to-date. Using cached result for `strom.modelling.cross_validate_pipe()` 2025-11-22 16:20:43\n\n♻️  stepit 'cross_validate_pipe': is up-to-date. Using cached result for `strom.modelling.cross_validate_pipe()` 2025-11-22 16:20:43\n\n\n\n\n\n\n\n\nMetrics based on the test set of the single split\n\nLine plotRadar plot\n\n\n\n\n                            \n                                            \n\n\n\n\n\n\n                            \n                                            \n\n\n\n\n\n\n\n\n\nLine plotRadar plot\n\n\n\n\n                            \n                                            \n\n\n\n\n\n\n                            \n                                            \n\n\n\n\n\n\nMAEMSERMSER2MAPEEVSMeAED2Pinball\n\n\n                            \n                                            \n\n\n                            \n                                            \n\n\n                            \n                                            \n\n\n                            \n                                            \n\n\n                            \n                                            \n\n\n                            \n                                            \n\n\n                            \n                                            \n\n\n                            \n                                            \n\n\n                            \n                                            \n\n\n\n\n\n\n\n\n\n                            \n                                            \n\n\nnext\n\n\n                            \n                                            \n\n\n\n\n\n\n\n                            \n                                            \n\n\n\n\n\n\n\n                            \n                                            \n\n\n\n\n\n\nc20adab48250658\n\n\nPipeline(steps=[('vars', ColumnSelector(columns=['tf_std_mean'])),\n                ('polynomial', PolynomialFeatures(degree=4)),\n                ('model', LinearRegression())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFitted\n        \n            \n                Parameters\n                \n\n\n\n\nsteps \n[('vars', ...), ('polynomial', ...), ...]\n\n\n\ntransform_input \nNone\n\n\n\nmemory \nNone\n\n\n\nverbose \nFalse\n\n\n\n\n            \n        \n    ColumnSelector\n        \n            \n                Parameters\n                \n\n\n\n\ncolumns \n['tf_std_mean']\n\n\n\n\n            \n        \n    PolynomialFeatures?Documentation for PolynomialFeatures\n        \n            \n                Parameters\n                \n\n\n\n\ndegree \n4\n\n\n\ninteraction_only \nFalse\n\n\n\ninclude_bias \nTrue\n\n\n\norder \n'C'\n\n\n\n\n            \n        \n    LinearRegression?Documentation for LinearRegression\n        \n            \n                Parameters\n                \n\n\n\n\nfit_intercept \nTrue\n\n\n\ncopy_X \nTrue\n\n\n\ntol \n1e-06\n\n\n\nn_jobs \nNone\n\n\n\npositive \nFalse\n\n\n\n\n            \n        \n    \n\n\nPipeline(steps=[('vars', ColumnSelector(columns=['rf_tu_mean', 'vp_std_mean'])),\n                ('model',\n                 GradientBoostingRegressor(max_depth=5, min_samples_leaf=5,\n                                           min_samples_split=48,\n                                           n_estimators=60, random_state=7,\n                                           subsample=1))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFitted\n        \n            \n                Parameters\n                \n\n\n\n\nsteps \n[('vars', ...), ('model', ...)]\n\n\n\ntransform_input \nNone\n\n\n\nmemory \nNone\n\n\n\nverbose \nFalse\n\n\n\n\n            \n        \n    ColumnSelector\n        \n            \n                Parameters\n                \n\n\n\n\ncolumns \n['rf_tu_mean', 'vp_std_mean']\n\n\n\n\n            \n        \n    GradientBoostingRegressor?Documentation for GradientBoostingRegressor\n        \n            \n                Parameters\n                \n\n\n\n\nloss \n'squared_error'\n\n\n\nlearning_rate \n0.1\n\n\n\nn_estimators \n60\n\n\n\nsubsample \n1\n\n\n\ncriterion \n'friedman_mse'\n\n\n\nmin_samples_split \n48\n\n\n\nmin_samples_leaf \n5\n\n\n\nmin_weight_fraction_leaf \n0.0\n\n\n\nmax_depth \n5\n\n\n\nmin_impurity_decrease \n0.0\n\n\n\ninit \nNone\n\n\n\nrandom_state \n7\n\n\n\nmax_features \nNone\n\n\n\nalpha \n0.9\n\n\n\nverbose \n0\n\n\n\nmax_leaf_nodes \nNone\n\n\n\nwarm_start \nFalse\n\n\n\nvalidation_fraction \n0.1\n\n\n\nn_iter_no_change \nNone\n\n\n\ntol \n0.0001\n\n\n\nccp_alpha \n0.0\n\n\n\n\n            \n        \n    \n\n\nPipeline(steps=[('vars',\n                 ColumnSelector(columns=['tt_tu_mean', 'td_mean', 'vp_std_mean',\n                                         'tf_std_mean'])),\n                ('model', LinearSVR(random_state=7))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFitted\n        \n            \n                Parameters\n                \n\n\n\n\nsteps \n[('vars', ...), ('model', ...)]\n\n\n\ntransform_input \nNone\n\n\n\nmemory \nNone\n\n\n\nverbose \nFalse\n\n\n\n\n            \n        \n    ColumnSelector\n        \n            \n                Parameters\n                \n\n\n\n\ncolumns \n['tt_tu_mean', 'td_mean', ...]\n\n\n\n\n            \n        \n    LinearSVR?Documentation for LinearSVR\n        \n            \n                Parameters\n                \n\n\n\n\nepsilon \n0.0\n\n\n\ntol \n0.0001\n\n\n\nC \n1.0\n\n\n\nloss \n'epsilon_insensitive'\n\n\n\nfit_intercept \nTrue\n\n\n\nintercept_scaling \n1.0\n\n\n\ndual \n'auto'\n\n\n\nverbose \n0\n\n\n\nrandom_state \n7\n\n\n\nmax_iter \n1000",
    "crumbs": [
      "Dashboard",
      "Modelling",
      "Compare Models"
    ]
  },
  {
    "objectID": "05_model_waermestrom/05_99_compare_models.html#metrics",
    "href": "05_model_waermestrom/05_99_compare_models.html#metrics",
    "title": "Compare Models",
    "section": "",
    "text": "Metrics based on the test set of the single split\n\nLine plotRadar plot\n\n\n\n\n                            \n                                            \n\n\n\n\n\n\n                            \n                                            \n\n\n\n\n\n\n\n\n\nLine plotRadar plot\n\n\n\n\n                            \n                                            \n\n\n\n\n\n\n                            \n                                            \n\n\n\n\n\n\nMAEMSERMSER2MAPEEVSMeAED2Pinball",
    "crumbs": [
      "Dashboard",
      "Modelling",
      "Compare Models"
    ]
  },
  {
    "objectID": "05_model_waermestrom/05_99_compare_models.html#predictions-residuals-observed",
    "href": "05_model_waermestrom/05_99_compare_models.html#predictions-residuals-observed",
    "title": "Compare Models",
    "section": "",
    "text": "next",
    "crumbs": [
      "Dashboard",
      "Modelling",
      "Compare Models"
    ]
  },
  {
    "objectID": "05_model_waermestrom/05_99_compare_models.html#model-details",
    "href": "05_model_waermestrom/05_99_compare_models.html#model-details",
    "title": "Compare Models",
    "section": "",
    "text": "c20adab48250658\n\n\nPipeline(steps=[('vars', ColumnSelector(columns=['tf_std_mean'])),\n                ('polynomial', PolynomialFeatures(degree=4)),\n                ('model', LinearRegression())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFitted\n        \n            \n                Parameters\n                \n\n\n\n\nsteps \n[('vars', ...), ('polynomial', ...), ...]\n\n\n\ntransform_input \nNone\n\n\n\nmemory \nNone\n\n\n\nverbose \nFalse\n\n\n\n\n            \n        \n    ColumnSelector\n        \n            \n                Parameters\n                \n\n\n\n\ncolumns \n['tf_std_mean']\n\n\n\n\n            \n        \n    PolynomialFeatures?Documentation for PolynomialFeatures\n        \n            \n                Parameters\n                \n\n\n\n\ndegree \n4\n\n\n\ninteraction_only \nFalse\n\n\n\ninclude_bias \nTrue\n\n\n\norder \n'C'\n\n\n\n\n            \n        \n    LinearRegression?Documentation for LinearRegression\n        \n            \n                Parameters\n                \n\n\n\n\nfit_intercept \nTrue\n\n\n\ncopy_X \nTrue\n\n\n\ntol \n1e-06\n\n\n\nn_jobs \nNone\n\n\n\npositive \nFalse\n\n\n\n\n            \n        \n    \n\n\nPipeline(steps=[('vars', ColumnSelector(columns=['rf_tu_mean', 'vp_std_mean'])),\n                ('model',\n                 GradientBoostingRegressor(max_depth=5, min_samples_leaf=5,\n                                           min_samples_split=48,\n                                           n_estimators=60, random_state=7,\n                                           subsample=1))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFitted\n        \n            \n                Parameters\n                \n\n\n\n\nsteps \n[('vars', ...), ('model', ...)]\n\n\n\ntransform_input \nNone\n\n\n\nmemory \nNone\n\n\n\nverbose \nFalse\n\n\n\n\n            \n        \n    ColumnSelector\n        \n            \n                Parameters\n                \n\n\n\n\ncolumns \n['rf_tu_mean', 'vp_std_mean']\n\n\n\n\n            \n        \n    GradientBoostingRegressor?Documentation for GradientBoostingRegressor\n        \n            \n                Parameters\n                \n\n\n\n\nloss \n'squared_error'\n\n\n\nlearning_rate \n0.1\n\n\n\nn_estimators \n60\n\n\n\nsubsample \n1\n\n\n\ncriterion \n'friedman_mse'\n\n\n\nmin_samples_split \n48\n\n\n\nmin_samples_leaf \n5\n\n\n\nmin_weight_fraction_leaf \n0.0\n\n\n\nmax_depth \n5\n\n\n\nmin_impurity_decrease \n0.0\n\n\n\ninit \nNone\n\n\n\nrandom_state \n7\n\n\n\nmax_features \nNone\n\n\n\nalpha \n0.9\n\n\n\nverbose \n0\n\n\n\nmax_leaf_nodes \nNone\n\n\n\nwarm_start \nFalse\n\n\n\nvalidation_fraction \n0.1\n\n\n\nn_iter_no_change \nNone\n\n\n\ntol \n0.0001\n\n\n\nccp_alpha \n0.0\n\n\n\n\n            \n        \n    \n\n\nPipeline(steps=[('vars',\n                 ColumnSelector(columns=['tt_tu_mean', 'td_mean', 'vp_std_mean',\n                                         'tf_std_mean'])),\n                ('model', LinearSVR(random_state=7))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFitted\n        \n            \n                Parameters\n                \n\n\n\n\nsteps \n[('vars', ...), ('model', ...)]\n\n\n\ntransform_input \nNone\n\n\n\nmemory \nNone\n\n\n\nverbose \nFalse\n\n\n\n\n            \n        \n    ColumnSelector\n        \n            \n                Parameters\n                \n\n\n\n\ncolumns \n['tt_tu_mean', 'td_mean', ...]\n\n\n\n\n            \n        \n    LinearSVR?Documentation for LinearSVR\n        \n            \n                Parameters\n                \n\n\n\n\nepsilon \n0.0\n\n\n\ntol \n0.0001\n\n\n\nC \n1.0\n\n\n\nloss \n'epsilon_insensitive'\n\n\n\nfit_intercept \nTrue\n\n\n\nintercept_scaling \n1.0\n\n\n\ndual \n'auto'\n\n\n\nverbose \n0\n\n\n\nrandom_state \n7\n\n\n\nmax_iter \n1000",
    "crumbs": [
      "Dashboard",
      "Modelling",
      "Compare Models"
    ]
  },
  {
    "objectID": "05_model_waermestrom/05_06_svm.html",
    "href": "05_model_waermestrom/05_06_svm.html",
    "title": "SVM",
    "section": "",
    "text": "Before moving forward with the to-do list, let’s throw a Random Forest to it.\n\nSVM\nFor many reasons, Random Forest is usually a very good baseline model. In this particular case I started with the polynomial OLS as baseline model, just because it was so evident from the correlations that the relationship between temperature and consumption follows a polynomial shape. But let’s go back to a beloved RF.\n\n\n/home/runner/work/strom/strom/.venv/lib/python3.10/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning:\n\nLiblinear failed to converge, increase the number of iterations.\n\nModel Cards provide a framework for transparent, responsible reporting. \n Use the vetiver `.qmd` Quarto template as a place to start, \n with vetiver.model_card()\nWriting pin:\nName: 'wd-svm'\nVersion: 20251122T162020Z-1869e\n\n\n\n\n\n♻️  stepit 'svm_raw': is up-to-date. Using cached result for `strom.modelling.assess_model()` 2025-11-22 16:20:20\n\n\n\n\n\nMetrics\n\nPlotTable\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSingle Split\nCV\n\n\n\ntrain\ntest\ntest\ntrain\n\n\n\n\nMAE - Mean Absolute Error\n2.432282\n2.419931\n2.673258\n3.329209\n\n\nMSE - Mean Squared Error\n18.289605\n22.503780\n24.400576\n23.520673\n\n\nRMSE - Root Mean Squared Error\n4.276635\n4.743815\n3.990919\n4.776235\n\n\nR2 - Coefficient of Determination\n0.805274\n0.749650\n0.301124\n0.760764\n\n\nMAPE - Mean Absolute Percentage Error\n0.193486\n0.200061\n0.203410\n0.343579\n\n\nEVS - Explained Variance Score\n0.824572\n0.789548\n0.611400\n0.820277\n\n\nMeAE - Median Absolute Error\n1.566992\n1.308510\n1.987998\n2.657292\n\n\nD2 - D2 Absolute Error Score\n0.648420\n0.643352\n0.214133\n0.531369\n\n\nPinball - Mean Pinball Loss\n1.216141\n1.209965\n1.336629\n1.664605\n\n\n\n\n\n\n\n\n\n\n\n\nScatter plot matrix\n\nTestTrain\n\n\n\n\n                            \n                                            \n\n\n\n\n\n\n                            \n                                            \n\n\n\n\n\n\n\nObserved vs. Predicted and Residuals vs. Predicted\n\n\nCheck for …\n\n\ncheck the residuals to assess the goodness of fit.\n\nwhite noise or is there a pattern?\nheteroscedasticity?\nnon-linearity?\n\n\n\n\nTestTrain\n\n\n\n\n                            \n                                            \n\n\n\n\n\n\n                            \n                                            \n\n\n\n\n\n\n\nNormality of Residuals:\n\n\nCheck for …\n\n\n\nAre residuals normally distributed?\n\n\n\n\nTestTrain\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTestTrain\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLeverage\n\nTestTrain\n\n\n\n\n\n\n\n\n\n\n\nScale-Location plot\n\nTestTrain\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResiduals Autocorrelation Plot\n\nTestTrain\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResiduals vs Time\n\nTestTrain\n\n\n\n\n                            \n                                            \n\n\n\n\n\n\n                            \n                                            \n\n\n\n\n\nWell, not that bad, but it is overfitting quite a lot.\n\n\n\n♻️  stepit 'grid_search_pipe': is up-to-date. Using cached result for `strom.modelling.grid_search_pipe()` 2025-11-22 16:20:24\n\nModel Cards provide a framework for transparent, responsible reporting. \n\n Use the vetiver `.qmd` Quarto template as a place to start, \n\n with vetiver.model_card()\n\nWriting pin:\n\nName: 'wd-svm'\n\nVersion: 20251122T162024Z-50658\n\n\n\n\n\n\n\n♻️  stepit 'svm_tuned': is up-to-date. Using cached result for `strom.modelling.assess_model()` 2025-11-22 16:20:24\n\n\n\n\n\n\nMetrics\n\nPlotTable\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSingle Split\nCV\n\n\n\ntrain\ntest\ntest\ntrain\n\n\n\n\nMAE - Mean Absolute Error\n2.247125\n2.106274\n2.047927\n2.378226\n\n\nMSE - Mean Squared Error\n16.223860\n19.443720\n19.789512\n16.932822\n\n\nRMSE - Root Mean Squared Error\n4.027885\n4.409503\n3.365674\n4.111345\n\n\nR2 - Coefficient of Determination\n0.827268\n0.783693\n0.583568\n0.826061\n\n\nMAPE - Mean Absolute Percentage Error\n0.184460\n0.203015\n0.152578\n0.185927\n\n\nEVS - Explained Variance Score\n0.827974\n0.791971\n0.657677\n0.827940\n\n\nMeAE - Median Absolute Error\n1.332851\n1.088117\n1.345858\n1.470077\n\n\nD2 - D2 Absolute Error Score\n0.675184\n0.689578\n0.402628\n0.663515\n\n\nPinball - Mean Pinball Loss\n1.123562\n1.053137\n1.023964\n1.189113\n\n\n\n\n\n\n\n\n\n\n\n\nScatter plot matrix\n\nTestTrain\n\n\n\n\n                            \n                                            \n\n\n\n\n\n\n                            \n                                            \n\n\n\n\n\n\n\nObserved vs. Predicted and Residuals vs. Predicted\n\n\nCheck for …\n\n\ncheck the residuals to assess the goodness of fit.\n\nwhite noise or is there a pattern?\nheteroscedasticity?\nnon-linearity?\n\n\n\n\nTestTrain\n\n\n\n\n                            \n                                            \n\n\n\n\n\n\n                            \n                                            \n\n\n\n\n\n\n\nNormality of Residuals:\n\n\nCheck for …\n\n\n\nAre residuals normally distributed?\n\n\n\n\nTestTrain\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTestTrain\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLeverage\n\nTestTrain\n\n\n\n\n\n\n\n\n\n\n\nScale-Location plot\n\nTestTrain\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResiduals Autocorrelation Plot\n\nTestTrain\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResiduals vs Time\n\nTestTrain\n\n\n\n\n                            \n                                            \n\n\n\n\n\n\n                            \n                                            \n\n\n\n\n\n\n\n\nTODOs",
    "crumbs": [
      "Dashboard",
      "Modelling",
      "SVM"
    ]
  },
  {
    "objectID": "05_model_waermestrom/05_04_random_forest.html",
    "href": "05_model_waermestrom/05_04_random_forest.html",
    "title": "Random forest",
    "section": "",
    "text": "Before moving forward with the to-do list, let’s throw a Random Forest to it.",
    "crumbs": [
      "Dashboard",
      "Modelling",
      "Random forest"
    ]
  },
  {
    "objectID": "05_model_waermestrom/05_04_random_forest.html#best-model",
    "href": "05_model_waermestrom/05_04_random_forest.html#best-model",
    "title": "Random forest",
    "section": "Best model",
    "text": "Best model",
    "crumbs": [
      "Dashboard",
      "Modelling",
      "Random forest"
    ]
  },
  {
    "objectID": "05_model_waermestrom/05_02_baseline.html",
    "href": "05_model_waermestrom/05_02_baseline.html",
    "title": "Baseline Model",
    "section": "",
    "text": "From the first look at the correlations, a model with polynomials looked fitting. So let’s here fit such a model, that would be the model to beat later down the line.\n\nPolynomial Model\nHere we stick to a simple OLS, but using a polynomial specification for the predictors temperature and air humidity, thereby, addressing one of the TODOs listed after the naive model\n\n\n\n♻️  stepit 'baseline': is up-to-date. Using cached result for `strom.modelling.assess_model()` 2025-11-22 16:19:17\n\n\n\n\n\nMetrics\n\nPlotTable\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSingle Split\nCV\n\n\n\ntrain\ntest\ntest\ntrain\n\n\n\n\nMAE - Mean Absolute Error\n2.198831\n2.247932\n2.103037\n2.310954\n\n\nMSE - Mean Squared Error\n13.273005\n18.946938\n18.170196\n13.350332\n\n\nRMSE - Root Mean Squared Error\n3.643214\n4.352808\n3.433667\n3.646162\n\n\nR2 - Coefficient of Determination\n0.858685\n0.789219\n0.498846\n0.863026\n\n\nMAPE - Mean Absolute Percentage Error\n0.203516\n0.243536\n0.175590\n0.208486\n\n\nEVS - Explained Variance Score\n0.858685\n0.793689\n0.564325\n0.863026\n\n\nMeAE - Median Absolute Error\n1.443200\n1.346771\n1.513959\n1.505330\n\n\nD2 - D2 Absolute Error Score\n0.682165\n0.668701\n0.341624\n0.672995\n\n\nPinball - Mean Pinball Loss\n1.099415\n1.123966\n1.051519\n1.155477\n\n\n\n\n\n\n\n\n\n\n\n\nScatter plot matrix\n\nTestTrain\n\n\n\n\n                            \n                                            \n\n\n\n\n\n\n                            \n                                            \n\n\n\n\n\n\n\nObserved vs. Predicted and Residuals vs. Predicted\n\n\nCheck for …\n\n\ncheck the residuals to assess the goodness of fit.\n\nwhite noise or is there a pattern?\nheteroscedasticity?\nnon-linearity?\n\n\n\n\nTestTrain\n\n\n\n\n                            \n                                            \n\n\n\n\n\n\n                            \n                                            \n\n\n\n\n\n\n\nNormality of Residuals:\n\n\nCheck for …\n\n\n\nAre residuals normally distributed?\n\n\n\n\nTestTrain\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTestTrain\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLeverage\n\nTestTrain\n\n\n\n\n\n\n\n\n\n\n\nScale-Location plot\n\nTestTrain\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResiduals Autocorrelation Plot\n\nTestTrain\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResiduals vs Time\n\nTestTrain\n\n\n\n\n                            \n                                            \n\n\n\n\n\n\n                            \n                                            \n\n\n\n\n\n\n\n\nTODOs\nSubstantial improvement here, we still have some items in the to-do list and given these results, I would add a coulpe more:\n\nCheck the degree of the polynomial\ndepite the much better fit, there are still some -although minor- heteroscedaticity, mostly associated to the lowest temperatures where the variance of strom use is higher. Here we can try other models that better handle that long tail, and also, try transformations.\nthere might be also a seasonal pattern, because we are not accounting for, for example, weekends and so on. Here we can see if the seasonal trend decomposition can help.\nfinally, there is still some evidence of autocorrelation. Which kinda makes sense: it is not the same to have one day with a very low temperature compared to a whole week with very low temperatures: while the former could be wintered with the heat already produced, as long as there is not much energy lose, the latter would certainly need much more work from the wärmepumpe to keep fighting the cold. In that sense, it seems unrealistic to just assume that the increase energy consumption of several days of extreme cold is just the sum of the daily increases. So if the one-day-increase due to extreme cold is 10, five consecutve days would probably result in an increase greater than 10*5=50, probably much greater. But let’s explore such hipothesis in the data and try to accomodate that in the model using lags, or moving average of the last few days.\nrun some PCA, with min, max mean and so on and think about a Cumulative Explained Variance plot https://archive.is/X1wrZ#selection-1059.8-1059.37 PCA seems sensible, because several climatic variables are highly correlated, but still could have different signals. E.g. temperature, the mean seems to capture well the process, but beyond that, how much it varies min, max, std dev, seems to have some predictive power.\nperhaps experiment with feature selection.\nand consider log-transforming the target, after all, the distribution of it is right-skewed with relatively long right tail\nConsider models to handle censored data, since the consumption can never be negative. This, being a linear model, can still predict negative values, even though the polynomial show a rather good fit. But there is no restriction and in fact, in the test set, in the warm period with low consumption, there are a few instances in which the model ends up predicting negative values. So we would need to take that into account and add some sort of constraint for that.\nHandle the outliers. These are really just a couple of points, but at least in these kind of model, they end up having a high leverage. So, we could just handle them by replacing them with locally-defined averages, or use models that are robust to those.\n\n\n\nPolynomials but using statsmodels …\n\n\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                     wd   R-squared:                       0.851\nModel:                            OLS   Adj. R-squared:                  0.849\nMethod:                 Least Squares   F-statistic:                     646.1\nDate:                Sat, 22 Nov 2025   Prob (F-statistic):               0.00\nTime:                        16:19:21   Log-Likelihood:                -3446.0\nNo. Observations:                1260   AIC:                             6916.\nDf Residuals:                    1248   BIC:                             6978.\nDf Model:                          11                                         \nCovariance Type:            nonrobust                                         \n=======================================================================================\n                          coef    std err          t      P&gt;|t|      [0.025      0.975]\n---------------------------------------------------------------------------------------\nIntercept              26.2928      2.718      9.673      0.000      20.960      31.625\ntt_tu_mean             -1.9251      0.093    -20.690      0.000      -2.108      -1.743\nI(tt_tu_mean ** 2)      0.1332      0.009     14.603      0.000       0.115       0.151\nI(tt_tu_mean ** 3)     -0.0091      0.002     -6.015      0.000      -0.012      -0.006\nI(tt_tu_mean ** 4)      0.0004      0.000      3.570      0.000       0.000       0.001\nI(tt_tu_mean ** 5)  -6.981e-06   2.64e-06     -2.643      0.008   -1.22e-05    -1.8e-06\nrf_tu_mean              0.0384      0.034      1.143      0.253      -0.028       0.104\nrf_tu_min              -0.0148      0.018     -0.840      0.401      -0.049       0.020\nrf_tu_max              -0.0480      0.040     -1.202      0.229      -0.126       0.030\ntt_tu_mean.shift(1)    -0.2579      0.075     -3.452      0.001      -0.405      -0.111\ntt_tu_mean.shift(2)     0.0446      0.073      0.609      0.542      -0.099       0.188\ntt_tu_mean.shift(3)    -0.1108      0.050     -2.227      0.026      -0.208      -0.013\n==============================================================================\nOmnibus:                     1303.569   Durbin-Watson:                   0.896\nProb(Omnibus):                  0.000   Jarque-Bera (JB):           125086.373\nSkew:                           4.750   Prob(JB):                         0.00\nKurtosis:                      50.878   Cond. No.                     4.85e+07\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 4.85e+07. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\nModel Cards provide a framework for transparent, responsible reporting. \n Use the vetiver `.qmd` Quarto template as a place to start, \n with vetiver.model_card()\nWriting pin:\nName: 'waermestrom'\nVersion: 20251122T161921Z-c9956\n\n\n&lt;vetiver.vetiver_model.VetiverModel at 0x7f16428508b0&gt;\n\n\nhere’s also the polynomial model, but using scikit-learn\n\n\n0.8451947680230854",
    "crumbs": [
      "Dashboard",
      "Modelling",
      "Baseline Model"
    ]
  },
  {
    "objectID": "04_climate/04_05_outliers_multivariate.html",
    "href": "04_climate/04_05_outliers_multivariate.html",
    "title": "Outliers Multivariate",
    "section": "",
    "text": "Prophet-based outlier detection, multivariate\n\n\n                            \n                                            \n\n\n/home/runner/work/strom/strom/.venv/lib/python3.10/site-packages/plotly/io/_json.py:558: UserWarning:\n\nDiscarding nonzero nanoseconds in conversion.\n\n\n\n                            \n                                            \n\n\nAdding these two relevant predictors -without much feature-engineering really-, substantially improves the fit and reduces the uncertainty band. This could actually be useful for outlier detection. Only concern I had, I would have expected the uncertainty band to be wider in the cold season (due to extreme values and higher variability). Additional concern, it can predict negative values, so we sould add some contraint about that.\n\n\n                            \n                                            \n\n\n\n\n                            \n                                            \n\n\nYeah, I think this is actually better. Still, the thing about the uncertainty during the cold periods being not so big, I think it is concerning. It end up detecting some outliers that are probably legit. It also may end up failing to catch some outliers in the warm period.\n\n\nTODOs\n\nPerhaps fine-tune this multivariate model?",
    "crumbs": [
      "Dashboard",
      "Strom and Climate data",
      "Outliers Multivariate"
    ]
  },
  {
    "objectID": "04_climate/04_02_trend_season_waermestrom.html",
    "href": "04_climate/04_02_trend_season_waermestrom.html",
    "title": "Seasonal-Trend decomposition: Wärmestrom",
    "section": "",
    "text": "This is the init_notebook_mode cell from ITables v2.5.2\n(you should not see this message - is your notebook trusted?)",
    "crumbs": [
      "Dashboard",
      "Strom and Climate data",
      "Seasonal-Trend decomposition: Wärmestrom"
    ]
  },
  {
    "objectID": "04_climate/04_02_trend_season_waermestrom.html#seasonal-trend-decomposition-using-loess-stl",
    "href": "04_climate/04_02_trend_season_waermestrom.html#seasonal-trend-decomposition-using-loess-stl",
    "title": "Seasonal-Trend decomposition: Wärmestrom",
    "section": "Seasonal-Trend decomposition using LOESS (STL)",
    "text": "Seasonal-Trend decomposition using LOESS (STL)\nQuick and dirty STL. Probably not much insight from it, because the serie is too short to really capture the main seasonality pattern (e.g. year, heizperiode/sommer)\n\n\n\n\n\nSTL-7\n\n\n\n\nIt seems to show some pattern, although that changes in time. Let’s see the raw data per week.\n\n\n                            \n                                            \n\n\nWell, on average, very small changes (~ 1 Kwh), somewhat higher on wednesday and the weekend, and somewhat lower on Thursday and Friday. But this is a very weak signal amid a lot of noise, comming from the changes through the year.\nNow let’s see the raw data within a month.\n\n\n                            \n                                            \n\n\nAgain, weak signal and a lot of noise. Some pattern in there, but apparently also strongly influenced by outliers.\nAnd finally, take a look at the seasonality that should mostly drive the game here.\n\n\n                            \n                                            \n\n\nAs expected here you can clearly see the pattern. The serie is but too short to pass year seasonality to STL, but, yeah, perhaps 90 days at least can try to approximate the heating-period vs. warmer period. Let’s see\n\n\n\n\n\n\n\n\n\nDoes the week signal for the shorter periods justify using MSTL here?",
    "crumbs": [
      "Dashboard",
      "Strom and Climate data",
      "Seasonal-Trend decomposition: Wärmestrom"
    ]
  },
  {
    "objectID": "04_climate/04_02_trend_season_waermestrom.html#multiple-seasonal-trend-decomposition-using-loess-mstl",
    "href": "04_climate/04_02_trend_season_waermestrom.html#multiple-seasonal-trend-decomposition-using-loess-mstl",
    "title": "Seasonal-Trend decomposition: Wärmestrom",
    "section": "Multiple Seasonal-Trend decomposition using LOESS (MSTL)",
    "text": "Multiple Seasonal-Trend decomposition using LOESS (MSTL)\n\n\n\n\n\n\n\n\nValidate periods\nAssess and compare fit",
    "crumbs": [
      "Dashboard",
      "Strom and Climate data",
      "Seasonal-Trend decomposition: Wärmestrom"
    ]
  },
  {
    "objectID": "04_climate/04_02_trend_season_waermestrom.html#prophet",
    "href": "04_climate/04_02_trend_season_waermestrom.html#prophet",
    "title": "Seasonal-Trend decomposition: Wärmestrom",
    "section": "Prophet",
    "text": "Prophet\nProphet shoud make it pretty straightforward to do something similar.\n\n\n                            \n                                            \n\n\n                            \n                                            \n\n\nOk, that’s not bad. But even though I increased the trend flexibility, it tends to underfit and smooth out the short-term variations, that are probably legit (e.g. due to temperature changes within cold season)\n\n\n\n\n    \n    \n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n    Loading ITables v2.5.2 from the init_notebook_mode cell...\n    (need help?)\n    \n\n\n\n\n\n{'changepoint_prior_scale': 0.1, 'seasonality_prior_scale': 0.01}",
    "crumbs": [
      "Dashboard",
      "Strom and Climate data",
      "Seasonal-Trend decomposition: Wärmestrom"
    ]
  },
  {
    "objectID": "04_climate/04_01_climate_and_strom.html",
    "href": "04_climate/04_01_climate_and_strom.html",
    "title": "Correlations",
    "section": "",
    "text": "Mean values of climate variables\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                            \n                                            \n\n\n\n\nMin values of climate variables\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                            \n                                            \n\n\n\n\nMax values of climate variables\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                            \n                                            \n\n\n\n\nMedian values of climate variables\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                            \n                                            \n\n\n\n\nStd. deviation of climate variables\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                            \n                                            \n\n\nsome first-look insights\n\nit’s good that the correlation between normal and warme strom consumption is very low. It is expected, and could be useful to use normal strom utilization as a proxy for other things (i.e. being at home basically)\nalso as expected, the correlation between normal strom utilization and temprature is low.\nthere is a nice and strong negative correlation between temperature and wärmestrom consumption. It is not strictly linear though. Rather polinomial (inverse) relatioship.\nthere seems to be some sort of weak positive correlation between wärmestrom and relative humidity. Not at all consistent though. It is positive with min and mean humidity, but negative or non-existent with max humidity. It may well be just an artifact or spurious correlation, explained by temperature, because there is indeed a correlation (not that strong, though) between temperature and humidity.\nthe number of observations are somewhat related to the temperature, which kinda makes sense, because the measurements are not random. I am much more likely to measure the thing when it’s cold!, and that seems to be reflected in the data.",
    "crumbs": [
      "Dashboard",
      "Strom and Climate data",
      "Correlations"
    ]
  },
  {
    "objectID": "02_waermestrom_results.html",
    "href": "02_waermestrom_results.html",
    "title": "Wärmestrom",
    "section": "",
    "text": "Note on how to calculate consumption\n\n\nNote that minute-level granularity makes it possible to calculate simple (unweighted) averages of the consumption per minute cm to extrapolate to periods. Thus, for a whole period (e.g. one day fully extended to minute-level) summing and averaging cm (times minutes in the period) are equivalent.\n\n\n\n\n\n\n\n\n\naveraging_cm\nsumming_cm\n\n\n\n\n0\n8.524196\n8.524196\n\n\n\n\n\n\n\nBut be careful, when dealing with wärmestrom, you actually have two different meters, so in averaging, the denominator will be twice the number of minutes, and you would need to account for that.\n\n\n\n\n\n\n\n\n\naveraging_cm\nsumming_cm\n\n\n\n\n0\n1.962985\n3.925969\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\naveraging_cm\nsumming_cm\n\n\n\n\n0\n3.925969\n3.925969\n\n\n\n\n\n\n\n\n\n\nWärmestrom average consumption\n\n\n\n\n\n\n\n\n\nMin\nMax\nFirst\nLast\nMins\nUse/Day\ncheck\nUse/Year\nYearly Exp\n\n\n\n\n0\n0\n18233\n2016-07-04 07:50:00\n2025-11-21 06:33:00\n4934803\n11.893737\n11.893739\n4341.213846\n1690.468672\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMin\nMax\nFirst\nLast\nMins\nUse/Day\ncheck\nUse/Year\nYearly Exp\n\n\n\n\n0\n0\n18233\n2016-07-04 07:50:00\n2025-11-21 06:33:00\n4934803\n12.687737\n6.343870\n4631.023996\n1803.320744\n\n\n1\n0\n16617\n2016-07-04 07:50:00\n2025-11-21 06:33:00\n4934803\n11.099736\n5.549869\n4051.403696\n1577.616599\n\n\n\n\n\n\n\n\n\npro Jahr\n\n\n\n\n\n\n\n\n\n\n\nyear\nMin\nMax\nFirst\nLast\nMins\nUse/Day\ncheck\nUse/Year\nYearly Exp\n\n\n\n\n0\n2016\n0\n547\n2016-07-04 07:50:00\n2016-11-28 15:23:00\n212133\n8.278452\n1.015309e+01\n3021.635045\n1176.624687\n\n\n1\n2017\n2677\n3347\n2017-11-25 15:20:00\n2017-11-25 15:23:00\n3\n13.673585\n2.395612e+06\n4990.858591\n1943.440335\n\n\n2\n2018\n4893\n5747\n2018-11-23 15:19:00\n2018-11-23 15:22:00\n3\n12.654172\n2.217011e+06\n4618.772826\n1798.550138\n\n\n3\n2019\n7224\n7914\n2019-11-29 14:18:00\n2019-11-29 14:22:00\n4\n12.037153\n1.581682e+06\n4393.560865\n1710.852601\n\n\n4\n2020\n9456\n10106\n2020-11-30 07:07:00\n2020-12-31 22:21:00\n45554\n12.091597\n1.398945e+02\n4413.432961\n1718.590795\n\n\n5\n2021\n9820\n11972\n2021-01-01 01:18:00\n2021-12-02 18:15:00\n483417\n11.440832\n1.243916e+01\n4175.903660\n1626.096885\n\n\n6\n2022\n13738\n14805\n2022-11-30 14:54:00\n2022-12-31 18:13:00\n44839\n13.182348\n1.545227e+02\n4811.556908\n1873.620260\n\n\n7\n2023\n14104\n17093\n2023-01-01 00:41:00\n2023-12-31 17:33:00\n525172\n10.825559\n1.083438e+01\n3951.329187\n1538.647585\n\n\n8\n2024\n0\n18233\n2024-01-01 01:03:00\n2024-12-31 23:57:00\n526974\n11.683155\n1.168462e+01\n4264.351688\n1660.538547\n\n\n9\n2025\n1399\n4730\n2025-01-01 08:19:00\n2025-11-21 06:33:00\n466454\n11.163314\n1.117528e+01\n4074.609664\n1586.653003\n\n\n\n\n\n\n\n\n\n\n\nWärmestrom consumption per day\n\n\n                            \n                                            \n\n\nWärmestrom zeigt natürlich das saisonale Muster. Es gibt einen hohen und volatilen Verbrauch in der Heizperiode und einen niedrigeren, weniger volatilen Verbrauch in den wärmeren Monaten. Offensichtlich beträgt der normale Verbrauch ohne Heizung etwa 3-4 kWh. Das ist wahrscheinlich nur für Warmwasser. Der Verbrauch an Urlaubstagen beträgt 2,69 kWh. Ist das einfach, um das Wasser warm zu halten?\nDer Verbrauch in der Heizperiode liegt zwischen 12 und bis zu 50 kWh. Vermutlich stark abhängig von der Außentemperatur.\nOverlay temperature Was ist am 25-26 Juli passiert?, warum im sommer solcher hoher verbreauch?, Wartung?\n\n\nHoch und Niedrig tarif\nPreis unterschied zwischen Hoch- und Niedrigtarif ist nicht so groß. 1 o 2 cent. Zub Beispiel, für 2024, es wird 27,63 fürs Hochtarif und 26,29 fürs Niedrigtarif.\nat durchschnitliche Verbrauch von 12 Kwh pro Tag, das Kostet 3.31 Euro in der Hochtarif und 3.15 Euro in der Niedrigtarif. Die Jährliche durschschnit Verbrauch von 4,400 Kwh heist 1,215 Euro vs. 1157 Euro pro jahr. ALso, wenig al 60 Euro Unterschied im Jahr, wenn alle Verbrauch in einen Tarif wäre.\nAlso, es ist nich viel relevant.\nTrotzdem, schau ma moi wir war der verlauf von denen. But let’s aggregate at weekly level; the daily plot just basically shows that the weekends its only niedrig.",
    "crumbs": [
      "Dashboard",
      "Wärmestrom"
    ]
  },
  {
    "objectID": "01_strom_results.html",
    "href": "01_strom_results.html",
    "title": "Strom",
    "section": "",
    "text": "Stromverbrauch im Durchschnitt\n\n\n\n\n\n\n\n\n\nMin\nMax\nFirst\nLast\nMins\nUse/Day\ncheck\nUse/Year\nYearly Exp\n\n\n\n\n0\n0\n23128\n2016-07-04 07:50:00\n2025-11-21 06:33:00\n4934803\n8.16707\n8.167072\n2980.980508\n1160.79381\n\n\n\n\n\n\n\n\n\npro Jahr\n\n\n\n\n\n\n\n\n\n\n\nyear\nMin\nMax\nFirst\nLast\nMins\nUse/Day\ncheck\nUse/Year\nYearly Exp\n\n\n\n\n0\n2016\n0\n659\n2016-07-04 07:50:00\n2016-11-28 16:16:00\n212186\n4.677272\n5.734996\n1707.204170\n664.785304\n\n\n1\n2017\n2679\n2679\n2017-11-25 15:15:00\n2017-11-25 15:15:00\n0\n5.764550\ninf\n2104.060678\n819.321228\n\n\n2\n2018\n5389\n5389\n2018-11-25 14:15:00\n2018-11-25 14:15:00\n0\n7.593902\ninf\n2771.774100\n1079.328835\n\n\n3\n2019\n8752\n8752\n2019-11-29 14:14:00\n2019-11-29 14:14:00\n0\n9.130284\ninf\n3332.553530\n1297.696345\n\n\n4\n2020\n12162\n12473\n2020-11-30 07:07:00\n2020-12-31 22:22:00\n45555\n9.344818\n108.113109\n3410.858443\n1328.188278\n\n\n5\n2021\n12474\n15340\n2021-01-01 01:18:00\n2021-12-02 18:14:00\n483416\n8.545174\n9.290846\n3118.988613\n1214.534166\n\n\n6\n2022\n18474\n18787\n2022-11-30 14:54:00\n2022-12-31 18:13:00\n44839\n8.761719\n102.704326\n3198.027261\n1245.311815\n\n\n7\n2023\n18791\n22020\n2023-01-01 00:41:00\n2023-12-31 17:33:00\n525172\n8.861831\n8.869053\n3234.568245\n1259.540874\n\n\n8\n2024\n0\n23128\n2024-01-01 01:04:00\n2024-12-31 23:57:00\n526973\n8.685481\n8.686586\n3170.200696\n1234.476151\n\n\n9\n2025\n2077\n4860\n2025-01-01 08:20:00\n2025-11-21 06:33:00\n466453\n8.585330\n8.594551\n3133.645520\n1220.241565\n\n\n\n\n\n\n\n\n\n\n\nStrom consumption per day\n\n\n                            \n                                            \n\n\nSo, an normalen Tagen beträgt der Stromverbrauch etwa 10-11 kWh. An Urlaubstagen liegt er bei 2,85 kWh. Also, man kann sagen, es sind 3 kWh. Das entspricht wahrscheinlich hauptsächlich dem Stromverbrauch des Kühlschranks, …, und was noch? Keine Ahnung. Aber gut, 10 - 11 kWh bedeuten bei aktuellen 0,38 Cent pro kWh, dass der Stromverbrauch etwa 4 Euro pro Tag beträgt. Im Urlaub, bei 2,85-3 kWh zu 38 Cent pro kWh, sind es ungefähr 1 Euro pro Tag. Das heißt, 10 Tage Urlaub sparen etwa 30 Euro (3 Euro pro Tag).\n\n\n\n\n\n\n\n\n\nmeterid\ndate\nvalue\nfirst\nminutes\nconsumption\ncm\n\n\n\n\n0\n1\n2025-11-21 06:33:00\n4860\n0\n630\n2\n0.003175\n\n\n1\n1\n2025-11-20 20:03:00\n4858\n0\n3680\n23\n0.006250\n\n\n2\n1\n2025-11-18 06:43:00\n4835\n0\n1463\n7\n0.004785\n\n\n3\n1\n2025-11-17 06:20:00\n4828\n0\n611\n2\n0.003273\n\n\n4\n1\n2025-11-16 20:09:00\n4826\n0\n739\n12\n0.016238\n\n\n5\n1\n2025-11-16 07:50:00\n4814\n0\n569\n2\n0.003515\n\n\n6\n1\n2025-11-15 22:21:00\n4812\n0\n805\n4\n0.004969\n\n\n7\n1\n2025-11-15 08:56:00\n4808\n0\n761\n3\n0.003942\n\n\n8\n1\n2025-11-14 20:15:00\n4805\n0\n130\n2\n0.015385\n\n\n9\n1\n2025-11-14 18:05:00\n4803\n0\n380\n3\n0.007895\n\n\n\n\n\n\n\n\n\nConsumption per week",
    "crumbs": [
      "Dashboard",
      "Strom"
    ]
  },
  {
    "objectID": "03_periods.html",
    "href": "03_periods.html",
    "title": "Periods",
    "section": "",
    "text": "%%sql\n\nCREATE OR REPLACE TABLE periods (\n    name VARCHAR,\n    begin TIMESTAMP,\n    fin TIMESTAMP\n);\n\nINSERT INTO periods (name, begin, fin)\nVALUES\n    ('18 grad', '2023-10-24 22:49:00', '2023-10-27 22:49:00'),\n    ('20 grad', '2023-10-27 22:49:00', '2023-10-30 22:49:00')\n;\n\n\n%%sql\n\nperiods &lt;&lt; SELECT\n  p.name,\n  p.begin,\n  p.fin,\n  24.0 * 60.0 * AVG(d.cm) AS cd\nFROM periods p\nJOIN waermestrom_minute d\nON d.minute BETWEEN p.begin AND p.fin\nGROUP BY p.name, p.begin, p.fin\n;\n\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nfig = px.bar(strom_per_day, y='cd', x='date')\nfig.update_xaxes(\n    rangeslider_visible=True,\n    rangeselector=dict(\n        buttons=list([\n            dict(count=15, label=\"15d\", step=\"day\", stepmode=\"backward\"),\n            dict(count=1, label=\"1m\", step=\"month\", stepmode=\"backward\"),\n            dict(count=6, label=\"6m\", step=\"month\", stepmode=\"backward\"),\n            dict(count=1, label=\"YTD\", step=\"year\", stepmode=\"todate\"),\n            dict(count=1, label=\"1y\", step=\"year\", stepmode=\"backward\"),\n            dict(step=\"all\")\n        ])\n    )\n)\nfig.update_xaxes(rangeslider_thickness = 0.1)\nfig.show()\n\n\nfor index, row in periods.iterrows():\n    fig.add_shape(\n        type='line',\n        x0=row['begin'],\n        y0=row['cd'],\n        x1=row['fin'],\n        y1=row['cd'],\n        line=dict(color='Red'),\n        xref='x',\n        yref='y'\n    )\nfig.show()\n\n\nperiods\n\nwe have to brind the temp data and other climatic variables here",
    "crumbs": [
      "Dashboard",
      "Periods"
    ]
  },
  {
    "objectID": "04_climate/04_02_trend_season_normalstrom.html",
    "href": "04_climate/04_02_trend_season_normalstrom.html",
    "title": "Seasonal-Trend decomposition: Normal Strom",
    "section": "",
    "text": "Again, let’s see the raw data per week.\n\n\n                            \n                                            \n\n\nSo, again, there does not seem to be a weekly pattern, nor a weekend thing going on.\nNow let’s see the raw data within a month.\n\n\n                            \n                                            \n\n\nMore variation, but again, no discernable pattern.\nYear-month changes, perhaps something related to December where consumption could increase due to christmas lights?, and sommer due to the pool?\n\n\n                            \n                                            \n\n\nWell, small differences though.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProphet shoud make it pretty straightforward to do something similar.\n\n\n                            \n                                            \n\n\n/home/runner/work/strom/strom/.venv/lib/python3.10/site-packages/plotly/io/_json.py:558: UserWarning:\n\nDiscarding nonzero nanoseconds in conversion.\n\n\n\n                            \n                                            \n\n\nOk. Not worth it exploring further, or fine-tuning this.",
    "crumbs": [
      "Dashboard",
      "Strom and Climate data",
      "Seasonal-Trend decomposition: Normal Strom"
    ]
  },
  {
    "objectID": "04_climate/04_02_trend_season_normalstrom.html#seasonal-trend-decomposition-using-loess-stl",
    "href": "04_climate/04_02_trend_season_normalstrom.html#seasonal-trend-decomposition-using-loess-stl",
    "title": "Seasonal-Trend decomposition: Normal Strom",
    "section": "",
    "text": "Again, let’s see the raw data per week.\n\n\n                            \n                                            \n\n\nSo, again, there does not seem to be a weekly pattern, nor a weekend thing going on.\nNow let’s see the raw data within a month.\n\n\n                            \n                                            \n\n\nMore variation, but again, no discernable pattern.\nYear-month changes, perhaps something related to December where consumption could increase due to christmas lights?, and sommer due to the pool?\n\n\n                            \n                                            \n\n\nWell, small differences though.",
    "crumbs": [
      "Dashboard",
      "Strom and Climate data",
      "Seasonal-Trend decomposition: Normal Strom"
    ]
  },
  {
    "objectID": "04_climate/04_02_trend_season_normalstrom.html#prophet",
    "href": "04_climate/04_02_trend_season_normalstrom.html#prophet",
    "title": "Seasonal-Trend decomposition: Normal Strom",
    "section": "",
    "text": "Prophet shoud make it pretty straightforward to do something similar.\n\n\n                            \n                                            \n\n\n/home/runner/work/strom/strom/.venv/lib/python3.10/site-packages/plotly/io/_json.py:558: UserWarning:\n\nDiscarding nonzero nanoseconds in conversion.\n\n\n\n                            \n                                            \n\n\nOk. Not worth it exploring further, or fine-tuning this.",
    "crumbs": [
      "Dashboard",
      "Strom and Climate data",
      "Seasonal-Trend decomposition: Normal Strom"
    ]
  },
  {
    "objectID": "04_climate/04_03_outliers.html#z-score",
    "href": "04_climate/04_03_outliers.html#z-score",
    "title": "Outliers",
    "section": "Z score",
    "text": "Z score\n\n\n\n\n\n\n\n\n\ndate\nnd\nwd\nnt\nht\nobs\ntt_tu_min\ntt_tu_max\ntt_tu_mean\ntt_tu_median\n...\nrs_ind_mean\nrs_ind_median\nrs_ind_std\nwrtr_min\nwrtr_max\nwrtr_mean\nwrtr_median\nwrtr_std\n_merge\nwd_z\n\n\n\n\n1652\n2021-01-11\n9.413296\n50.236627\n20.267084\n29.969543\n4.0\n-10.7\n-2.4\n-6.797917\n-6.80\n...\n0.000000\n0.0\n0.000000\nNaN\nNaN\nNaN\nNaN\nNaN\nboth\n3.910129\n\n\n1656\n2021-01-15\n8.286358\n41.735698\n12.286358\n29.449339\n4.0\n-11.1\n-2.6\n-6.541667\n-6.15\n...\n0.020833\n0.0\n0.144338\nNaN\nNaN\nNaN\nNaN\nNaN\nboth\n3.031216\n\n\n1683\n2021-02-11\n8.751573\n59.208399\n21.817963\n37.390437\n4.0\n-12.4\n-4.5\n-8.064583\n-8.00\n...\n0.229167\n0.0\n0.424744\nNaN\nNaN\nNaN\nNaN\nNaN\nboth\n4.837723\n\n\n1684\n2021-02-12\n10.626823\n64.885871\n23.812605\n41.073266\n3.0\n-15.0\n-3.0\n-8.837500\n-8.50\n...\n0.000000\n0.0\n0.000000\nNaN\nNaN\nNaN\nNaN\nNaN\nboth\n5.424718\n\n\n1685\n2021-02-13\n12.659750\n60.386958\n58.232787\n2.154171\n3.0\n-13.4\n-2.9\n-8.031250\n-8.70\n...\n0.000000\n0.0\n0.000000\nNaN\nNaN\nNaN\nNaN\nNaN\nboth\n4.959575\n\n\n1686\n2021-02-14\n7.790416\n51.393652\n51.258327\n0.135325\n4.0\n-14.2\n0.7\n-7.366667\n-7.65\n...\n0.000000\n0.0\n0.000000\nNaN\nNaN\nNaN\nNaN\nNaN\nboth\n4.029754\n\n\n2352\n2022-12-12\n10.483619\n55.413417\n23.588144\n31.825273\n0.0\n-12.2\n-0.2\n-6.552083\n-6.45\n...\n0.000000\n0.0\n0.000000\nNaN\nNaN\nNaN\nNaN\nNaN\nboth\n4.445359\n\n\n2353\n2022-12-13\n10.483619\n55.413417\n23.588144\n31.825273\n0.0\n-15.1\n-4.1\n-8.937500\n-8.80\n...\n0.000000\n0.0\n0.000000\nNaN\nNaN\nNaN\nNaN\nNaN\nboth\n4.445359\n\n\n2357\n2022-12-17\n9.732795\n44.614500\n40.742659\n3.871841\n1.0\n-7.1\n-1.3\n-3.133333\n-2.70\n...\n0.083333\n0.0\n0.279310\nNaN\nNaN\nNaN\nNaN\nNaN\nboth\n3.328856\n\n\n2358\n2022-12-18\n9.573892\n57.289325\n56.609472\n0.679853\n1.0\n-8.2\n-0.6\n-4.979167\n-5.35\n...\n0.000000\n0.0\n0.000000\nNaN\nNaN\nNaN\nNaN\nNaN\nboth\n4.639310\n\n\n2745\n2024-01-09\n9.708277\n42.242300\n12.162534\n30.079766\n5.0\n-6.5\n-4.3\n-5.506250\n-5.50\n...\n0.000000\n0.0\n0.000000\nNaN\nNaN\nNaN\nNaN\nNaN\nboth\n3.083594\n\n\n2746\n2024-01-10\n6.645277\n41.582167\n15.946443\n25.635724\n4.0\n-7.7\n-1.2\n-5.337500\n-5.65\n...\n0.000000\n0.0\n0.000000\nNaN\nNaN\nNaN\nNaN\nNaN\nboth\n3.015342\n\n\n2749\n2024-01-13\n12.093837\n41.894842\n41.894842\n0.000000\n2.0\n-7.9\n-0.5\n-4.202083\n-4.15\n...\n0.000000\n0.0\n0.000000\nNaN\nNaN\nNaN\nNaN\nNaN\nboth\n3.047670\n\n\n2753\n2024-01-17\n8.293448\n78.472398\n27.005158\n51.467241\n2.0\n-7.7\n3.7\n-1.081250\n-0.30\n...\n0.208333\n0.0\n0.410414\nNaN\nNaN\nNaN\nNaN\nNaN\nboth\n6.829433\n\n\n2756\n2024-01-20\n8.135308\n43.717538\n42.186335\n1.531202\n3.0\n-11.0\n-0.2\n-6.272917\n-7.20\n...\n0.000000\n0.0\n0.000000\nNaN\nNaN\nNaN\nNaN\nNaN\nboth\n3.236119\n\n\n3097\n2024-12-26\n9.066030\n44.944476\n44.944476\n0.000000\n6.0\n-7.9\n-0.5\n-4.900000\n-5.05\n...\n0.000000\n0.0\n0.000000\nNaN\nNaN\nNaN\nNaN\nNaN\nboth\n3.362972\n\n\n3101\n2024-12-30\n7.049364\n45.245536\n16.109633\n29.135903\n4.0\n-7.1\n-0.2\n-4.233333\n-4.95\n...\n0.000000\n0.0\n0.000000\nNaN\nNaN\nNaN\nNaN\nNaN\nboth\n3.394099\n\n\n3102\n2024-12-31\n10.732171\n41.449659\n18.585562\n22.864097\n6.0\n-7.7\n0.4\n-4.175000\n-4.40\n...\n0.000000\n0.0\n0.000000\nNaN\nNaN\nNaN\nNaN\nNaN\nboth\n3.001642\n\n\n3106\n2025-01-04\n5.903743\n46.109890\n45.229434\n0.880455\n1.0\n-8.6\n-2.2\n-5.245833\n-4.65\n...\n0.000000\n0.0\n0.000000\nNaN\nNaN\nNaN\nNaN\nNaN\nboth\n3.483465\n\n\n3116\n2025-01-14\n8.003773\n42.891896\n16.238845\n26.653051\n3.0\n-8.8\n-0.1\n-4.795833\n-5.35\n...\n0.000000\n0.0\n0.000000\nNaN\nNaN\nNaN\nNaN\nNaN\nboth\n3.150756\n\n\n3121\n2025-01-19\n11.454509\n42.858147\n42.858147\n0.000000\n2.0\n-5.1\n7.2\n-0.562500\n-0.45\n...\n0.000000\n0.0\n0.000000\nNaN\nNaN\nNaN\nNaN\nNaN\nboth\n3.147266\n\n\n3413\n2025-11-07\n10.195547\n51.275826\n31.870871\n19.404954\n4.0\n-1.0\n8.4\n2.258333\n1.50\n...\n0.000000\n0.0\n0.000000\nNaN\nNaN\nNaN\nNaN\nNaN\nboth\n4.017572\n\n\n3414\n2025-11-08\n10.756562\n49.397565\n48.633630\n0.763934\n7.0\n-1.6\n5.9\n2.708696\n4.20\n...\n0.000000\n0.0\n0.000000\nNaN\nNaN\nNaN\nNaN\nNaN\nboth\n3.823378\n\n\n3415\n2025-11-09\n11.125413\n44.946610\n44.946610\n0.000000\n3.0\n4.2\n8.4\n5.987500\n6.00\n...\n0.500000\n0.5\n0.510754\nNaN\nNaN\nNaN\nNaN\nNaN\nboth\n3.363193\n\n\n3416\n2025-11-10\n11.320897\n49.463393\n12.114932\n37.348460\n4.0\n2.0\n10.2\n6.070833\n5.95\n...\n0.250000\n0.0\n0.442326\nNaN\nNaN\nNaN\nNaN\nNaN\nboth\n3.830184\n\n\n\n\n25 rows × 73 columns",
    "crumbs": [
      "Dashboard",
      "Strom and Climate data",
      "Outliers"
    ]
  },
  {
    "objectID": "04_climate/04_03_outliers.html#z-score-1",
    "href": "04_climate/04_03_outliers.html#z-score-1",
    "title": "Outliers",
    "section": "Z score",
    "text": "Z score\n\n\n\n\n\n\n\n\n\ndate\nnd\nwd\nnt\nht\nobs\ntt_tu_min\ntt_tu_max\ntt_tu_mean\ntt_tu_median\n...\nrs_ind_median\nrs_ind_std\nwrtr_min\nwrtr_max\nwrtr_mean\nwrtr_median\nwrtr_std\n_merge\nwd_z\nwd_zr\n\n\n\n\n1652\n2021-01-11\n9.413296\n50.236627\n20.267084\n29.969543\n4.0\n-10.7\n-2.4\n-6.797917\n-6.80\n...\n0.0\n0.000000\nNaN\nNaN\nNaN\nNaN\nNaN\nboth\n3.910129\n7.288129\n\n\n1683\n2021-02-11\n8.751573\n59.208399\n21.817963\n37.390437\n4.0\n-12.4\n-4.5\n-8.064583\n-8.00\n...\n0.0\n0.424744\nNaN\nNaN\nNaN\nNaN\nNaN\nboth\n4.837723\n8.904646\n\n\n1684\n2021-02-12\n10.626823\n64.885871\n23.812605\n41.073266\n3.0\n-15.0\n-3.0\n-8.837500\n-8.50\n...\n0.0\n0.000000\nNaN\nNaN\nNaN\nNaN\nNaN\nboth\n5.424718\n9.927601\n\n\n1685\n2021-02-13\n12.659750\n60.386958\n58.232787\n2.154171\n3.0\n-13.4\n-2.9\n-8.031250\n-8.70\n...\n0.0\n0.000000\nNaN\nNaN\nNaN\nNaN\nNaN\nboth\n4.959575\n9.116996\n\n\n1686\n2021-02-14\n7.790416\n51.393652\n51.258327\n0.135325\n4.0\n-14.2\n0.7\n-7.366667\n-7.65\n...\n0.0\n0.000000\nNaN\nNaN\nNaN\nNaN\nNaN\nboth\n4.029754\n7.496599\n\n\n2352\n2022-12-12\n10.483619\n55.413417\n23.588144\n31.825273\n0.0\n-12.2\n-0.2\n-6.552083\n-6.45\n...\n0.0\n0.000000\nNaN\nNaN\nNaN\nNaN\nNaN\nboth\n4.445359\n8.220873\n\n\n2353\n2022-12-13\n10.483619\n55.413417\n23.588144\n31.825273\n0.0\n-15.1\n-4.1\n-8.937500\n-8.80\n...\n0.0\n0.000000\nNaN\nNaN\nNaN\nNaN\nNaN\nboth\n4.445359\n8.220873\n\n\n2358\n2022-12-18\n9.573892\n57.289325\n56.609472\n0.679853\n1.0\n-8.2\n-0.6\n-4.979167\n-5.35\n...\n0.0\n0.000000\nNaN\nNaN\nNaN\nNaN\nNaN\nboth\n4.639310\n8.558870\n\n\n2753\n2024-01-17\n8.293448\n78.472398\n27.005158\n51.467241\n2.0\n-7.7\n3.7\n-1.081250\n-0.30\n...\n0.0\n0.410414\nNaN\nNaN\nNaN\nNaN\nNaN\nboth\n6.829433\n12.375596\n\n\n3413\n2025-11-07\n10.195547\n51.275826\n31.870871\n19.404954\n4.0\n-1.0\n8.4\n2.258333\n1.50\n...\n0.0\n0.000000\nNaN\nNaN\nNaN\nNaN\nNaN\nboth\n4.017572\n7.475370\n\n\n3414\n2025-11-08\n10.756562\n49.397565\n48.633630\n0.763934\n7.0\n-1.6\n5.9\n2.708696\n4.20\n...\n0.0\n0.000000\nNaN\nNaN\nNaN\nNaN\nNaN\nboth\n3.823378\n7.136948\n\n\n3416\n2025-11-10\n11.320897\n49.463393\n12.114932\n37.348460\n4.0\n2.0\n10.2\n6.070833\n5.95\n...\n0.0\n0.442326\nNaN\nNaN\nNaN\nNaN\nNaN\nboth\n3.830184\n7.148809\n\n\n\n\n12 rows × 74 columns\n\n\n\nThis is of course naive and catches many allegedly legit observations.",
    "crumbs": [
      "Dashboard",
      "Strom and Climate data",
      "Outliers"
    ]
  },
  {
    "objectID": "04_climate/04_03_outliers.html#mstl",
    "href": "04_climate/04_03_outliers.html#mstl",
    "title": "Outliers",
    "section": "MSTL",
    "text": "MSTL\n\n\n\n\n\n\n\n\n\n\n\n                            \n                                            \n\n\nThis seems better. It catches some observations that, looking only at the univariate distribution, may not seem like an outlier, but within the typical consumption in the season, the seem extreme.\nI’ve spotted a couple of those points already in the correlation matrix. So let’s see how do these outliers look out there.\n\n\n                            \n                                            \n\n\n\n\n                            \n                                            \n\n\nWell, a bit better than the naive approach, but still fails to detect a couple of points that, for higher temperatures show a very high consumption. Perhaps we need to resort to multivariate outlier detection.",
    "crumbs": [
      "Dashboard",
      "Strom and Climate data",
      "Outliers"
    ]
  },
  {
    "objectID": "05_model_waermestrom/05_01_naive_model.html",
    "href": "05_model_waermestrom/05_01_naive_model.html",
    "title": "Naive Model",
    "section": "",
    "text": "Naive model\nA simple OLS model with temperature and air humidity, without any transformation or feature engineering -no thinking at all-.\n\n\n\n♻️  stepit 'naive': is up-to-date. Using cached result for `strom.modelling.assess_model()` 2025-11-22 16:19:03\n\n\n\n\n\nMetrics\n\nPlotTable\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSingle Split\nCV\n\n\n\ntrain\ntest\ntest\ntrain\n\n\n\n\nMAE - Mean Absolute Error\n3.362078\n3.280748\n2.837764\n3.520149\n\n\nMSE - Mean Squared Error\n26.574541\n26.388140\n26.874432\n27.787971\n\n\nRMSE - Root Mean Squared Error\n5.155050\n5.136939\n4.280400\n5.268673\n\n\nR2 - Coefficient of Determination\n0.717066\n0.706438\n-0.746993\n0.714391\n\n\nMAPE - Mean Absolute Percentage Error\n0.365973\n0.470347\n0.277364\n0.375591\n\n\nEVS - Explained Variance Score\n0.717066\n0.719489\n-0.381325\n0.714391\n\n\nMeAE - Median Absolute Error\n2.470082\n2.379462\n1.887684\n2.609201\n\n\nD2 - D2 Absolute Error Score\n0.514021\n0.516485\n-0.041633\n0.501956\n\n\nPinball - Mean Pinball Loss\n1.681039\n1.640374\n1.418882\n1.760074\n\n\n\n\n\n\n\n\n\n\n\n\nScatter plot matrix\n\nTestTrain\n\n\n\n\n                            \n                                            \n\n\n\n\n\n\n                            \n                                            \n\n\n\n\n\n\n\nObserved vs. Predicted and Residuals vs. Predicted\n\n\nCheck for …\n\n\ncheck the residuals to assess the goodness of fit.\n\nwhite noise or is there a pattern?\nheteroscedasticity?\nnon-linearity?\n\n\n\n\nTestTrain\n\n\n\n\n                            \n                                            \n\n\n\n\n\n\n                            \n                                            \n\n\n\n\n\n\n\nNormality of Residuals:\n\n\nCheck for …\n\n\n\nAre residuals normally distributed?\n\n\n\n\nTestTrain\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTestTrain\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLeverage\n\nTestTrain\n\n\n\n\n\n\n\n\n\n\n\nScale-Location plot\n\nTestTrain\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResiduals Autocorrelation Plot\n\nTestTrain\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResiduals vs Time\n\nTestTrain\n\n\n\n\n                            \n                                            \n\n\n\n\n\n\n                            \n                                            \n\n\n\n\n\n\n\n\nTODOs\nClearly the naive model is not a good fit (as expected).\n\n\n\nListing 1\n\n\n\nIt needs a polynomial with the temperature (second order perhaps)\nCheck if the association with relative humidity, if it is just an artifact of the correlation with temperature, or if there might something meaningful going on there\nand bring other climatic data, rainfall and snowfall might be relevant\n\n\n\n\n\n\nNaive Model, but using statsmodels …\n\n\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                     wd   R-squared:                       0.719\nModel:                            OLS   Adj. R-squared:                  0.719\nMethod:                 Least Squares   F-statistic:                     1612.\nDate:                Sat, 22 Nov 2025   Prob (F-statistic):               0.00\nTime:                        16:19:07   Log-Likelihood:                -3856.0\nNo. Observations:                1263   AIC:                             7718.\nDf Residuals:                    1260   BIC:                             7733.\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         27.2790      1.297     21.033      0.000      24.735      29.823\ntt_tu_mean    -1.1223      0.021    -54.192      0.000      -1.163      -1.082\nrf_tu_mean    -0.0538      0.015     -3.540      0.000      -0.084      -0.024\n==============================================================================\nOmnibus:                      899.687   Durbin-Watson:                   0.626\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            19708.274\nSkew:                           3.033   Prob(JB):                         0.00\nKurtosis:                      21.377   Cond. No.                         723.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\nR-squared: 0.719004717391805\n\n\n\n\n\n\nNaive model, but using scikit-learn without pipeline …",
    "crumbs": [
      "Dashboard",
      "Modelling",
      "Naive Model"
    ]
  },
  {
    "objectID": "05_model_waermestrom/05_03_tune_polynomial.html",
    "href": "05_model_waermestrom/05_03_tune_polynomial.html",
    "title": "Fine tuning the polynomial Model",
    "section": "",
    "text": "Let’s address here a couple of the to-dos from the previous pages. Namely, let’s check the degree of the polynomial using a grid search and checkout other climatic variables, playing a bit with variable selection.",
    "crumbs": [
      "Dashboard",
      "Modelling",
      "Fine tuning the polynomial Model"
    ]
  },
  {
    "objectID": "05_model_waermestrom/05_03_tune_polynomial.html#assessment-of-the-best-model-in-that-brute-force-approach",
    "href": "05_model_waermestrom/05_03_tune_polynomial.html#assessment-of-the-best-model-in-that-brute-force-approach",
    "title": "Fine tuning the polynomial Model",
    "section": "Assessment of the best model in that brute force approach",
    "text": "Assessment of the best model in that brute force approach\n\n\n\n♻️  stepit 'poly': is up-to-date. Using cached result for `strom.modelling.assess_model()` 2025-11-22 16:19:33\n\n\n\n\n\nMetrics\n\nPlotTable\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    \n    \n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n    Loading ITables v2.5.2 from the init_notebook_mode cell...\n    (need help?)\n    \n\n\n\n\n\n\n\n\nScatter plot matrix\n\nTestTrain\n\n\n\n\n                            \n                                            \n\n\n\n\n\n\n                            \n                                            \n\n\n\n\n\n\n\nObserved vs. Predicted and Residuals vs. Predicted\n\n\nCheck for …\n\n\ncheck the residuals to assess the goodness of fit.\n\nwhite noise or is there a pattern?\nheteroscedasticity?\nnon-linearity?\n\n\n\n\nTestTrain\n\n\n\n\n                            \n                                            \n\n\n\n\n\n\n                            \n                                            \n\n\n\n\n\n\n\nNormality of Residuals:\n\n\nCheck for …\n\n\n\nAre residuals normally distributed?\n\n\n\n\nTestTrain\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTestTrain\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLeverage\n\nTestTrain\n\n\n\n\n\n\n\n\n\n\n\nScale-Location plot\n\nTestTrain\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResiduals Autocorrelation Plot\n\nTestTrain\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResiduals vs Time\n\nTestTrain",
    "crumbs": [
      "Dashboard",
      "Modelling",
      "Fine tuning the polynomial Model"
    ]
  },
  {
    "objectID": "05_model_waermestrom/05_03_tune_polynomial.html#metrics-1",
    "href": "05_model_waermestrom/05_03_tune_polynomial.html#metrics-1",
    "title": "Fine tuning the polynomial Model",
    "section": "Metrics",
    "text": "Metrics\n\nSingle split\nMetrics based on the test set of the single split\n\nLine plotRadar plot\n\n\n\n\n                            \n                                            \n\n\n\n\n\n\n                            \n                                            \n\n\n\n\n\n\n\nCross validation\n\nLine plotRadar plot\n\n\n\n\n                            \n                                            \n\n\n\n\n\n\n                            \n                                            \n\n\n\n\n\n\nMAEMSERMSER2MAPEEVSMeAED2Pinball",
    "crumbs": [
      "Dashboard",
      "Modelling",
      "Fine tuning the polynomial Model"
    ]
  },
  {
    "objectID": "05_model_waermestrom/05_03_tune_polynomial.html#predictions-residuals-observed",
    "href": "05_model_waermestrom/05_03_tune_polynomial.html#predictions-residuals-observed",
    "title": "Fine tuning the polynomial Model",
    "section": "Predictions, residuals, observed",
    "text": "Predictions, residuals, observed\n\n\n                            \n                                            \n\n\nnext",
    "crumbs": [
      "Dashboard",
      "Modelling",
      "Fine tuning the polynomial Model"
    ]
  },
  {
    "objectID": "05_model_waermestrom/05_03_tune_polynomial.html#time-vs.-predicted-and-observed",
    "href": "05_model_waermestrom/05_03_tune_polynomial.html#time-vs.-predicted-and-observed",
    "title": "Fine tuning the polynomial Model",
    "section": "Time vs. Predicted and Observed",
    "text": "Time vs. Predicted and Observed",
    "crumbs": [
      "Dashboard",
      "Modelling",
      "Fine tuning the polynomial Model"
    ]
  },
  {
    "objectID": "05_model_waermestrom/05_03_tune_polynomial.html#time-vs.-residuals",
    "href": "05_model_waermestrom/05_03_tune_polynomial.html#time-vs.-residuals",
    "title": "Fine tuning the polynomial Model",
    "section": "Time vs. Residuals",
    "text": "Time vs. Residuals",
    "crumbs": [
      "Dashboard",
      "Modelling",
      "Fine tuning the polynomial Model"
    ]
  },
  {
    "objectID": "05_model_waermestrom/05_03_tune_polynomial.html#model-details",
    "href": "05_model_waermestrom/05_03_tune_polynomial.html#model-details",
    "title": "Fine tuning the polynomial Model",
    "section": "Model details",
    "text": "Model details\n\n2c71577de0c20ad\n\n\nPipeline(steps=[('vars', ColumnSelector(columns=['tt_tu_mean', 'rf_tu_mean'])),\n                ('polynomial', PolynomialFeatures()),\n                ('model', LinearRegression())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFitted\n        \n            \n                Parameters\n                \n\n\n\n\nsteps \n[('vars', ...), ('polynomial', ...), ...]\n\n\n\ntransform_input \nNone\n\n\n\nmemory \nNone\n\n\n\nverbose \nFalse\n\n\n\n\n            \n        \n    ColumnSelector\n        \n            \n                Parameters\n                \n\n\n\n\ncolumns \n['tt_tu_mean', 'rf_tu_mean']\n\n\n\n\n            \n        \n    PolynomialFeatures?Documentation for PolynomialFeatures\n        \n            \n                Parameters\n                \n\n\n\n\ndegree \n2\n\n\n\ninteraction_only \nFalse\n\n\n\ninclude_bias \nTrue\n\n\n\norder \n'C'\n\n\n\n\n            \n        \n    LinearRegression?Documentation for LinearRegression\n        \n            \n                Parameters\n                \n\n\n\n\nfit_intercept \nTrue\n\n\n\ncopy_X \nTrue\n\n\n\ntol \n1e-06\n\n\n\nn_jobs \nNone\n\n\n\npositive \nFalse\n\n\n\n\n            \n        \n    \n\n\nPipeline(steps=[('vars', ColumnSelector(columns=['tt_tu_mean', 'tf_std_mean'])),\n                ('polynomial', PolynomialFeatures(degree=4)),\n                ('model', LinearRegression())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFitted\n        \n            \n                Parameters\n                \n\n\n\n\nsteps \n[('vars', ...), ('polynomial', ...), ...]\n\n\n\ntransform_input \nNone\n\n\n\nmemory \nNone\n\n\n\nverbose \nFalse\n\n\n\n\n            \n        \n    ColumnSelector\n        \n            \n                Parameters\n                \n\n\n\n\ncolumns \n['tt_tu_mean', 'tf_std_mean']\n\n\n\n\n            \n        \n    PolynomialFeatures?Documentation for PolynomialFeatures\n        \n            \n                Parameters\n                \n\n\n\n\ndegree \n4\n\n\n\ninteraction_only \nFalse\n\n\n\ninclude_bias \nTrue\n\n\n\norder \n'C'\n\n\n\n\n            \n        \n    LinearRegression?Documentation for LinearRegression\n        \n            \n                Parameters\n                \n\n\n\n\nfit_intercept \nTrue\n\n\n\ncopy_X \nTrue\n\n\n\ntol \n1e-06\n\n\n\nn_jobs \nNone\n\n\n\npositive \nFalse\n\n\n\n\n            \n        \n    \n\n\nPipeline(steps=[('vars', ColumnSelector(columns=['tf_std_mean'])),\n                ('polynomial', PolynomialFeatures(degree=4)),\n                ('model', LinearRegression())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFitted\n        \n            \n                Parameters\n                \n\n\n\n\nsteps \n[('vars', ...), ('polynomial', ...), ...]\n\n\n\ntransform_input \nNone\n\n\n\nmemory \nNone\n\n\n\nverbose \nFalse\n\n\n\n\n            \n        \n    ColumnSelector\n        \n            \n                Parameters\n                \n\n\n\n\ncolumns \n['tf_std_mean']\n\n\n\n\n            \n        \n    PolynomialFeatures?Documentation for PolynomialFeatures\n        \n            \n                Parameters\n                \n\n\n\n\ndegree \n4\n\n\n\ninteraction_only \nFalse\n\n\n\ninclude_bias \nTrue\n\n\n\norder \n'C'\n\n\n\n\n            \n        \n    LinearRegression?Documentation for LinearRegression\n        \n            \n                Parameters\n                \n\n\n\n\nfit_intercept \nTrue\n\n\n\ncopy_X \nTrue\n\n\n\ntol \n1e-06\n\n\n\nn_jobs \nNone\n\n\n\npositive \nFalse\n\n\n\n\n            \n        \n    \n\n\n\nA nice improvement over the baseline model, and the model without humidity is slightly better across all metrics (a rather small improvement, but across all metrics).",
    "crumbs": [
      "Dashboard",
      "Modelling",
      "Fine tuning the polynomial Model"
    ]
  },
  {
    "objectID": "05_model_waermestrom/05_05_gradient_boost.html",
    "href": "05_model_waermestrom/05_05_gradient_boost.html",
    "title": "Gradient Boost",
    "section": "",
    "text": "Before moving forward with the to-do list, let’s throw a Random Forest to it.",
    "crumbs": [
      "Dashboard",
      "Modelling",
      "Gradient Boost"
    ]
  },
  {
    "objectID": "05_model_waermestrom/05_05_gradient_boost.html#parameter-param_model__learning_rate",
    "href": "05_model_waermestrom/05_05_gradient_boost.html#parameter-param_model__learning_rate",
    "title": "Gradient Boost",
    "section": "Parameter: param_model__learning_rate",
    "text": "Parameter: param_model__learning_rate\n/home/runner/work/strom/strom/src/strom/strom.py:837: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['0.1' '0.1' '0.1' ... '0.1' '0.1' '0.1']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n/home/runner/work/strom/strom/src/strom/strom.py:837: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['5' '5' '5' ... '5' '5' '5']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n/home/runner/work/strom/strom/src/strom/strom.py:837: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['5' '5' '5' ... '5' '5' '5']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n/home/runner/work/strom/strom/src/strom/strom.py:837: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['48' '48' '48' ... '48' '48' '48']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n/home/runner/work/strom/strom/src/strom/strom.py:837: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['60' '60' '60' ... '60' '60' '60']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n/home/runner/work/strom/strom/src/strom/strom.py:837: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['1' '1' '1' ... '1' '1' '1']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n\nMAEMSERMSER2MAPEEVSMeAED2Pinball",
    "crumbs": [
      "Dashboard",
      "Modelling",
      "Gradient Boost"
    ]
  },
  {
    "objectID": "05_model_waermestrom/05_05_gradient_boost.html#parameter-param_model__max_depth",
    "href": "05_model_waermestrom/05_05_gradient_boost.html#parameter-param_model__max_depth",
    "title": "Gradient Boost",
    "section": "Parameter: param_model__max_depth",
    "text": "Parameter: param_model__max_depth\n/home/runner/work/strom/strom/src/strom/strom.py:837: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['0.1' '0.1' '0.1' ... '0.1' '0.1' '0.1']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n/home/runner/work/strom/strom/src/strom/strom.py:837: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['5' '5' '5' ... '5' '5' '5']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n/home/runner/work/strom/strom/src/strom/strom.py:837: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['5' '5' '5' ... '5' '5' '5']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n/home/runner/work/strom/strom/src/strom/strom.py:837: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['48' '48' '48' ... '48' '48' '48']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n/home/runner/work/strom/strom/src/strom/strom.py:837: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['60' '60' '60' ... '60' '60' '60']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n/home/runner/work/strom/strom/src/strom/strom.py:837: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['1' '1' '1' ... '1' '1' '1']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n\nMAEMSERMSER2MAPEEVSMeAED2Pinball",
    "crumbs": [
      "Dashboard",
      "Modelling",
      "Gradient Boost"
    ]
  },
  {
    "objectID": "05_model_waermestrom/05_05_gradient_boost.html#parameter-param_model__min_samples_leaf",
    "href": "05_model_waermestrom/05_05_gradient_boost.html#parameter-param_model__min_samples_leaf",
    "title": "Gradient Boost",
    "section": "Parameter: param_model__min_samples_leaf",
    "text": "Parameter: param_model__min_samples_leaf\n/home/runner/work/strom/strom/src/strom/strom.py:837: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['0.1' '0.1' '0.1' ... '0.1' '0.1' '0.1']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n/home/runner/work/strom/strom/src/strom/strom.py:837: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['5' '5' '5' ... '5' '5' '5']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n/home/runner/work/strom/strom/src/strom/strom.py:837: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['5' '5' '5' ... '5' '5' '5']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n/home/runner/work/strom/strom/src/strom/strom.py:837: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['48' '48' '48' ... '48' '48' '48']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n/home/runner/work/strom/strom/src/strom/strom.py:837: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['60' '60' '60' ... '60' '60' '60']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n/home/runner/work/strom/strom/src/strom/strom.py:837: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['1' '1' '1' ... '1' '1' '1']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n\nMAEMSERMSER2MAPEEVSMeAED2Pinball",
    "crumbs": [
      "Dashboard",
      "Modelling",
      "Gradient Boost"
    ]
  },
  {
    "objectID": "05_model_waermestrom/05_05_gradient_boost.html#parameter-param_model__min_samples_split",
    "href": "05_model_waermestrom/05_05_gradient_boost.html#parameter-param_model__min_samples_split",
    "title": "Gradient Boost",
    "section": "Parameter: param_model__min_samples_split",
    "text": "Parameter: param_model__min_samples_split\n/home/runner/work/strom/strom/src/strom/strom.py:837: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['0.1' '0.1' '0.1' ... '0.1' '0.1' '0.1']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n/home/runner/work/strom/strom/src/strom/strom.py:837: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['5' '5' '5' ... '5' '5' '5']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n/home/runner/work/strom/strom/src/strom/strom.py:837: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['5' '5' '5' ... '5' '5' '5']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n/home/runner/work/strom/strom/src/strom/strom.py:837: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['48' '48' '48' ... '48' '48' '48']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n/home/runner/work/strom/strom/src/strom/strom.py:837: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['60' '60' '60' ... '60' '60' '60']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n/home/runner/work/strom/strom/src/strom/strom.py:837: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['1' '1' '1' ... '1' '1' '1']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n\nMAEMSERMSER2MAPEEVSMeAED2Pinball",
    "crumbs": [
      "Dashboard",
      "Modelling",
      "Gradient Boost"
    ]
  },
  {
    "objectID": "05_model_waermestrom/05_05_gradient_boost.html#parameter-param_model__n_estimators",
    "href": "05_model_waermestrom/05_05_gradient_boost.html#parameter-param_model__n_estimators",
    "title": "Gradient Boost",
    "section": "Parameter: param_model__n_estimators",
    "text": "Parameter: param_model__n_estimators\n/home/runner/work/strom/strom/src/strom/strom.py:837: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['0.1' '0.1' '0.1' ... '0.1' '0.1' '0.1']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n/home/runner/work/strom/strom/src/strom/strom.py:837: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['5' '5' '5' ... '5' '5' '5']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n/home/runner/work/strom/strom/src/strom/strom.py:837: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['5' '5' '5' ... '5' '5' '5']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n/home/runner/work/strom/strom/src/strom/strom.py:837: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['48' '48' '48' ... '48' '48' '48']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n/home/runner/work/strom/strom/src/strom/strom.py:837: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['60' '60' '60' ... '60' '60' '60']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n/home/runner/work/strom/strom/src/strom/strom.py:837: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['1' '1' '1' ... '1' '1' '1']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n\nMAEMSERMSER2MAPEEVSMeAED2Pinball",
    "crumbs": [
      "Dashboard",
      "Modelling",
      "Gradient Boost"
    ]
  },
  {
    "objectID": "05_model_waermestrom/05_05_gradient_boost.html#parameter-param_model__subsample",
    "href": "05_model_waermestrom/05_05_gradient_boost.html#parameter-param_model__subsample",
    "title": "Gradient Boost",
    "section": "Parameter: param_model__subsample",
    "text": "Parameter: param_model__subsample\n/home/runner/work/strom/strom/src/strom/strom.py:837: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['0.1' '0.1' '0.1' ... '0.1' '0.1' '0.1']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n/home/runner/work/strom/strom/src/strom/strom.py:837: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['5' '5' '5' ... '5' '5' '5']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n/home/runner/work/strom/strom/src/strom/strom.py:837: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['5' '5' '5' ... '5' '5' '5']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n/home/runner/work/strom/strom/src/strom/strom.py:837: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['48' '48' '48' ... '48' '48' '48']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n/home/runner/work/strom/strom/src/strom/strom.py:837: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['60' '60' '60' ... '60' '60' '60']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n/home/runner/work/strom/strom/src/strom/strom.py:837: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['1' '1' '1' ... '1' '1' '1']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n\nMAEMSERMSER2MAPEEVSMeAED2Pinball",
    "crumbs": [
      "Dashboard",
      "Modelling",
      "Gradient Boost"
    ]
  },
  {
    "objectID": "05_model_waermestrom/05_05_gradient_boost.html#parameter-param_vars__columns",
    "href": "05_model_waermestrom/05_05_gradient_boost.html#parameter-param_vars__columns",
    "title": "Gradient Boost",
    "section": "Parameter: param_vars__columns",
    "text": "Parameter: param_vars__columns\n/home/runner/work/strom/strom/src/strom/strom.py:837: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['0.1' '0.1' '0.1' ... '0.1' '0.1' '0.1']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n/home/runner/work/strom/strom/src/strom/strom.py:837: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['5' '5' '5' ... '5' '5' '5']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n/home/runner/work/strom/strom/src/strom/strom.py:837: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['5' '5' '5' ... '5' '5' '5']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n/home/runner/work/strom/strom/src/strom/strom.py:837: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['48' '48' '48' ... '48' '48' '48']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n/home/runner/work/strom/strom/src/strom/strom.py:837: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['60' '60' '60' ... '60' '60' '60']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n/home/runner/work/strom/strom/src/strom/strom.py:837: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['1' '1' '1' ... '1' '1' '1']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n\nMAEMSERMSER2MAPEEVSMeAED2Pinball",
    "crumbs": [
      "Dashboard",
      "Modelling",
      "Gradient Boost"
    ]
  },
  {
    "objectID": "05_model_waermestrom/05_05_gradient_boost.html#best-model",
    "href": "05_model_waermestrom/05_05_gradient_boost.html#best-model",
    "title": "Gradient Boost",
    "section": "Best model",
    "text": "Best model\n\n\n{'model__learning_rate': 0.1,\n 'model__max_depth': 5,\n 'model__min_samples_leaf': 5,\n 'model__min_samples_split': 48,\n 'model__n_estimators': 60,\n 'model__subsample': 1,\n 'vars__columns': ['rf_tu_mean', 'vp_std_mean']}\n\n\n\n\n\n♻️  stepit 'gb_tuned': is up-to-date. Using cached result for `strom.modelling.assess_model()` 2025-11-22 16:20:01\n\n\n\n\n\nMetrics\n\nPlotTable\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSingle Split\nCV\n\n\n\ntrain\ntest\ntest\ntrain\n\n\n\n\nMAE - Mean Absolute Error\n1.563379\n2.374218\n2.208940\n1.548676\n\n\nMSE - Mean Squared Error\n6.409281\n21.100475\n22.067249\n6.006925\n\n\nRMSE - Root Mean Squared Error\n2.531656\n4.593525\n3.773892\n2.440925\n\n\nR2 - Coefficient of Determination\n0.931762\n0.765262\n0.450495\n0.938437\n\n\nMAPE - Mean Absolute Percentage Error\n0.141916\n0.237917\n0.170104\n0.134164\n\n\nEVS - Explained Variance Score\n0.931762\n0.771610\n0.503243\n0.938437\n\n\nMeAE - Median Absolute Error\n1.028037\n1.231788\n1.418443\n1.030482\n\n\nD2 - D2 Absolute Error Score\n0.774018\n0.650089\n0.352238\n0.780897\n\n\nPinball - Mean Pinball Loss\n0.781690\n1.187109\n1.104470\n0.774338\n\n\n\n\n\n\n\n\n\n\n\n\nScatter plot matrix\n\nTestTrain\n\n\n\n\n                            \n                                            \n\n\n\n\n\n\n                            \n                                            \n\n\n\n\n\n\n\nObserved vs. Predicted and Residuals vs. Predicted\n\n\nCheck for …\n\n\ncheck the residuals to assess the goodness of fit.\n\nwhite noise or is there a pattern?\nheteroscedasticity?\nnon-linearity?\n\n\n\n\nTestTrain\n\n\n\n\n                            \n                                            \n\n\n\n\n\n\n                            \n                                            \n\n\n\n\n\n\n\nNormality of Residuals:\n\n\nCheck for …\n\n\n\nAre residuals normally distributed?\n\n\n\n\nTestTrain\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTestTrain\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLeverage\n\nTestTrain\n\n\n\n\n\n\n\n\n\n\n\nScale-Location plot\n\nTestTrain\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResiduals Autocorrelation Plot\n\nTestTrain\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResiduals vs Time\n\nTestTrain",
    "crumbs": [
      "Dashboard",
      "Modelling",
      "Gradient Boost"
    ]
  },
  {
    "objectID": "05_model_waermestrom/05_05_gradient_boost.html#metrics-2",
    "href": "05_model_waermestrom/05_05_gradient_boost.html#metrics-2",
    "title": "Gradient Boost",
    "section": "Metrics",
    "text": "Metrics\n\nSingle split\nMetrics based on the test set of the single split\n\nLine plotRadar plot\n\n\n\n\n                            \n                                            \n\n\n\n\n\n\n                            \n                                            \n\n\n\n\n\n\n\nCross validation\n\nLine plotRadar plot\n\n\n\n\n                            \n                                            \n\n\n\n\n\n\n                            \n                                            \n\n\n\n\n\n\nMAEMSERMSER2MAPEEVSMeAED2Pinball",
    "crumbs": [
      "Dashboard",
      "Modelling",
      "Gradient Boost"
    ]
  },
  {
    "objectID": "05_model_waermestrom/05_05_gradient_boost.html#predictions-residuals-observed",
    "href": "05_model_waermestrom/05_05_gradient_boost.html#predictions-residuals-observed",
    "title": "Gradient Boost",
    "section": "Predictions, residuals, observed",
    "text": "Predictions, residuals, observed\n\n\n                            \n                                            \n\n\nnext",
    "crumbs": [
      "Dashboard",
      "Modelling",
      "Gradient Boost"
    ]
  },
  {
    "objectID": "05_model_waermestrom/05_05_gradient_boost.html#time-vs.-predicted-and-observed",
    "href": "05_model_waermestrom/05_05_gradient_boost.html#time-vs.-predicted-and-observed",
    "title": "Gradient Boost",
    "section": "Time vs. Predicted and Observed",
    "text": "Time vs. Predicted and Observed",
    "crumbs": [
      "Dashboard",
      "Modelling",
      "Gradient Boost"
    ]
  },
  {
    "objectID": "05_model_waermestrom/05_05_gradient_boost.html#time-vs.-residuals",
    "href": "05_model_waermestrom/05_05_gradient_boost.html#time-vs.-residuals",
    "title": "Gradient Boost",
    "section": "Time vs. Residuals",
    "text": "Time vs. Residuals",
    "crumbs": [
      "Dashboard",
      "Modelling",
      "Gradient Boost"
    ]
  },
  {
    "objectID": "05_model_waermestrom/05_05_gradient_boost.html#model-details",
    "href": "05_model_waermestrom/05_05_gradient_boost.html#model-details",
    "title": "Gradient Boost",
    "section": "Model details",
    "text": "Model details\n\n6a5a6ab482\n\n\nPipeline(steps=[('vars',\n                 ColumnSelector(columns=['tt_tu_mean', 'rf_tu_mean', 'td_mean',\n                                         'vp_std_mean', 'tf_std_mean'])),\n                ('model', GradientBoostingRegressor(random_state=7))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFitted\n        \n            \n                Parameters\n                \n\n\n\n\nsteps \n[('vars', ...), ('model', ...)]\n\n\n\ntransform_input \nNone\n\n\n\nmemory \nNone\n\n\n\nverbose \nFalse\n\n\n\n\n            \n        \n    ColumnSelector\n        \n            \n                Parameters\n                \n\n\n\n\ncolumns \n['tt_tu_mean', 'rf_tu_mean', ...]\n\n\n\n\n            \n        \n    GradientBoostingRegressor?Documentation for GradientBoostingRegressor\n        \n            \n                Parameters\n                \n\n\n\n\nloss \n'squared_error'\n\n\n\nlearning_rate \n0.1\n\n\n\nn_estimators \n100\n\n\n\nsubsample \n1.0\n\n\n\ncriterion \n'friedman_mse'\n\n\n\nmin_samples_split \n2\n\n\n\nmin_samples_leaf \n1\n\n\n\nmin_weight_fraction_leaf \n0.0\n\n\n\nmax_depth \n3\n\n\n\nmin_impurity_decrease \n0.0\n\n\n\ninit \nNone\n\n\n\nrandom_state \n7\n\n\n\nmax_features \nNone\n\n\n\nalpha \n0.9\n\n\n\nverbose \n0\n\n\n\nmax_leaf_nodes \nNone\n\n\n\nwarm_start \nFalse\n\n\n\nvalidation_fraction \n0.1\n\n\n\nn_iter_no_change \nNone\n\n\n\ntol \n0.0001\n\n\n\nccp_alpha \n0.0\n\n\n\n\n            \n        \n    \n\n\nPipeline(steps=[('vars', ColumnSelector(columns=['rf_tu_mean', 'vp_std_mean'])),\n                ('model',\n                 GradientBoostingRegressor(max_depth=5, min_samples_leaf=5,\n                                           min_samples_split=48,\n                                           n_estimators=60, random_state=7,\n                                           subsample=1))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFitted\n        \n            \n                Parameters\n                \n\n\n\n\nsteps \n[('vars', ...), ('model', ...)]\n\n\n\ntransform_input \nNone\n\n\n\nmemory \nNone\n\n\n\nverbose \nFalse\n\n\n\n\n            \n        \n    ColumnSelector\n        \n            \n                Parameters\n                \n\n\n\n\ncolumns \n['rf_tu_mean', 'vp_std_mean']\n\n\n\n\n            \n        \n    GradientBoostingRegressor?Documentation for GradientBoostingRegressor\n        \n            \n                Parameters\n                \n\n\n\n\nloss \n'squared_error'\n\n\n\nlearning_rate \n0.1\n\n\n\nn_estimators \n60\n\n\n\nsubsample \n1\n\n\n\ncriterion \n'friedman_mse'\n\n\n\nmin_samples_split \n48\n\n\n\nmin_samples_leaf \n5\n\n\n\nmin_weight_fraction_leaf \n0.0\n\n\n\nmax_depth \n5\n\n\n\nmin_impurity_decrease \n0.0\n\n\n\ninit \nNone\n\n\n\nrandom_state \n7\n\n\n\nmax_features \nNone\n\n\n\nalpha \n0.9\n\n\n\nverbose \n0\n\n\n\nmax_leaf_nodes \nNone\n\n\n\nwarm_start \nFalse\n\n\n\nvalidation_fraction \n0.1\n\n\n\nn_iter_no_change \nNone\n\n\n\ntol \n0.0001\n\n\n\nccp_alpha \n0.0",
    "crumbs": [
      "Dashboard",
      "Modelling",
      "Gradient Boost"
    ]
  },
  {
    "objectID": "05_model_waermestrom/05_07_h2o.html",
    "href": "05_model_waermestrom/05_07_h2o.html",
    "title": "H20 AutoML",
    "section": "",
    "text": "So let’s start from the pipeline with the best model cross-validated before\n\nimport pandas as pd\nimport numpy as np\nimport epyfun\nimport strom\n\nimport strom\nstrom_climate = strom.read_result(\"merge_strom_climate_data\")\n# strom_climate = pd.read_parquet(\"interim/strom_climate.parquet\")\nX = strom_climate.drop(columns=\"wd\")\ny = strom_climate[\"wd\"]\n\n\ndimensions=[\n    \"tt_tu_mean\",\n    \"tt_tu_min\",\n    \"tt_tu_max\",\n    \"rf_tu_mean\",\n    \"rf_tu_min\",\n    \"rf_tu_max\",\n]\n\n\nimport h2o\nfrom h2o.automl import H2OAutoML\n\n# Initialize the H2O cluster\nh2o.init()\n\n# Load your data into a H2OFrame\n# Assume strom_climate is a pandas DataFrame\nhf = h2o.H2OFrame(strom_climate)\n\n# Specify the target and predictor columns\ntarget = 'wd'\npredictors = [\n    \"tt_tu_min\", \"tt_tu_mean\", \"tt_tu_max\",\n\n    \"rf_tu_min\", \"rf_tu_mean\", \"rf_tu_max\",\n\n    'tt_min', 'tt_mean', 'tt_max',\n\n    'td_min', 'td_mean', 'td_max',\n\n    'vp_std_min', 'vp_std_mean', 'vp_std_max',\n\n    'tf_std_min', 'tf_std_mean', 'tf_std_max',\n\n    'p_std_min', 'p_std_mean', 'p_std_max',\n\n    'tt_std_min', 'tt_std_mean', 'tt_std_max',\n\n    'rf_std_min', 'rf_std_mean', 'rf_std_max',\n\n    'td_std_min', 'td_std_mean', 'td_std_max',\n\n    \"r1_min\", \"r1_mean\", \"r1_max\",\n    #'rs_ind_min', 'rs_ind_mean', 'rs_ind_max', 'wrtr_min', 'wrtr_mean', 'wrtr_max'\n]\n\n# Run AutoML\naml = H2OAutoML(seed=1, max_runtime_secs=3600) # 28800\naml.train(x=predictors, y=target, training_frame=hf)\n\n# View the AutoML Leaderboard\nlb = aml.leaderboard\n#print(lb.head(rows=lb.nrows))\nprint(lb.head(rows=10))\n\n\n# Get the best model\nbest_model = aml.leader\n\n# Make predictions on the strom_climate data\npredictions = best_model.predict(hf)\n\n# Convert predictions to pandas DataFrame\ny_pred = h2o.as_list(predictions)[\"predict\"]\n\n\nstrom.scatter_fitted_observed(y, y_pred, strom_climate)\n\n\nstrom.splom_fitted_observed(\n    y,\n    y_pred,\n    strom_climate,\n    dimensions=[\n        \"tt_tu_mean\",\n        \"tt_tu_min\",\n        \"tt_tu_max\",\n        \"rf_tu_mean\",\n        \"rf_tu_min\",\n        \"rf_tu_max\",\n        \"td_mean\",\n        \"vp_std_mean\",\n        \"r1_mean\"\n    ],\n).show()\n\n\nstrom.residuals_fitted(y, y_pred)\n\n\nstrom.residuals_hist(y, y_pred)\n\n\nstrom.residuals_qq(y, y_pred)\n\n\nbest_model.explain(hf)",
    "crumbs": [
      "Dashboard",
      "Modelling",
      "H20 AutoML"
    ]
  },
  {
    "objectID": "Process/00_strom_data_src.html",
    "href": "Process/00_strom_data_src.html",
    "title": "Strom Data Source",
    "section": "",
    "text": "%%sql\nCREATE OR REPLACE VIEW fail AS\nSELECT * FROM sqlite_scan('{{latest_file}}', 'meter');\nSELECT * FROM fail;\n\n\nMeters table\n\n%%sql\nSELECT \n    _id,\n    decode(name)::STRING,\n    decode(units)::STRING,\n    decode(comment)::STRING,\n    decode(vsf)::STRING,\n    decode(tsf)::STRING,\n    decode(cost)::STRING,\n    decode(fcost)::STRING,\n    decode(invert)::STRING,\n    decode(vmsetup)::STRING,\n    decode(type)::STRING,\n    decode(currency)::STRING,\n    decode(scaling)::STRING,\n    decode(phyunits)::STRING,\n    decode(bidir)::STRING,\n    decode(prod)::STRING\nFROM sqlite_scan('{{latest_file}}', 'meter')\n;\n\n\n\nStrom data\n\n%%sql \n\nSELECT \n  meterid, \n  -- Blob Functions, because most columns get read as blob\n  -- https://duckdb.org/docs/sql/functions/blob\n  decode(date)::DATETIME AS date, \n  decode(value)::INT AS value,\n  decode(comment)::STRING AS comment,\n  decode(color)::STRING AS color,\n  decode(cost)::STRING AS cost,\n  decode(vcor)::STRING AS vcor,\n  decode(ccor)::STRING AS ccor,\n  decode(first)::STRING AS first\nFROM sqlite_scan('{{latest_file}}', 'reading') \nWHERE meterid = 1\n;\n\n\n%%sql \n\ntoy &lt;&lt; \nWITH strom_sqlite AS (\n  SELECT \n    meterid, \n    -- Blob Functions, because most columns get read as blob\n    -- https://duckdb.org/docs/sql/functions/blob\n    decode(date)::DATETIME AS date, \n    decode(value)::INT AS value,\n    decode(first)::INT AS first\n  FROM sqlite_scan('{{latest_file}}', 'reading') \n  WHERE meterid = 1\n)\nSELECT *,\ndate_sub('minute', lag(date, 1, '2020-11-30 00:00:00') over(order by date), date) AS minutes, \nvalue * (1/(1-first)) - lag(value, 1, 12160) over(order by date) AS consumption,\n1.0 * consumption / minutes AS cm,\n24.0 * 60.0 * consumption / minutes AS consumption_day_equivalent\nFROM strom_sqlite\nORDER BY date\n;\n\n\n\nStrom data\n\n%%sql \n\nATTACH '{{latest_file}}' AS ecas (TYPE SQLITE);\nUSE ecas;\n\n\n%%sql \n\nSELECT * FROM reading;\n\n\n%%sql \n\n\nUPDATE reading\nSET meterid = 1\nWHERE meterid = 16;\n\n\n%%sql \n\n\nUPDATE reading\nSET meterid = 3\nWHERE meterid = 17;\n\n\n%%sql \n\n\nUPDATE reading\nSET meterid = 2\nWHERE meterid = 18;\n\n\n%%sql\n\nCREATE OR REPLACE TABLE normalstrom_minute_nulls AS\nWITH minutes_table AS (\n    SELECT UNNEST(generate_series(ts[1], ts[2], interval 1 minute)) AS minute\n    FROM (VALUES (\n    [(SELECT MIN(date) FROM normalstrom), (SELECT MAX(DATE) FROM normalstrom)]\n    )) t(ts)\n)\nSELECT * \nFROM minutes_table\nLEFT JOIN normalstrom\nON minutes_table.minute = normalstrom.date\n;\n\nSELECT * FROM normalstrom_minute_nulls ORDER BY minute;\n\n\n%%sql\n\nCREATE OR REPLACE TABLE normalstrom_minute AS\nSELECT\n    minute,\n    date,\n    value,\n    minutes,\n    consumption,\n    FIRST_VALUE(cm IGNORE NULLS) OVER(\n    ORDER BY minute ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING \n    ) AS cm\nFROM normalstrom_minute_nulls t1\nORDER BY t1.minute\n;\n\n\n%%sql\n\nSELECT * FROM strom_per_day WHERE date &gt;= '2021-01-01' AND date &lt;= '2021-12-31';\n\nSELECT MAX(date) FROM strom_per_day;\n\n\n%%sql \n\nSELECT \n  meterid, \n  decode(date)::DATETIME AS date, \n  decode(value)::INT AS value,\n  decode(first)::STRING AS first\nFROM sqlite_scan('{{latest_file}}', 'reading') \nWHERE meterid = 2\nAND date &gt;= '2023-05-21'\nAND date &lt;= '2023-05-31'\nORDER BY date\n;",
    "crumbs": [
      "Dashboard",
      "Process",
      "Strom Data Source"
    ]
  },
  {
    "objectID": "Process/02_waermestrom_process.html",
    "href": "Process/02_waermestrom_process.html",
    "title": "Wärmestrom",
    "section": "",
    "text": "https://learnsql.com/blog/moving-average-in-sql/ https://stackoverflow.com/questions/55491046/how-to-set-the-running-file-path-of-jupyter-in-vscode\n\nGet Wärmestrom data\nBy querying directly the SQLite table and filtering by meter id. But for Wärmestrom we have two different meters, because there are two tarifs depending on the time of the day.\n\n%%sql \n\nCREATE OR REPLACE TABLE waermestrom_sqlite AS \nSELECT \n  meterid, \n  -- Blob Functions, because most columns get read as blob\n  -- https://duckdb.org/docs/sql/functions/blob\n  decode(date)::DATETIME AS date, \n  decode(value)::INT AS value\nFROM sqlite_scan('{{latest_file}}', 'reading') \nWHERE meterid = 2 OR meterid = 3\n;\nSELECT * FROM waermestrom_sqlite;\n\n\n\n\n\n\n\n\nmeterid\ndate\nvalue\n\n\n\n\n0\n3\n2020-11-30 17:36:00\n9773\n\n\n1\n2\n2020-11-30 17:36:00\n9456\n\n\n2\n2\n2020-12-01 00:00:00\n9464\n\n\n3\n3\n2020-12-01 00:00:00\n9779\n\n\n4\n2\n2020-12-01 14:23:00\n9470\n\n\n...\n...\n...\n...\n\n\n4408\n3\n2025-11-18 06:43:00\n4678\n\n\n4409\n2\n2025-11-20 20:03:00\n3502\n\n\n4410\n3\n2025-11-20 20:03:00\n4728\n\n\n4411\n2\n2025-11-21 06:33:00\n3507\n\n\n4412\n3\n2025-11-21 06:33:00\n4730\n\n\n\n\n4413 rows × 3 columns\n\n\n\nIdeally, there would be one measurement for each tarif, for every date (minute). But we cannot guarantee that’s the case (e.g. measurement for one tarif can be at 13:08:59 and for the other at 13:09:00). So let’s see if that’s the case.\n\n%%sql \n\nSELECT date, count(*) AS cnt\nFROM waermestrom_sqlite\nGROUP BY date\n;\n\n\n\n\n\n\n\n\ndate\ncnt\n\n\n\n\n0\n2020-11-30 17:36:00\n2\n\n\n1\n2020-12-01 07:18:00\n1\n\n\n2\n2020-12-01 07:19:00\n1\n\n\n3\n2020-12-01 20:22:00\n1\n\n\n4\n2020-12-03 02:38:00\n2\n\n\n...\n...\n...\n\n\n2416\n2025-11-09 20:27:00\n2\n\n\n2417\n2025-11-11 20:36:00\n2\n\n\n2418\n2025-11-12 08:56:00\n2\n\n\n2419\n2025-11-16 20:09:00\n2\n\n\n2420\n2025-11-20 20:03:00\n2\n\n\n\n\n2421 rows × 2 columns\n\n\n\nYeap, there are some cases with cnt=1. More precisely, the below number of cases:\n\n%%sql \n\nWITH ucnt AS (\n  SELECT date, count(*) AS cnt\n  FROM waermestrom_sqlite\n  GROUP BY date\n)\nSELECT cnt, COUNT(*) FROM ucnt GROUP BY cnt\n;\n\n\n\n\n\n\n\n\ncnt\ncount_star()\n\n\n\n\n0\n2\n1992\n\n\n1\n1\n429\n\n\n\n\n\n\n\nSafest would be to just join the dates and make sure to fill in the gaps with the closest value. Let’s see\n\n%%sql\n\nCREATE OR REPLACE TABLE waermestrom_nulls AS\nWITH\nws181 AS (\n  SELECT \n    'Hoch' AS tariff,\n    decode(date)::DATETIME AS date, \n    decode(value)::INT AS value\n  FROM sqlite_scan('{{latest_file}}', 'reading') \n  WHERE meterid = 3 \n),\nws182 AS (\n  SELECT \n    'Niedrig' AS tariff, \n    decode(date)::DATETIME AS date, \n    decode(value)::INT AS value\n  FROM sqlite_scan('{{latest_file}}', 'reading') \n  WHERE meterid = 2\n)\nSELECT\n  COALESCE(ws181.date, ws182.date) AS date,\n  ws181.value AS value_hoch,\n  ws182.value AS value_niedrig\nFROM ws181 \nFULL JOIN ws182 \nON ws181.date = ws182.date\nORDER BY date\n;\nSELECT * FROM waermestrom_nulls LIMIT 20;\n\n\n\n\n\n\n\n\ndate\nvalue_hoch\nvalue_niedrig\n\n\n\n\n0\n2016-07-04 07:50:00\n0\n0\n\n\n1\n2016-07-04 08:00:00\n1\n1\n\n\n2\n2016-11-28 00:21:00\n547\n&lt;NA&gt;\n\n\n3\n2016-11-28 15:23:00\n&lt;NA&gt;\n484\n\n\n4\n2017-11-25 15:20:00\n3347\n&lt;NA&gt;\n\n\n5\n2017-11-25 15:23:00\n&lt;NA&gt;\n2677\n\n\n6\n2018-11-23 15:19:00\n5747\n&lt;NA&gt;\n\n\n7\n2018-11-23 15:22:00\n&lt;NA&gt;\n4893\n\n\n8\n2019-11-29 14:18:00\n7914\n&lt;NA&gt;\n\n\n9\n2019-11-29 14:22:00\n&lt;NA&gt;\n7224\n\n\n10\n2020-11-30 07:07:00\n9764\n9456\n\n\n11\n2020-11-30 17:36:00\n9773\n9456\n\n\n12\n2020-12-01 00:00:00\n9779\n9464\n\n\n13\n2020-12-01 07:18:00\n9782\n&lt;NA&gt;\n\n\n14\n2020-12-01 07:19:00\n&lt;NA&gt;\n9470\n\n\n15\n2020-12-01 14:23:00\n9792\n9470\n\n\n16\n2020-12-01 20:21:00\n9798\n&lt;NA&gt;\n\n\n17\n2020-12-01 20:22:00\n&lt;NA&gt;\n9470\n\n\n18\n2020-12-02 00:01:00\n9800\n9473\n\n\n19\n2020-12-02 07:29:00\n9802\n9476\n\n\n\n\n\n\n\nYeah, those are the cases: 2020-12-01 07:18:00 has value hoch, but no niedrig 2020-12-01 07:19:00 has value niedrig, but no hoch\nSo now we want to fill in those gaps using the value of the same column, that has the closest date. So it’s tricky, because it cannot simply be a fill-down, or fill-up. Because in one case, the correct value would be one position up, and in other case one position down. Here’s one approach (it just assumes there are no consecutive nulls for the value columns; please also note that it takes advantage of DuckDB’s flexible SQL syntax -otherwhise it would have been even longer, with a bunch of CTEs-)\n\n%%sql\n\nCREATE OR REPLACE TABLE waermestrom_nonulls AS\nSELECT\n  date,\n  value_hoch, value_niedrig, \n  -- calculate minutes diff with previous and next date, to see which is closer\n  -- note the use of a default value for lag/lead, substracting and adding one day\n  -- for lag and lead respectively, to avoid NULLs in the first and las rows\n  date_sub('minute', lag(date, 1, date - INTERVAL 1 DAY) over(order by date), date) AS minutes_lag,\n  date_sub('minute', date, lead(date, 1, date + INTERVAL 1 DAY) over(order by date)) AS minutes_lead,\n  -- and we want to replace null values column, with the value from closest date\n  CASE\n    WHEN value_hoch IS NULL AND minutes_lag &lt;= minutes_lead \n    THEN lag(value_hoch) over(order by date)\n    WHEN value_hoch IS NULL AND minutes_lag &gt; minutes_lead \n    THEN lead(value_hoch) over(order by date)\n    ELSE value_hoch\n  END AS value_hoch_fix,\n  CASE\n    WHEN value_niedrig IS NULL AND minutes_lag &lt;= minutes_lead \n    THEN lag(value_niedrig) over(order by date)\n    WHEN value_niedrig IS NULL AND minutes_lag &gt; minutes_lead \n    THEN lead(value_niedrig) over(order by date)\n    ELSE value_niedrig\n  END AS value_niedrig_fix,\n  value_hoch_fix + value_niedrig_fix AS value\nFROM waermestrom_nulls \nORDER BY date\n;\nSELECT * FROM waermestrom_nonulls ORDER BY date;\n\n\n\n\n\n\n\n\ndate\nvalue_hoch\nvalue_niedrig\nminutes_lag\nminutes_lead\nvalue_hoch_fix\nvalue_niedrig_fix\nvalue\n\n\n\n\n0\n2016-07-04 07:50:00\n0\n0\n1440\n10\n0\n0\n0\n\n\n1\n2016-07-04 08:00:00\n1\n1\n10\n211221\n1\n1\n2\n\n\n2\n2016-11-28 00:21:00\n547\n&lt;NA&gt;\n211221\n902\n547\n484\n1031\n\n\n3\n2016-11-28 15:23:00\n&lt;NA&gt;\n484\n902\n521277\n547\n484\n1031\n\n\n4\n2017-11-25 15:20:00\n3347\n&lt;NA&gt;\n521277\n3\n3347\n2677\n6024\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2416\n2025-11-16 20:09:00\n4667\n3480\n739\n611\n4667\n3480\n8147\n\n\n2417\n2025-11-17 06:20:00\n4667\n3484\n611\n1463\n4667\n3484\n8151\n\n\n2418\n2025-11-18 06:43:00\n4678\n3489\n1463\n3680\n4678\n3489\n8167\n\n\n2419\n2025-11-20 20:03:00\n4728\n3502\n3680\n630\n4728\n3502\n8230\n\n\n2420\n2025-11-21 06:33:00\n4730\n3507\n630\n1440\n4730\n3507\n8237\n\n\n\n\n2421 rows × 8 columns\n\n\n\nGood, now we just need to calculate the consumption and create the main table.\n\n%%sql \n\nCREATE OR REPLACE TABLE waermestrom AS\nSELECT \n  date,\n  value,\n  value_hoch_fix AS value_hoch,\n  value_niedrig_fix AS value_niedrig,\n  minutes_lag AS minutes,\n  -- add default values to lag(), to prevent null in the first row\n  -- use 11kwh less than the first value which is approximately the avg consumption per day\n  -- and would be equivalent to the minutes in the first row, that we set with the default\n  -- of one day in the previous query \n  value - lag(value, 1, value-11) over(order by date) AS consumption,\n  1.0 * consumption / minutes_lag AS cm,\n  24.0 * 60.0 * consumption / minutes_lag AS consumption_day_equivalent,\n  -- now calculate consumption per tariff\n  value_hoch_fix - lag(value_hoch_fix, 1, value_hoch_fix-11) over(order by date) AS consumption_hoch,\n  value_niedrig_fix - lag(value_niedrig_fix, 1, value_niedrig_fix-11) over(order by date) AS consumption_niedrig,\n  1.0 * consumption_hoch / minutes_lag AS cm_hoch,\n  1.0 * consumption_niedrig / minutes_lag AS cm_niedrig\nFROM waermestrom_nonulls \nWHERE minutes &gt; 1 --get rid of the artificially short periods\n;\nSELECT * FROM waermestrom ORDER BY date;\n\n\n\n\n\n\n\n\ndate\nvalue\nvalue_hoch\nvalue_niedrig\nminutes\nconsumption\ncm\nconsumption_day_equivalent\nconsumption_hoch\nconsumption_niedrig\ncm_hoch\ncm_niedrig\n\n\n\n\n0\n2016-07-04 07:50:00\n0\n0\n0\n1440\n11\n0.007639\n11.000000\n11\n11\n0.007639\n0.007639\n\n\n1\n2016-07-04 08:00:00\n2\n1\n1\n10\n2\n0.200000\n288.000000\n1\n1\n0.100000\n0.100000\n\n\n2\n2016-11-28 00:21:00\n1031\n547\n484\n211221\n1029\n0.004872\n7.015212\n546\n483\n0.002585\n0.002287\n\n\n3\n2016-11-28 15:23:00\n1031\n547\n484\n902\n0\n0.000000\n0.000000\n0\n0\n0.000000\n0.000000\n\n\n4\n2017-11-25 15:20:00\n6024\n3347\n2677\n521277\n4993\n0.009578\n13.792897\n2800\n2193\n0.005371\n0.004207\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2207\n2025-11-16 20:09:00\n8147\n4667\n3480\n739\n5\n0.006766\n9.742896\n0\n5\n0.000000\n0.006766\n\n\n2208\n2025-11-17 06:20:00\n8151\n4667\n3484\n611\n4\n0.006547\n9.427169\n0\n4\n0.000000\n0.006547\n\n\n2209\n2025-11-18 06:43:00\n8167\n4678\n3489\n1463\n16\n0.010936\n15.748462\n11\n5\n0.007519\n0.003418\n\n\n2210\n2025-11-20 20:03:00\n8230\n4728\n3502\n3680\n63\n0.017120\n24.652174\n50\n13\n0.013587\n0.003533\n\n\n2211\n2025-11-21 06:33:00\n8237\n4730\n3507\n630\n7\n0.011111\n16.000000\n2\n5\n0.003175\n0.007937\n\n\n\n\n2212 rows × 12 columns\n\n\n\n\n15483-15472\n16695-16662\n\n33\n\n\n\n\nVisualize the data\n\n%%sql \nwaermestrom &lt;&lt; SELECT * FROM waermestrom;\n\nAgain, very noisy data, with substantial variation in the consumption day equivalent and there is 1.5 years without data. But here you kinda already see the seasonal pattern of higher consumption in winter time.\n\nimport plotly.express as px\nfig = px.line(waermestrom, x='date', y=\"consumption_day_equivalent\")\nfig.show()\n\n                            \n                                            \n\n\nThe minutes show a similar pattern, but with a bunch of very low values (probably 1), that should be due to the combination of the two meters when they do not fall in exactly the same minute.\n\nimport plotly.express as px\nfig = px.histogram(waermestrom.query(\"minutes &lt; 10000\"), x=\"minutes\", marginal=\"box\")\nfig.show()\n\n                            \n                                            \n\n\nThe consumption day equivalent varies also substantially, and it is of course higher than the normal strom.\n\nimport plotly.express as px\nfig = px.histogram(waermestrom.query(\"minutes &lt; 10000\"), x=\"consumption_day_equivalent\", marginal=\"box\")\nfig.show()\n\n                            \n                                            \n\n\nHere the pattern of minutes and consumption is not so marked as in the normal strom.\n\nfrom matplotlib import pyplot\npyplot.scatter(\n    waermestrom.query(\"minutes &lt; 10000\")[\"minutes\"], \n    waermestrom.query(\"minutes &lt; 10000\")[\"consumption_day_equivalent\"]\n)\n\n\n\n\n\n\n\n\n\nimport plotly.express as px\nfig = px.scatter(\n    data_frame=waermestrom.query(\"minutes &lt; 10000\"), \n    x=\"minutes\", \n    y=\"consumption_day_equivalent\", hover_data=['date'],\n    marginal_x=\"histogram\", \n    marginal_y=\"histogram\"\n)\nfig.show()\n\n                            \n                                            \n\n\n\n\nConsumption by hour\nSo, again let’s take the inefficient but straightforward way. First expand in minutes to the whole range.\n\n%%sql\n\nCREATE OR REPLACE TABLE waermestrom_minute_nulls AS\nWITH minutes_table AS (\n  SELECT UNNEST(generate_series(ts[1], ts[2], interval 1 minute)) as minute\n  FROM (VALUES (\n    [(SELECT MIN(date) FROM waermestrom), (SELECT MAX(DATE) FROM waermestrom)]\n  )) t(ts)\n)\nSELECT * \nFROM minutes_table\nLEFT JOIN waermestrom\nON minutes_table.minute = waermestrom.date\n;\nSELECT * FROM waermestrom_minute_nulls ORDER BY minute LIMIT 10;\n\n\n\n\n\n\n\n\nminute\ndate\nvalue\nvalue_hoch\nvalue_niedrig\nminutes\nconsumption\ncm\nconsumption_day_equivalent\nconsumption_hoch\nconsumption_niedrig\ncm_hoch\ncm_niedrig\n\n\n\n\n0\n2016-07-04 07:50:00\n2016-07-04 07:50:00\n0\n0\n0\n1440\n11\n0.007639\n11.0\n11\n11\n0.007639\n0.007639\n\n\n1\n2016-07-04 07:51:00\nNaT\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\nNaN\nNaN\n&lt;NA&gt;\n&lt;NA&gt;\nNaN\nNaN\n\n\n2\n2016-07-04 07:52:00\nNaT\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\nNaN\nNaN\n&lt;NA&gt;\n&lt;NA&gt;\nNaN\nNaN\n\n\n3\n2016-07-04 07:53:00\nNaT\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\nNaN\nNaN\n&lt;NA&gt;\n&lt;NA&gt;\nNaN\nNaN\n\n\n4\n2016-07-04 07:54:00\nNaT\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\nNaN\nNaN\n&lt;NA&gt;\n&lt;NA&gt;\nNaN\nNaN\n\n\n5\n2016-07-04 07:55:00\nNaT\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\nNaN\nNaN\n&lt;NA&gt;\n&lt;NA&gt;\nNaN\nNaN\n\n\n6\n2016-07-04 07:56:00\nNaT\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\nNaN\nNaN\n&lt;NA&gt;\n&lt;NA&gt;\nNaN\nNaN\n\n\n7\n2016-07-04 07:57:00\nNaT\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\nNaN\nNaN\n&lt;NA&gt;\n&lt;NA&gt;\nNaN\nNaN\n\n\n8\n2016-07-04 07:58:00\nNaT\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\nNaN\nNaN\n&lt;NA&gt;\n&lt;NA&gt;\nNaN\nNaN\n\n\n9\n2016-07-04 07:59:00\nNaT\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\nNaN\nNaN\n&lt;NA&gt;\n&lt;NA&gt;\nNaN\nNaN\n\n\n\n\n\n\n\nAnd fill in the NULLS\n\n%%sql\n\nCREATE OR REPLACE TABLE waermestrom_minute AS\nSELECT\n  minute,\n  date,\n  value,\n  value_hoch,\n  value_niedrig,\n  minutes,\n  consumption,\n  FIRST_VALUE(cm IGNORE NULLS) OVER(\n    ORDER BY minute ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING \n  ) AS cm,\n  FIRST_VALUE(cm_hoch IGNORE NULLS) OVER(\n    ORDER BY minute ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING \n  ) AS cm_hoch,\n  FIRST_VALUE(cm_niedrig IGNORE NULLS) OVER(\n    ORDER BY minute ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING \n  ) AS cm_niedrig\nFROM waermestrom_minute_nulls t1\nORDER BY t1.minute\n;\nSELECT * FROM waermestrom_minute ORDER BY minute LIMIT 100;\n\n\n\n\n\n\n\n\n\n\n\nminute\ndate\nvalue\nvalue_hoch\nvalue_niedrig\nminutes\nconsumption\ncm\ncm_hoch\ncm_niedrig\n\n\n\n\n0\n2016-07-04 07:50:00\n2016-07-04 07:50:00\n0\n0\n0\n1440\n11\n0.007639\n0.007639\n0.007639\n\n\n1\n2016-07-04 07:51:00\nNaT\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n0.200000\n0.100000\n0.100000\n\n\n2\n2016-07-04 07:52:00\nNaT\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n0.200000\n0.100000\n0.100000\n\n\n3\n2016-07-04 07:53:00\nNaT\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n0.200000\n0.100000\n0.100000\n\n\n4\n2016-07-04 07:54:00\nNaT\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n0.200000\n0.100000\n0.100000\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n95\n2016-07-04 09:25:00\nNaT\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n0.004872\n0.002585\n0.002287\n\n\n96\n2016-07-04 09:26:00\nNaT\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n0.004872\n0.002585\n0.002287\n\n\n97\n2016-07-04 09:27:00\nNaT\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n0.004872\n0.002585\n0.002287\n\n\n98\n2016-07-04 09:28:00\nNaT\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n0.004872\n0.002585\n0.002287\n\n\n99\n2016-07-04 09:29:00\nNaT\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n&lt;NA&gt;\n0.004872\n0.002585\n0.002287\n\n\n\n\n100 rows × 10 columns\n\n\n\nAnd now we can simply aggregate per day and hour, and the average will be correct, as all the rows have comparable units (consumption for one minute, with equal weight).\n\n%%sql\n\nconsumption_hour_avg &lt;&lt; SELECT \n  COUNT(*) AS cnt,\n  hour(minute) AS hour, \n  AVG(cm)*60*24 AS cmy\nFROM waermestrom_minute\nGROUP BY hour(minute)\n;\n\n\nimport plotly.express as px\nfig = px.line(consumption_hour_avg, y='cmy', x='hour')\nfig.show()\n\n                            \n                                            \n\n\n\n%%sql\n\nconsumption_hour_avg &lt;&lt; SELECT \n  hour(minute) AS hour, \n  1.0*AVG(cm)*60*24 AS cmy\nFROM waermestrom_minute\nWHERE minute &lt;= '2021-05-25' OR minute &gt;= '2022-11-30'\nGROUP BY hour(minute)\n;\n\n\nimport plotly.express as px\nfig = px.bar(consumption_hour_avg, y='cmy', x='hour')\nfig.show()\n\n                            \n                                            \n\n\n\n\nTraces plot\n\n%%sql\n\n\nSELECT\n  hour(minute) AS hour, \n  AVG(1.0 * 60 * 24 * cm) AS cm\nFROM waermestrom_minute\nWHERE minute &lt;= '2021-05-25' OR minute &gt;= '2022-11-30'\nGROUP BY hour(minute)\n;\n\n\n\n\n\n\n\n\nhour\ncm\n\n\n\n\n0\n0\n12.552908\n\n\n1\n1\n12.491177\n\n\n2\n2\n12.473391\n\n\n3\n3\n12.469446\n\n\n4\n4\n12.450680\n\n\n5\n5\n12.419435\n\n\n6\n6\n12.361208\n\n\n7\n7\n12.327572\n\n\n8\n8\n12.189145\n\n\n9\n9\n-259.561161\n\n\n10\n10\n11.974442\n\n\n11\n11\n11.872776\n\n\n12\n12\n11.758993\n\n\n13\n13\n11.715702\n\n\n14\n14\n11.715422\n\n\n15\n15\n11.673751\n\n\n16\n16\n11.634943\n\n\n17\n17\n11.586455\n\n\n18\n18\n11.649886\n\n\n19\n19\n11.875831\n\n\n20\n20\n12.336025\n\n\n21\n21\n12.680694\n\n\n22\n22\n12.759397\n\n\n23\n23\n12.689571\n\n\n\n\n\n\n\n\n%%sql\n\nSELECT \n  minute,\n  date,\n  1.0 * 60 * 24 * cm AS cm,\n  AVG(1.0 * 60 * 24 * cm) OVER(\n    ORDER BY minute ROWS BETWEEN 240 PRECEDING AND CURRENT ROW\n  ) AS cmma\nFROM waermestrom_minute\nWHERE minute &gt; '2022-11-30'\n;\n\n\n\n\n\n\n\n\nminute\ndate\ncm\ncmma\n\n\n\n\n0\n2022-11-30 00:01:00\nNaT\n11.974301\n11.974301\n\n\n1\n2022-11-30 00:02:00\nNaT\n11.974301\n11.974301\n\n\n2\n2022-11-30 00:03:00\nNaT\n11.974301\n11.974301\n\n\n3\n2022-11-30 00:04:00\nNaT\n11.974301\n11.974301\n\n\n4\n2022-11-30 00:05:00\nNaT\n11.974301\n11.974301\n\n\n...\n...\n...\n...\n...\n\n\n1565668\n2025-11-21 06:29:00\nNaT\n16.000000\n16.000000\n\n\n1565669\n2025-11-21 06:30:00\nNaT\n16.000000\n16.000000\n\n\n1565670\n2025-11-21 06:31:00\nNaT\n16.000000\n16.000000\n\n\n1565671\n2025-11-21 06:32:00\nNaT\n16.000000\n16.000000\n\n\n1565672\n2025-11-21 06:33:00\n2025-11-21 06:33:00\n16.000000\n16.000000\n\n\n\n\n1565673 rows × 4 columns\n\n\n\n\n%%sql\n\nCREATE OR REPLACE TABLE toy AS\nWITH hourly_average AS (\n  SELECT\n    hour(minute) AS hour, \n    AVG(1.0 * 60 * 24 * cm) AS cmha\n  FROM waermestrom_minute\n  --This was originally here, because we wanted to see the hourly variation\n  --and keeping this long period without measurements just smoothed things\n  --but for waermestrom, it has a misleading implication: since we have \n  --measurements mostly in the wintertime, the average without this period\n  --is high, reflecting the higher energy consumption during winter\n  --WHERE minute &lt;= '2021-05-25' OR minute &gt;= '2022-11-30'\n  GROUP BY hour(minute)\n),\nlast_measurements AS (\n  SELECT \n    minute,\n    date,\n    1.0 * 60 * 24 * cm AS cm,\n    AVG(1.0 * 60 * 24 * cm) OVER(\n      ORDER BY minute ROWS BETWEEN 60*4 PRECEDING AND CURRENT ROW\n    ) AS cmma\n  FROM waermestrom_minute\n  WHERE minute &gt;= '2022-11-30'\n)\nSELECT *, CASE WHEN date IS NOT NULL THEN cm ELSE NULL END AS cmdate\nFROM last_measurements\nLEFT JOIN hourly_average\nON hour(last_measurements.minute) = hourly_average.hour\n;\n\n\n\n\n\n\n\n\nSuccess\n\n\n\n\n\n\n\n\n\n\n%sql toy &lt;&lt; SELECT * FROM toy;\n\n\nimport plotly.graph_objects as go\nfig = px.area(toy, x='minute', y='cmha')\nfig.add_trace(go.Scatter(\n  x=toy['minute'], y=toy['cmma'], mode='lines', showlegend=False\n))\nfig.add_trace(go.Scatter(\n  x=toy['date'], y=toy['cmdate'], mode='markers', showlegend=False\n))\n\n# Add range slider\nfig.update_layout(\n    xaxis=dict(\n        rangeselector=dict(\n            buttons=list([\n                dict(count=1,\n                     label=\"1m\",\n                     step=\"month\",\n                     stepmode=\"backward\"),\n                dict(count=7,\n                     label=\"7d\",\n                     step=\"day\",\n                     stepmode=\"backward\"),\n                dict(count=15,\n                     label=\"15d\",\n                     step=\"day\",\n                     stepmode=\"backward\"),\n                dict(step=\"all\")\n            ])\n        ),\n        rangeslider=dict(\n            visible=True\n        ),\n        type=\"date\"\n    )\n)\n\nfig.show()\n\nTODO: fix the range, which range?\nLet’s see the rate hoch tarif vs. niedrig\nniedrig_fraction &lt;&lt; SELECT date AS day, AVG(cm_niedrig/cm) AS niedrig_fraction, AVG(cm_niedrig)/AVG(cm) AS niedrig_fraction2, AVG(niedrig_fraction) OVER( ORDER BY minute ROWS BETWEEN 60 * 24 * 7 PRECEDING AND CURRENT ROW ) AS niedrig_fraction_ma FROM waermestrom_minute GROUP BY date ORDER BY date ;\nimport plotly.express as px fig = px.bar(niedrig_fraction, x=‘day’, y=‘niedrig_fraction2’) fig.show()",
    "crumbs": [
      "Dashboard",
      "Process",
      "Wärmestrom"
    ]
  },
  {
    "objectID": "Setup/01_quarto.html",
    "href": "Setup/01_quarto.html",
    "title": "A. Quarto Web Site",
    "section": "",
    "text": "This is a Quarto website that will contain all the strom project.\nIt uses “Auto Generation” to populate the contents of the web site, so al lthe qmd files will be included. You just need to arrange them in folders so they will be arranged in the navigation accordingly.\nThe website is a sub-directory of the main project. We want everything relative to the main project. Therefore, when rendering/previewing the website, you should set the working directory. For example, using Quarto’s CLI:\nquarto preview /OneDrive/All/R/strom/quarto --execute-dir /OneDrive/All/R/strom\nor, relative to the current directory in the shell, simply:\nquarto render 'quarto' --execute-dir '.'\nOtherwise, Quarto would use the website sub-directory as the working directory to run the code. And we do not want that!\nHere’s a workaround that seems to work to control the order of execution of the different files in the project https://github.com/quarto-dev/quarto-cli/discussions/6944\nquarto render ‘quarto’ –execute-dir ‘.’ ; ..html",
    "crumbs": [
      "Dashboard",
      "Setup",
      "A. Quarto Web Site"
    ]
  },
  {
    "objectID": "Setup/03_load_data.html",
    "href": "Setup/03_load_data.html",
    "title": "C. Loading Data",
    "section": "",
    "text": "We need to fire up DuckDB, in-memory or pointing to a file.\n```{python}\n%%sql\n\nduckdb:///:memory:\n-- duckdb:///path/to/file.db\n```\nAnd the data come in a SQLite file. DuckDB has a SQLite extension, so we just need to make sure it is installed and load it.\n```{python}\n%%sql\n\nINSTALL sqlite;\nLOAD sqlite;\n```\nAnd finally we just need to attach the SQLite file, which creates a bunch of views in the DuckDB side, for each table in the SQLite file.\n```{python}\n%%sql\n\nCALL sqlite_attach('data/2022-12-28-ecas-export.db');\nPRAGMA show_tables;\n```\nWith everything loaded, we can simply query the tables. For example, let’s see the data about the meters.\n\n%%sql \n\nSELECT * FROM meter;\n\n\n\n\n\n\n\n\n_id\nname\nunits\ncomment\nvsf\ntsf\ncost\nfcost\ninvert\nvmsetup\ntype\ncurrency\nscaling\nphyunits\nbidir\nprod\n\n\n\n\n0\n1\n[83, 116, 114, 111, 109]\n[]\n[]\n[53, 48]\n[53, 48]\n[]\n[]\n[48]\n&lt;NA&gt;\n[48]\n[36]\n[49, 46, 48]\n[]\n[48]\n[48]\n\n\n1\n2\n[87, 195, 164, 114, 109, 101, 115, 116, 114, 1...\n[]\n[]\n[53, 48]\n[53, 48]\n[]\n[]\n[48]\n&lt;NA&gt;\n[48]\n[36]\n[49, 46, 48]\n[]\n[48]\n[48]\n\n\n2\n3\n[87, 195, 164, 114, 109, 101, 115, 116, 114, 1...\n[]\n[]\n[53, 48]\n[53, 48]\n[]\n[]\n[48]\n&lt;NA&gt;\n[48]\n[36]\n[49, 46, 48]\n[]\n[48]\n[48]\n\n\n3\n4\n[87, 97, 115, 115, 101, 114]\n[]\n[]\n[53, 48]\n[53, 48]\n[]\n[]\n[48]\n&lt;NA&gt;\n[48]\n[36]\n[49, 46, 48]\n[]\n[48]\n[48]\n\n\n4\n5\n[76, 195, 188, 102, 116, 117, 110, 103, 115, 9...\n[]\n[]\n[53, 48]\n[53, 48]\n[]\n[]\n[48]\n&lt;NA&gt;\n[48]\n[36]\n[49, 46, 48]\n[]\n[48]\n[48]\n\n\n5\n6\n[87, 97, 115, 99, 104, 109, 97, 115, 99, 104, ...\n[]\n[]\n[53, 48]\n[53, 48]\n[]\n[]\n[48]\n&lt;NA&gt;\n[48]\n[36]\n[49, 46, 48]\n[]\n[48]\n[48]\n\n\n6\n7\n[71, 101, 115, 99, 104, 105, 114, 114, 115, 11...\n[]\n[]\n[53, 48]\n[53, 48]\n[]\n[]\n[48]\n&lt;NA&gt;\n[48]\n[36]\n[49, 46, 48]\n[]\n[48]\n[48]\n\n\n7\n8\n[79, 102, 102, 101, 110]\n[]\n[]\n[53, 48]\n[53, 48]\n[]\n[]\n[48]\n&lt;NA&gt;\n[48]\n[36]\n[49, 46, 48]\n[]\n[48]\n[48]\n\n\n8\n9\n[82, 101, 115, 101, 116]\n[]\n[]\n[53, 48]\n[53, 48]\n[]\n[]\n[48]\n&lt;NA&gt;\n[48]\n[36]\n[49, 46, 48]\n[]\n[48]\n[48]\n\n\n9\n10\n[78, 97, 99, 104, 108, 97, 100, 101, 110]\n[]\n[]\n[53, 48]\n[53, 48]\n[]\n[]\n[48]\n&lt;NA&gt;\n[48]\n[36]\n[49, 46, 48]\n[]\n[48]\n[48]\n\n\n10\n11\n[84, 101, 109, 112]\n[]\n[]\n[53, 48]\n[53, 48]\n[]\n[]\n[48]\n&lt;NA&gt;\n[48]\n[36]\n[49, 46, 48]\n[]\n[48]\n[48]\n\n\n11\n12\n[87, 195, 164, 114, 109, 101, 114]\n[]\n[]\n[53, 48]\n[53, 48]\n[]\n[]\n[48]\n&lt;NA&gt;\n[48]\n[36]\n[49, 46, 48]\n[]\n[48]\n[48]\n\n\n\n\n\n\n\nTODO: Temporarilly we will put this in a file to include. Later we will improve how to load and share the data across scripts.\nAlso, let’s not use sqlite_attach. Otherwise you would need a new session every time you want to attach a new file because it fails trying to attach again the same table names (the views already exist). And there does not seem to be a way of telling sqlite_attach to simply replace everything. So better to use sqlite_scan to query only what we need.",
    "crumbs": [
      "Dashboard",
      "Setup",
      "C. Loading Data"
    ]
  }
]