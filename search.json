[
  {
    "objectID": "08_h2o.html",
    "href": "08_h2o.html",
    "title": "H20 AutoML",
    "section": "",
    "text": "So let’s start from the pipeline with the best model cross-validated before\n\nimport pandas as pd\nimport numpy as np\nimport epyfun\nimport strom\n\nimport strom\nstrom_climate = strom.read_result(\"merge_strom_climate_data\")\n# strom_climate = pd.read_parquet(\"interim/strom_climate.parquet\")\nX = strom_climate.drop(columns=\"wd\")\ny = strom_climate[\"wd\"]\n\n\ndimensions=[\n    \"tt_tu_mean\",\n    \"tt_tu_min\",\n    \"tt_tu_max\",\n    \"rf_tu_mean\",\n    \"rf_tu_min\",\n    \"rf_tu_max\",\n]\n\n\nimport h2o\nfrom h2o.automl import H2OAutoML\n\n# Initialize the H2O cluster\nh2o.init()\n\n# Load your data into a H2OFrame\n# Assume strom_climate is a pandas DataFrame\nhf = h2o.H2OFrame(strom_climate)\n\n# Specify the target and predictor columns\ntarget = 'wd'\npredictors = [\n    \"tt_tu_min\", \"tt_tu_mean\", \"tt_tu_max\",\n\n    \"rf_tu_min\", \"rf_tu_mean\", \"rf_tu_max\",\n\n    'tt_min', 'tt_mean', 'tt_max',\n\n    'td_min', 'td_mean', 'td_max',\n\n    'vp_std_min', 'vp_std_mean', 'vp_std_max',\n\n    'tf_std_min', 'tf_std_mean', 'tf_std_max',\n\n    'p_std_min', 'p_std_mean', 'p_std_max',\n\n    'tt_std_min', 'tt_std_mean', 'tt_std_max',\n\n    'rf_std_min', 'rf_std_mean', 'rf_std_max',\n\n    'td_std_min', 'td_std_mean', 'td_std_max',\n\n    \"r1_min\", \"r1_mean\", \"r1_max\",\n    #'rs_ind_min', 'rs_ind_mean', 'rs_ind_max', 'wrtr_min', 'wrtr_mean', 'wrtr_max'\n]\n\n# Run AutoML\naml = H2OAutoML(seed=1, max_runtime_secs=3600) # 28800\naml.train(x=predictors, y=target, training_frame=hf)\n\n# View the AutoML Leaderboard\nlb = aml.leaderboard\n#print(lb.head(rows=lb.nrows))\nprint(lb.head(rows=10))\n\n\n# Get the best model\nbest_model = aml.leader\n\n# Make predictions on the strom_climate data\npredictions = best_model.predict(hf)\n\n# Convert predictions to pandas DataFrame\ny_pred = h2o.as_list(predictions)[\"predict\"]\n\n\nstrom.scatter_fitted_observed(y, y_pred, strom_climate)\n\n\nstrom.splom_fitted_observed(\n    y,\n    y_pred,\n    strom_climate,\n    dimensions=[\n        \"tt_tu_mean\",\n        \"tt_tu_min\",\n        \"tt_tu_max\",\n        \"rf_tu_mean\",\n        \"rf_tu_min\",\n        \"rf_tu_max\",\n        \"td_mean\",\n        \"vp_std_mean\",\n        \"r1_mean\"\n    ],\n).show()\n\n\nstrom.residuals_fitted(y, y_pred)\n\n\nstrom.residuals_hist(y, y_pred)\n\n\nstrom.residuals_qq(y, y_pred)\n\n\nbest_model.explain(hf)",
    "crumbs": [
      "Dashboard",
      "H20 AutoML"
    ]
  },
  {
    "objectID": "Pool/pool_temp.html",
    "href": "Pool/pool_temp.html",
    "title": "Pool",
    "section": "",
    "text": "import os\nfrom epyfun.fs import convert_to_utf8\n\n# Example usage:\n#file_path = './data/pooltemp/Outdoor_log_from_20230701_to_20230727.csv'\n#encoding = convert_to_utf8(file_path, \"interim/pooltemp\")\n#print(f\"The file encoding was: {encoding}\")\n\ninput_directory = './data/pooltemp'\nfor filename in os.listdir(input_directory):\n    if filename.endswith('.csv'):\n        input_file = os.path.join(input_directory, filename)\n        convert_to_utf8(input_file, 'interim/pooltemp/' + filename)\n\n\n%load_ext sql\n%config SqlMagic.autopandas = True\n%config SqlMagic.feedback = False\n%config SqlMagic.displaycon = False\n\nLoading configurations from /home/runner/work/strom/strom/pyproject.toml.\n\n\nSettings changed:\n\n\n\n\n\n\n\n\nConfig\nvalue\n\n\n\n\nfeedback\nTrue\n\n\nautopandas\nTrue\n\n\ndisplaycon\nFalse\n\n\ndsn_filename\n./connections.ini\n\n\n\n\n\n\n\n%sql duckdb:///:default:\n# %sql duckdb:///:memory:\n# %sql duckdb:///path/to/file.db\n\n\nIngest\n\n%%sql\n\nCREATE OR REPLACE VIEW pooltemp AS\nSELECT *\nFROM read_csv(\n  'interim/pooltemp/*.csv',\n  columns = {'time': 'TIMESTAMP', 'temp': 'DOUBLE'},\n  decimal_separator = ',',\n  delim = '\\t',\n  filename = True,\n  header = True,\n  skip = 1\n)\n;\n\n\n\n\n\n\n\n\n\nSuccess\n\n\n\n\n\n\n\n\n\n\n\n#%sqlcmd profile -t pooltemp;\n#this does not seem to be working, it does not work on saved queries, \n#nor if I create the view, nor if I create the table\n\n\n%%sql\n\nCREATE OR REPLACE MACRO add_dups_count(_srctbl, _cols) AS TABLE \n(SELECT *, COUNT(*) OVER (PARTITION BY _cols) AS _cnt\nFROM pooltemp)\n--TODO: figure out how to pass a table name to macros in DuckDB\n--FROM _srctbl\n;\n\n\n\n\n\n\n\n\n\nSuccess\n\n\n\n\n\n\n\n\n\n\n\n%%sql\n\nSELECT * FROM add_dups_count(pooltemp, time);\n\n\n\n\n\n\n\n\n\ntime\ntemp\nfilename\n_cnt\n\n\n\n\n0\n2023-06-14 20:52:50\n25.63\ninterim/pooltemp/Outdoor_log_from_20220101_to_...\n1\n\n\n1\n2023-06-14 20:53:00\n25.63\ninterim/pooltemp/Outdoor_log_from_20220101_to_...\n1\n\n\n2\n2023-06-14 20:54:10\n25.64\ninterim/pooltemp/Outdoor_log_from_20220101_to_...\n1\n\n\n3\n2023-06-14 20:54:30\n25.66\ninterim/pooltemp/Outdoor_log_from_20220101_to_...\n1\n\n\n4\n2023-06-14 20:55:40\n25.73\ninterim/pooltemp/Outdoor_log_from_20220101_to_...\n1\n\n\n...\n...\n...\n...\n...\n\n\n296575\n2023-08-12 20:43:20\n26.08\ninterim/pooltemp/Outdoor_log_from_20230701_to_...\n1\n\n\n296576\n2023-08-12 20:45:10\n26.08\ninterim/pooltemp/Outdoor_log_from_20230701_to_...\n1\n\n\n296577\n2023-08-12 20:45:40\n26.08\ninterim/pooltemp/Outdoor_log_from_20230701_to_...\n1\n\n\n296578\n2023-08-12 20:49:10\n26.08\ninterim/pooltemp/Outdoor_log_from_20230701_to_...\n1\n\n\n296579\n2023-08-12 20:50:10\n26.08\ninterim/pooltemp/Outdoor_log_from_20230701_to_...\n1\n\n\n\n\n296580 rows × 4 columns\n\n\n\n\n\n%%sql\n\nCREATE OR REPLACE MACRO count_dups_by(_srctbl, _cols) AS TABLE\nSELECT \n  COUNT(*) AS _tot,\n  COUNT(*) FILTER(WHERE _cnt &gt; 1) AS _dups,\n  COUNT(DISTINCT _cols) AS _uniq,\n  _dups - (_tot - _uniq) AS _duniq,\n  _dups / _duniq AS _puniq\nFROM add_dups_count(_srctbl, _cols) \n;\n\n\n\n\n\n\n\n\n\nSuccess\n\n\n\n\n\n\n\n\n\n\n\n%%sql\n\nSELECT * FROM count_dups_by(pooltemp, (time, temp));\n\n\n\n\n\n\n\n\n\n_tot\n_dups\n_uniq\n_duniq\n_puniq\n\n\n\n\n0\n296580\n42787\n270593\n16800\n2.546845\n\n\n\n\n\n\n\n\n\n%%sql --save pooltemp_clean\n\npooltemp_clean =&lt;&lt; SELECT time, avg(temp) AS temp\nFROM pooltemp\nGROUP BY time\nORDER BY time\n;\n\n\n\n\n\n\n\n\n\ntime\ntemp\n\n\n\n\n0\n2023-06-14 20:52:20\n25.63\n\n\n1\n2023-06-14 20:52:30\n25.63\n\n\n2\n2023-06-14 20:52:40\n25.63\n\n\n3\n2023-06-14 20:52:50\n25.63\n\n\n4\n2023-06-14 20:53:00\n25.63\n\n\n...\n...\n...\n\n\n267979\n2023-08-12 20:50:10\n26.08\n\n\n267980\n2023-08-12 20:50:20\n26.08\n\n\n267981\n2023-08-12 20:50:30\n26.08\n\n\n267982\n2023-08-12 20:50:40\n26.08\n\n\n267983\n2023-08-12 20:50:50\n26.08\n\n\n\n\n267984 rows × 2 columns\n\n\n\n\n\npooltemp_clean\n\n\n\n\n\n\n\n\n\ntime\ntemp\n\n\n\n\n0\n2023-06-14 20:52:20\n25.63\n\n\n1\n2023-06-14 20:52:30\n25.63\n\n\n2\n2023-06-14 20:52:40\n25.63\n\n\n3\n2023-06-14 20:52:50\n25.63\n\n\n4\n2023-06-14 20:53:00\n25.63\n\n\n...\n...\n...\n\n\n267979\n2023-08-12 20:50:10\n26.08\n\n\n267980\n2023-08-12 20:50:20\n26.08\n\n\n267981\n2023-08-12 20:50:30\n26.08\n\n\n267982\n2023-08-12 20:50:40\n26.08\n\n\n267983\n2023-08-12 20:50:50\n26.08\n\n\n\n\n267984 rows × 2 columns\n\n\n\n\n\nimport plotly.express as px\n\nimport pandas as pd\nimport numpy as np\n\nfig = px.scatter(pooltemp_clean, x=\"time\", y=\"temp\", render_mode='webgl')\nfig.update_traces(marker_line=dict(width=1, color='DarkSlateGray'))\nfig.show()\n\n                                                \n\n\n%%timeit %%sql SELECT COUNT(DISTINCT (time, temp)), COUNT(*) FROM pooltemp ;\n%%timeit %%sql SELECT COUNT() FROM ( SELECT time, COUNT() FROM pooltemp GROUP BY time HAVING COUNT(*) &gt; 1\n) ;\n%%timeit %%sql –save duplicates_detail\nSELECT  FROM ( SELECT , COUNT() OVER (PARTITION BY time) AS cnt COUNT() OVER (PARTITION BY time, temp) AS cnt2 FROM pooltemp ) WHERE cnt &gt; 1 – AND cnt &lt;&gt; cnt2 ORDER BY -cnt, time ;\n%%sql\nSELECT COUNT(*), COUNT(DISTINCT time),\nCOUNT(DISTINCT (time, temp))\nFROM duplicates_detail ;\n%%timeit %%sql\nSELECT time, avg(temp) AS temp FROM pooltemp GROUP BY time ORDER BY time\n%%sql\nCREATE OR REPLACE MACRO count_uniq_by(cols) AS TABLE SELECT COUNT(DISTINCT cols) FROM pooltemp",
    "crumbs": [
      "Dashboard",
      "Pool",
      "Pool"
    ]
  },
  {
    "objectID": "05_model.html",
    "href": "05_model.html",
    "title": "Model",
    "section": "",
    "text": "Naive model\nAfter fitting a multiple linear regression model using statsmodels, you can perform model diagnosis and check the residuals to assess the goodness of fit. Here are some steps you can take:\n\n1. Residuals Analysis:\nThis scatter plot helps you check for patterns in the residuals, such as heteroscedasticity or non-linearity. This plot helps you check for linearity and homoscedasticity.\n\n\n2. Normality of Residuals:\nThese plots help assess whether the residuals are normally distributed.\n\nScale-Location (Spread-Location) Plot: This plot helps in detecting heteroscedasticity.\n\n\n\n3. Homoscedasticity:\nThese plots help check for homoscedasticity (constant variance) of residuals across different levels of the independent variables.\n\n\n4. Influence and Outliers:\nThis plot helps identify influential points or outliers.\n\n\n5. Leverage-Residuals plot:\nThis plot helps identify points with high leverage.\n\n\n6. Cook’s Distance:\nThis plot helps identify influential observations.\n\n\n6. Residuals Autocorrelation Plot:\nThis plot helps assess if there is any autocorrelation in the residuals, which may be a concern in time series data.\n\n\n7. Partial Regression Plots:\nThese plots help identify the influence of each independent variable on the dependent variable while holding other variables constant.\nThese diagnostic plots can provide insights into the performance of your regression model and help you identify any violations of the assumptions of linear regression. Adjustments to the model or data transformation may be necessary based on the diagnostic results.\n\n\n7. Partial Regression (Added Variable) Plot:\nThis plot helps visualize the relationship between an individual predictor and the response variable while accounting for the other predictors in the model.\n\nResiduals vs. Time Plot (for Time Series Regression): If you are working with time series data, a plot of residuals against time can help identify patterns or trends.\n\nclearly the naive model is not a good fit.\n\nIt needs a polinomial with the temperature (second order perhaps)\ncheck if the association with relative humidity, if it is just an artifact of the correlation with temperature, or if there might something meaningful there going on\nand bring other climatic data, rainfall and snowfall might be relevant\n\n\n\n\nNaive model, but using scikit-learn\n\n\nPolinomial Model\n\n1. Residuals Analysis:\nThis scatter plot helps you check for patterns in the residuals, such as heteroscedasticity or non-linearity. This plot helps you check for linearity and homoscedasticity.\n\n\n2. Normality of Residuals:\nThese plots help assess whether the residuals are normally distributed.\n\nScale-Location (Spread-Location) Plot: This plot helps in detecting heteroscedasticity.\n\n\n\n3. Homoscedasticity:\nThese plots help check for homoscedasticity (constant variance) of residuals across different levels of the independent variables.\n\n\n4. Influence and Outliers:\nThis plot helps identify influential points or outliers.\n\n\n5. Leverage-Residuals plot:\nThis plot helps identify points with high leverage.\n\n\n6. Cook’s Distance:\nThis plot helps identify influential observations.\n\n\n6. Residuals Autocorrelation Plot:\nThis plot helps assess if there is any autocorrelation in the residuals, which may be a concern in time series data.\n\n\n7. Partial Regression Plots:\nThese plots help identify the influence of each independent variable on the dependent variable while holding other variables constant.\nThese diagnostic plots can provide insights into the performance of your regression model and help you identify any violations of the assumptions of linear regression. Adjustments to the model or data transformation may be necessary based on the diagnostic results.\n\n\n7. Partial Regression (Added Variable) Plot:\nThis plot helps visualize the relationship between an individual predictor and the response variable while accounting for the other predictors in the model.\n\nResiduals vs. Time Plot (for Time Series Regression): If you are working with time series data, a plot of residuals against time can help identify patterns or trends.\n\nhere’s also the polinomial model, but using scikit-learn\nsubstantial improvement Here, we still have some items in the to-do list and given this results, I would add a coulpe more\n\nIt needs a polinomial with the temperature (second order perhaps)\ncheck if the association with relative humidity, if it is just an artifact of the correlation with temperature, or if there might something meaningful there going on\nand bring other climatic data, rainfall and snowfall might be relevant\ndepite the much better fit, there are still some -although minor- heteroscedaticity, mostly associated to the lowest temperatures where the variance of strom use is higher\nthere might be also a seasonal pattern, because we are not accounting for, for example, weekends and so on\nfinally, there is still some evidence of autocorrelation. Which kinda makes sense: it is not the same to have one day with a very low temperature compared to a whole week with very low temperatures: while the former could be wintered with the heat already produced, as long as there is not much energy lose, the latter would certainly need much more work from the wärmepumpe to keep fighting the cold. In that sense, it seems unrealistic to just assume that the increase energy consumption of several days of extreme cold is just the sum of the daily increases. So if the one-day-increase due to extreme cold is 10, five consecutve days would probably result in an increase greater than 10*5=50, probably much greater. But let’s explore such hipothesis in the data and try to accomodate that in the model using lags, or moving average of the last few days.\ninfluence plot and leverage plots should be interactive with tooltips with all the data associated to the points. Cook’s distance perhaps as well.\nrun some PCA, with min, max mean and so on and think about a Cumulative Explained Variance plot https://archive.is/X1wrZ#selection-1059.8-1059.37\nand use cross validation\nperhaps experiment with feature selection\nand consider log-transforming the target, after all, the distribution of it is right-skewed with relatively long right tail",
    "crumbs": [
      "Dashboard",
      "Model"
    ]
  },
  {
    "objectID": "04_climate_and_strom.html",
    "href": "04_climate_and_strom.html",
    "title": "Correlations",
    "section": "",
    "text": "A few rows have different number of obs. How can this be?\n\n\n2016-11-28', '2017-11-25', '2018-11-23', '2018-11-25', '2019-11-29', '2020-11-30', '2020-12-01', '2021-01-23', '2023-01-02', '2023-01-08', '2024-03-12\n\n\n\n\n\n\n\n\n\n\ndate\nnd\nnobs\nwd\nwobs\n\n\n\n\n0\n2016-11-28\n5.580764\n1.0\n5.023351\n2.0\n\n\n362\n2017-11-25\n6.252045\n1.0\n13.378399\n2.0\n\n\n725\n2018-11-23\n7.425505\n0.0\n12.477706\n2.0\n\n\n727\n2018-11-25\n8.110218\n1.0\n12.125442\n0.0\n\n\n1096\n2019-11-29\n9.189087\n1.0\n11.693605\n2.0\n\n\n1463\n2020-11-30\n8.763889\n3.0\n26.272154\n2.0\n\n\n1464\n2020-12-01\n10.981818\n3.0\n30.037212\n4.0\n\n\n1517\n2021-01-23\n8.297101\n3.0\n19.039309\n4.0\n\n\n1673\n2023-01-02\n7.981242\n6.0\n10.518445\n5.0\n\n\n1679\n2023-01-08\n7.703441\n3.0\n18.456920\n4.0\n\n\n2108\n2024-03-12\n9.275517\n2.0\n9.223981\n3.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nminute\ndate\nvalue\nvalue_hoch\nvalue_niedrig\nminutes\nconsumption\ncm\ncm_hoch\ncm_niedrig\n\n\n\n\n0\n2016-11-28 00:21:00\n2016-11-28 00:21:00\n1031\n547\n484\n1440\n11\n0.007639\n0.007639\n0.007639\n\n\n1\n2016-11-28 15:23:00\n2016-11-28 15:23:00\n1031\n547\n484\n902\n0\n0.000000\n0.000000\n0.000000\n\n\n2\n2017-11-25 15:20:00\n2017-11-25 15:20:00\n6024\n3347\n2677\n521277\n4993\n0.009578\n0.005371\n0.004207\n\n\n3\n2017-11-25 15:23:00\n2017-11-25 15:23:00\n6024\n3347\n2677\n3\n0\n0.000000\n0.000000\n0.000000\n\n\n4\n2018-11-23 15:19:00\n2018-11-23 15:19:00\n10640\n5747\n4893\n522716\n4616\n0.008831\n0.004591\n0.004239\n\n\n5\n2018-11-23 15:22:00\n2018-11-23 15:22:00\n10640\n5747\n4893\n3\n0\n0.000000\n0.000000\n0.000000\n\n\n6\n2019-11-29 14:18:00\n2019-11-29 14:18:00\n15138\n7914\n7224\n534176\n4498\n0.008420\n0.004057\n0.004364\n\n\n7\n2019-11-29 14:22:00\n2019-11-29 14:22:00\n15138\n7914\n7224\n4\n0\n0.000000\n0.000000\n0.000000\n\n\n8\n2020-11-30 07:07:00\n2020-11-30 07:07:00\n19220\n9764\n9456\n528045\n4082\n0.007730\n0.003503\n0.004227\n\n\n9\n2020-11-30 17:36:00\n2020-11-30 17:36:00\n19229\n9773\n9456\n629\n9\n0.014308\n0.014308\n0.000000\n\n\n10\n2020-12-01 00:00:00\n2020-12-01 00:00:00\n19243\n9779\n9464\n384\n14\n0.036458\n0.015625\n0.020833\n\n\n11\n2020-12-01 07:18:00\n2020-12-01 07:18:00\n19252\n9782\n9470\n438\n9\n0.020548\n0.006849\n0.013699\n\n\n12\n2020-12-01 14:23:00\n2020-12-01 14:23:00\n19262\n9792\n9470\n424\n10\n0.023585\n0.023585\n0.000000\n\n\n13\n2020-12-01 20:21:00\n2020-12-01 20:21:00\n19268\n9798\n9470\n358\n6\n0.016760\n0.016760\n0.000000\n\n\n14\n2021-01-23 08:25:00\n2021-01-23 08:25:00\n20476\n10326\n10150\n851\n8\n0.009401\n0.002350\n0.007051\n\n\n15\n2021-01-23 17:10:00\n2021-01-23 17:10:00\n20483\n10326\n10157\n525\n7\n0.013333\n0.000000\n0.013333\n\n\n16\n2021-01-23 20:27:00\n2021-01-23 20:27:00\n20486\n10326\n10160\n197\n3\n0.015228\n0.000000\n0.015228\n\n\n17\n2021-01-23 22:50:00\n2021-01-23 22:50:00\n20489\n10326\n10163\n142\n3\n0.021127\n0.000000\n0.021127\n\n\n18\n2023-01-02 07:18:00\n2023-01-02 07:18:00\n28925\n14105\n14820\n896\n7\n0.007812\n0.001116\n0.006696\n\n\n19\n2023-01-02 12:38:00\n2023-01-02 12:38:00\n28927\n14107\n14820\n320\n2\n0.006250\n0.006250\n0.000000\n\n\n20\n2023-01-02 16:59:00\n2023-01-02 16:59:00\n28929\n14109\n14820\n261\n2\n0.007663\n0.007663\n0.000000\n\n\n21\n2023-01-02 20:57:00\n2023-01-02 20:57:00\n28930\n14110\n14820\n238\n1\n0.004202\n0.004202\n0.000000\n\n\n22\n2023-01-02 23:47:00\n2023-01-02 23:47:00\n28932\n14110\n14822\n169\n2\n0.011834\n0.000000\n0.011834\n\n\n23\n2023-01-08 08:33:00\n2023-01-08 08:33:00\n29007\n14137\n14870\n516\n6\n0.011628\n0.000000\n0.011628\n\n\n24\n2023-01-08 10:06:00\n2023-01-08 10:06:00\n29008\n14137\n14871\n93\n1\n0.010753\n0.000000\n0.010753\n\n\n25\n2023-01-08 16:27:00\n2023-01-08 16:27:00\n29016\n14137\n14879\n381\n8\n0.020997\n0.000000\n0.020997\n\n\n26\n2023-01-08 20:07:00\n2023-01-08 20:07:00\n29017\n14137\n14880\n219\n1\n0.004566\n0.000000\n0.004566\n\n\n27\n2024-03-12 07:05:00\n2024-03-12 07:05:00\n34286\n16383\n17903\n2813\n20\n0.007110\n0.002133\n0.004977\n\n\n28\n2024-03-12 07:35:00\n2024-03-12 07:35:00\n34290\n16387\n17903\n30\n4\n0.133333\n0.133333\n0.000000\n\n\n29\n2024-03-12 19:35:00\n2024-03-12 19:35:00\n34290\n16387\n17903\n720\n0\n0.000000\n0.000000\n0.000000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nminute\ndate\nvalue\nminutes\nconsumption\ncm\n\n\n\n\n0\n2016-11-28 16:16:00\n2016-11-28 16:16:00\n659\nNaN\nNaN\n0.003876\n\n\n1\n2017-11-25 15:15:00\n2017-11-25 15:15:00\n2679\n521219.0\n2020.0\n0.003876\n\n\n2\n2018-11-25 14:15:00\n2018-11-25 14:15:00\n5389\n525540.0\n2710.0\n0.005157\n\n\n3\n2019-11-29 14:14:00\n2019-11-29 14:14:00\n8752\n531359.0\n3363.0\n0.006329\n\n\n4\n2020-11-30 07:07:00\n2020-11-30 07:07:00\n12162\n528053.0\n3410.0\n0.006458\n\n\n5\n2020-11-30 17:35:00\n2020-11-30 17:35:00\n12165\n628.0\n3.0\n0.004777\n\n\n6\n2020-11-30 23:59:00\n2020-11-30 23:59:00\n12168\n384.0\n3.0\n0.007812\n\n\n7\n2020-12-01 07:18:00\n2020-12-01 07:18:00\n12170\n439.0\n2.0\n0.004556\n\n\n8\n2020-12-01 14:23:00\n2020-12-01 14:23:00\n12173\n425.0\n3.0\n0.007059\n\n\n9\n2020-12-01 20:21:00\n2020-12-01 20:21:00\n12177\n358.0\n4.0\n0.011173\n\n\n10\n2021-01-23 08:25:00\n2021-01-23 08:25:00\n12682\n851.0\n4.0\n0.004700\n\n\n11\n2021-01-23 17:10:00\n2021-01-23 17:10:00\n12686\n525.0\n4.0\n0.007619\n\n\n12\n2021-01-23 20:28:00\n2021-01-23 20:28:00\n12687\n198.0\n1.0\n0.005051\n\n\n13\n2023-01-02 07:18:00\n2023-01-02 07:18:00\n18801\n895.0\n4.0\n0.004469\n\n\n14\n2023-01-02 08:58:00\n2023-01-02 08:58:00\n18802\n100.0\n1.0\n0.010000\n\n\n15\n2023-01-02 12:39:00\n2023-01-02 12:39:00\n18803\n221.0\n1.0\n0.004525\n\n\n16\n2023-01-02 16:59:00\n2023-01-02 16:59:00\n18804\n260.0\n1.0\n0.003846\n\n\n17\n2023-01-02 20:58:00\n2023-01-02 20:58:00\n18806\n239.0\n2.0\n0.008368\n\n\n18\n2023-01-02 23:47:00\n2023-01-02 23:47:00\n18807\n169.0\n1.0\n0.005917\n\n\n19\n2023-01-08 08:33:00\n2023-01-08 08:33:00\n18848\n516.0\n1.0\n0.001938\n\n\n20\n2023-01-08 16:27:00\n2023-01-08 16:27:00\n18851\n474.0\n3.0\n0.006329\n\n\n21\n2023-01-08 20:07:00\n2023-01-08 20:07:00\n18854\n220.0\n3.0\n0.013636\n\n\n22\n2024-03-12 07:05:00\n2024-03-12 07:05:00\n22640\n2813.0\n14.0\n0.004977\n\n\n23\n2024-03-12 19:35:00\n2024-03-12 19:35:00\n22646\n750.0\n6.0\n0.008000\n\n\n\n\n\n\n\n\nOk, there are a couple of cases explained by measurement at midnight, and waermestrom was measured before midnight and normal storm after midnight, so it counts at the next day. Other case seems to simply be that I did not measure normal strom (presumably, because there was not measurable consumption, due to short period)\nso let’s just keep one of those and move on\n\n\n\n\n\n\n\n\n\n\ndate\nnd\nwd\nobs\n\n\n\n\n0\n2016-11-28\n5.580764\n5.023351\n2.0\n\n\n1\n2016-11-29\n5.580764\n13.792897\n0.0\n\n\n2\n2016-11-30\n5.580764\n13.792897\n0.0\n\n\n3\n2016-12-01\n5.580764\n13.792897\n0.0\n\n\n4\n2016-12-02\n5.580764\n13.792897\n0.0\n\n\n...\n...\n...\n...\n...\n\n\n2180\n2024-05-23\n7.842645\n7.685974\n1.0\n\n\n2181\n2024-05-24\n10.919332\n6.546488\n2.0\n\n\n2182\n2024-05-25\n10.313539\n6.011990\n1.0\n\n\n2183\n2024-05-26\n13.428571\n7.434112\n4.0\n\n\n2184\n2024-05-27\n2.589928\n2.585278\n2.0\n\n\n\n\n2185 rows × 4 columns\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndate\nnd\nwd\nobs\ntt_tu_min\ntt_tu_mean\ntt_tu_max\nrf_tu_min\nrf_tu_mean\nrf_tu_max\n...\ntd_std_max\nr1_min\nr1_mean\nr1_max\nrs_ind_min\nrs_ind_mean\nrs_ind_max\nwrtr_min\nwrtr_mean\nwrtr_max\n\n\n\n\n0\n2016-11-28\n5.580764\n5.023351\n2.0\n-4.6\n-0.166667\n3.0\n59.0\n78.979167\n97.0\n...\n1.6\n0.0\n0.000000\n0.0\n0.0\n0.000000\n0.0\nNaN\nNaN\nNaN\n\n\n1\n2016-11-29\n5.580764\n13.792897\n0.0\n-7.0\n-3.604167\n1.4\n58.0\n81.000000\n94.0\n...\n-5.1\n0.0\n0.000000\n0.0\n0.0\n0.000000\n0.0\nNaN\nNaN\nNaN\n\n\n2\n2016-11-30\n5.580764\n13.792897\n0.0\n-7.2\n-1.610417\n5.8\n45.0\n77.750000\n92.0\n...\n-2.0\n0.0\n0.000000\n0.0\n0.0\n0.000000\n0.0\nNaN\nNaN\nNaN\n\n\n3\n2016-12-01\n5.580764\n13.792897\n0.0\n-2.3\n0.845833\n4.7\n59.0\n78.916667\n94.0\n...\n-0.4\n0.0\n0.000000\n0.0\n0.0\n0.000000\n0.0\nNaN\nNaN\nNaN\n\n\n4\n2016-12-02\n5.580764\n13.792897\n0.0\n-2.2\n2.022917\n6.6\n63.0\n82.604167\n99.0\n...\n2.7\n0.0\n0.039583\n0.4\n0.0\n0.229167\n1.0\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2180\n2024-05-23\n7.842645\n7.685974\n1.0\n9.8\n13.345833\n20.0\n56.0\n87.000000\n98.0\n...\n14.1\n0.0\n0.595833\n11.9\n0.0\n0.166667\n1.0\nNaN\nNaN\nNaN\n\n\n2181\n2024-05-24\n10.919332\n6.546488\n2.0\n9.3\n13.904167\n19.7\n53.0\n77.875000\n97.0\n...\n12.4\n0.0\n0.000000\n0.0\n0.0\n0.125000\n1.0\nNaN\nNaN\nNaN\n\n\n2182\n2024-05-25\n10.313539\n6.011990\n1.0\n10.9\n13.683333\n17.8\n60.0\n84.083333\n98.0\n...\n12.2\n0.0\n0.116667\n1.1\n0.0\n0.250000\n1.0\nNaN\nNaN\nNaN\n\n\n2183\n2024-05-26\n13.428571\n7.434112\n4.0\n9.4\n15.970833\n22.3\n47.0\n72.833333\n97.0\n...\n12.2\n0.0\n0.000000\n0.0\n0.0\n0.000000\n0.0\nNaN\nNaN\nNaN\n\n\n2184\n2024-05-27\n2.589928\n2.585278\n2.0\n12.3\n16.820833\n23.4\n51.0\n77.958333\n97.0\n...\n15.9\n0.0\n0.141667\n1.7\n0.0\n0.166667\n1.0\nNaN\nNaN\nNaN\n\n\n\n\n2185 rows × 43 columns\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                \n\n\n\n\n                                                \n\n\n\n\n                                                \n\n\nsome first-look insights\n\nit’s good that the correlation between normal and warme strom consumption is very low. It is expected, and could be useful to use normal strom utilization as a proxy for other things (i.e. being at home basically)\nalso as expected, the correlation between normal strom utilization and temprature is low.\nthere is a nice and strong negative correlation between temperature and wärmestrom consumption. It is not strictly linear though. Rather polinomial (inverse) relatioship.\nthere seems to be some sort of weak positive correlation between wärmestrom and relative humidity. Not at all consistent though. It is positive with min and mean humidity, but negative or non-existent with max humidity. It may well be just an artifact or spurious correlation, explained by temperature, because there is indeed a correlation (not that strong, though) between temperature and humidity.\nthe number of observations are somewhat related to the temperature, which kinda makes sense, because the measurements are not random. I am much more likely to measure the thing when it’s cold!, and that seems to be reflected in the data.",
    "crumbs": [
      "Dashboard",
      "Correlations"
    ]
  },
  {
    "objectID": "Setup/03_load_data.html",
    "href": "Setup/03_load_data.html",
    "title": "C. Loading Data",
    "section": "",
    "text": "We need to fire up DuckDB, in-memory or pointing to a file.\n```{python}\n%%sql\n\nduckdb:///:memory:\n-- duckdb:///path/to/file.db\n```\nAnd the data come in a SQLite file. DuckDB has a SQLite extension, so we just need to make sure it is installed and load it.\n```{python}\n%%sql\n\nINSTALL sqlite;\nLOAD sqlite;\n```\nAnd finally we just need to attach the SQLite file, which creates a bunch of views in the DuckDB side, for each table in the SQLite file.\n```{python}\n%%sql\n\nCALL sqlite_attach('data/2022-12-28-ecas-export.db');\nPRAGMA show_tables;\n```\nWith everything loaded, we can simply query the tables. For example, let’s see the data about the meters.\n\n%%sql \n\nSELECT * FROM meter;\n\n\n\n\n\n\n\n\n\n_id\nname\nunits\ncomment\nvsf\ntsf\ncost\nfcost\ninvert\nvmsetup\ntype\ncurrency\nscaling\nphyunits\nbidir\nprod\n\n\n\n\n0\n1\n[83, 116, 114, 111, 109]\n[]\n[]\n[53, 48]\n[53, 48]\n[]\n[]\n[48]\nNaN\n[48]\n[36]\n[49, 46, 48]\n[]\n[48]\n[48]\n\n\n1\n2\n[87, 195, 164, 114, 109, 101, 115, 116, 114, 1...\n[]\n[]\n[53, 48]\n[53, 48]\n[]\n[]\n[48]\nNaN\n[48]\n[36]\n[49, 46, 48]\n[]\n[48]\n[48]\n\n\n2\n3\n[87, 195, 164, 114, 109, 101, 115, 116, 114, 1...\n[]\n[]\n[53, 48]\n[53, 48]\n[]\n[]\n[48]\nNaN\n[48]\n[36]\n[49, 46, 48]\n[]\n[48]\n[48]\n\n\n3\n4\n[87, 97, 115, 115, 101, 114]\n[]\n[]\n[53, 48]\n[53, 48]\n[]\n[]\n[48]\nNaN\n[48]\n[36]\n[49, 46, 48]\n[]\n[48]\n[48]\n\n\n4\n5\n[76, 195, 188, 102, 116, 117, 110, 103, 115, 9...\n[]\n[]\n[53, 48]\n[53, 48]\n[]\n[]\n[48]\nNaN\n[48]\n[36]\n[49, 46, 48]\n[]\n[48]\n[48]\n\n\n5\n6\n[87, 97, 115, 99, 104, 109, 97, 115, 99, 104, ...\n[]\n[]\n[53, 48]\n[53, 48]\n[]\n[]\n[48]\nNaN\n[48]\n[36]\n[49, 46, 48]\n[]\n[48]\n[48]\n\n\n6\n7\n[71, 101, 115, 99, 104, 105, 114, 114, 115, 11...\n[]\n[]\n[53, 48]\n[53, 48]\n[]\n[]\n[48]\nNaN\n[48]\n[36]\n[49, 46, 48]\n[]\n[48]\n[48]\n\n\n7\n8\n[79, 102, 102, 101, 110]\n[]\n[]\n[53, 48]\n[53, 48]\n[]\n[]\n[48]\nNaN\n[48]\n[36]\n[49, 46, 48]\n[]\n[48]\n[48]\n\n\n8\n9\n[82, 101, 115, 101, 116]\n[]\n[]\n[53, 48]\n[53, 48]\n[]\n[]\n[48]\nNaN\n[48]\n[36]\n[49, 46, 48]\n[]\n[48]\n[48]\n\n\n9\n10\n[78, 97, 99, 104, 108, 97, 100, 101, 110]\n[]\n[]\n[53, 48]\n[53, 48]\n[]\n[]\n[48]\nNaN\n[48]\n[36]\n[49, 46, 48]\n[]\n[48]\n[48]\n\n\n10\n11\n[84, 101, 109, 112]\n[]\n[]\n[53, 48]\n[53, 48]\n[]\n[]\n[48]\nNaN\n[48]\n[36]\n[49, 46, 48]\n[]\n[48]\n[48]\n\n\n11\n12\n[87, 195, 164, 114, 109, 101, 114]\n[]\n[]\n[53, 48]\n[53, 48]\n[]\n[]\n[48]\nNaN\n[48]\n[36]\n[49, 46, 48]\n[]\n[48]\n[48]\n\n\n\n\n\n\n\n\nTODO: Temporarilly we will put this in a file to include. Later we will improve how to load and share the data across scripts.\nAlso, let’s not use sqlite_attach. Otherwise you would need a new session every time you want to attach a new file because it fails trying to attach again the same table names (the views already exist). And there does not seem to be a way of telling sqlite_attach to simply replace everything. So better to use sqlite_scan to query only what we need.",
    "crumbs": [
      "Dashboard",
      "Setup",
      "C. Loading Data"
    ]
  },
  {
    "objectID": "Setup/02_duckdb.html",
    "href": "Setup/02_duckdb.html",
    "title": "B. DuckDB",
    "section": "",
    "text": "We will be setting-up DuckDB to work in Jupyter-powered Quarto Documents.\n\nSet-up jupysql to be able to write sql queries directly\nTo do it with DuckDB, basically following this guide, just need to make sure jupysql, SQLAlchemy and duckdb-engine are installed, besides the core libraries (notebook, pandas, duckdb). If any of them mssing, simply pip install them.\nStep 1 is then to import extension. It enables SQL cells in Jupyter. It supports inline SQL using %sql and a whole SQL cell starting it with %%sql.\n\nimport duckdb\nimport pandas as pd\nimport sqlalchemy # No need to import duckdb_engine, \n                  # SQLAlchemy will auto-detect \n\n%load_ext sql\n%config SqlMagic.autopandas = True\n%config SqlMagic.feedback = False\n%config SqlMagic.displaycon = False\n\nLoading configurations from /home/runner/work/strom/strom/pyproject.toml.\n\n\nSettings changed:\n\n\n\n\n\n\n\n\nConfig\nvalue\n\n\n\n\nfeedback\nTrue\n\n\nautopandas\nTrue\n\n\ndisplaycon\nFalse\n\n\ndsn_filename\n./connections.ini\n\n\n\n\n\n\nI prefer Quarto to edit my notebooks, and the above still works. However, it seems Quarto’s SQL engine is still only for R since it requires knitr and does not seem to support the combo ipython-sql-SQLAlchemy. So you cannot simply use an SQl chunk like this\n```{sql}\nSELECT * FROM test;\n```\nBut you have to use a standard python chunk and use the %sql or %%sql to be able to write SQL direcly.\nStep 2 is to fire-up DuckDB, either in memory or pointing to a file.\n\n%sql duckdb:///:memory:\n# %sql duckdb:///path/to/file.db\n\nTest it’s working\n\n%sql SELECT 'Off and flying!' as a_duckdb_column\n\n\n\n\n\n\n\n\n\na_duckdb_column\n\n\n\n\n0\nOff and flying!\n\n\n\n\n\n\n\n\n\n%sql SELECT * FROM duckdb_settings();\n\n\n\n\n\n\n\n\n\nname\nvalue\ndescription\ninput_type\n\n\n\n\n0\naccess_mode\nautomatic\nAccess mode of the database (AUTOMATIC, READ_O...\nVARCHAR\n\n\n1\ncheckpoint_threshold\n16.7MB\nThe WAL size threshold at which to automatical...\nVARCHAR\n\n\n2\ndebug_checkpoint_abort\nnone\nDEBUG SETTING: trigger an abort while checkpoi...\nVARCHAR\n\n\n3\ndebug_force_external\nfalse\nDEBUG SETTING: force out-of-core computation f...\nBOOLEAN\n\n\n4\ndebug_force_no_cross_product\nfalse\nDEBUG SETTING: Force disable cross product gen...\nBOOLEAN\n\n\n...\n...\n...\n...\n...\n\n\n63\ncustom_user_agent\n\nMetadata from DuckDB callers\nVARCHAR\n\n\n64\npandas_analyze_sample\n1000\nThe maximum number of rows to sample when anal...\nUBIGINT\n\n\n65\nbinary_as_string\n\nIn Parquet files, interpret binary data as a s...\nBOOLEAN\n\n\n66\nCalendar\ngregorian\nThe current calendar\nVARCHAR\n\n\n67\nTimeZone\nEtc/UTC\nThe current time zone\nVARCHAR\n\n\n\n\n68 rows × 4 columns\n\n\n\n\nWe will need to make sure we load the extension for all Quarto documents, as they will all be different jupyter notebooks (TODO: double-check this). So we will use Quarto includes to keep the source files not-so-verbose.\n\nNote that we use an underscore (_) prefix for the included file. You should always use an underscore prefix with included files so that they are automatically ignored (i.e. not treated as standalone files) by a quarto render of a project).\n\nAlso, remember the “trick” here\n\nIn Quarto projects, an include path that begins with a leading slash will be interpreted as project relative, meaning that you should be able to use an include such as:\n\n{{&lt; include /_codebit.qmd &gt;}}\n\nto include from the project root no matter the subdirectory of the file itself.",
    "crumbs": [
      "Dashboard",
      "Setup",
      "B. DuckDB"
    ]
  },
  {
    "objectID": "Process/02_waermestrom_process.html",
    "href": "Process/02_waermestrom_process.html",
    "title": "Wärmestrom",
    "section": "",
    "text": "https://learnsql.com/blog/moving-average-in-sql/ https://stackoverflow.com/questions/55491046/how-to-set-the-running-file-path-of-jupyter-in-vscode\n\nGet Wärmestrom data\nBy querying directly the SQLite table and filtering by meter id. But for Wärmestrom we have two different meters, because there are two tarifs depending on the time of the day.\n\n%%sql \n\nCREATE OR REPLACE TABLE waermestrom_sqlite AS \nSELECT \n  meterid, \n  -- Blob Functions, because most columns get read as blob\n  -- https://duckdb.org/docs/sql/functions/blob\n  decode(date)::DATETIME AS date, \n  decode(value)::INT AS value\nFROM sqlite_scan('{{latest_file}}', 'reading') \nWHERE meterid = 2 OR meterid = 3\n;\nSELECT * FROM waermestrom_sqlite;\n\n\n\n\n\n\n\n\n\nmeterid\ndate\nvalue\n\n\n\n\n0\n2\n2020-11-30 17:36:00\n9456\n\n\n1\n2\n2020-12-01 00:00:00\n9464\n\n\n2\n2\n2020-12-01 14:23:00\n9470\n\n\n3\n2\n2020-12-01 07:19:00\n9470\n\n\n4\n2\n2020-12-01 20:22:00\n9470\n\n\n...\n...\n...\n...\n\n\n2058\n3\n2017-11-25 15:20:00\n3347\n\n\n2059\n3\n2016-11-28 00:21:00\n547\n\n\n2060\n3\n2024-05-27 06:32:00\n2362\n\n\n2061\n3\n2021-12-02 18:14:00\n11778\n\n\n2062\n3\n2024-05-27 09:16:00\n2363\n\n\n\n\n2063 rows × 3 columns\n\n\n\n\nIdeally, there would be one measurement for each tarif, for every date (minute). But we cannot guarantee that’s the case (e.g. measurement for one tarif can be at 13:08:59 and for the other at 13:09:00). So let’s see if that’s the case.\n\n%%sql \n\nSELECT date, count(*) AS cnt\nFROM waermestrom_sqlite\nGROUP BY date\n;\n\n\n\n\n\n\n\n\n\ndate\ncnt\n\n\n\n\n0\n2020-12-01 00:00:00\n2\n\n\n1\n2020-12-02 12:48:00\n2\n\n\n2\n2020-12-03 08:53:00\n2\n\n\n3\n2020-12-07 17:16:00\n1\n\n\n4\n2020-12-08 19:50:00\n1\n\n\n...\n...\n...\n\n\n1130\n2024-01-10 10:40:00\n1\n\n\n1131\n2024-01-14 09:40:00\n1\n\n\n1132\n2024-02-14 06:02:00\n1\n\n\n1133\n2024-03-04 05:54:00\n1\n\n\n1134\n2024-03-06 07:32:00\n1\n\n\n\n\n1135 rows × 2 columns\n\n\n\n\nYeap, there are some cases with cnt=1. More precisely, the below number of cases:\n\n%%sql \n\nWITH ucnt AS (\n  SELECT date, count(*) AS cnt\n  FROM waermestrom_sqlite\n  GROUP BY date\n)\nSELECT cnt, COUNT(*) FROM ucnt GROUP BY cnt\n;\n\n\n\n\n\n\n\n\n\ncnt\ncount_star()\n\n\n\n\n0\n2\n928\n\n\n1\n1\n207\n\n\n\n\n\n\n\n\nSafest would be to just join the dates and make sure to fill in the gaps with the closest value. Let’s see\n\n%%sql\n\nCREATE OR REPLACE TABLE waermestrom_nulls AS\nWITH\nws181 AS (\n  SELECT \n    'Hoch' AS tariff,\n    decode(date)::DATETIME AS date, \n    decode(value)::INT AS value\n  FROM sqlite_scan('{{latest_file}}', 'reading') \n  WHERE meterid = 3 \n),\nws182 AS (\n  SELECT \n    'Niedrig' AS tariff, \n    decode(date)::DATETIME AS date, \n    decode(value)::INT AS value\n  FROM sqlite_scan('{{latest_file}}', 'reading') \n  WHERE meterid = 2\n)\nSELECT\n  COALESCE(ws181.date, ws182.date) AS date,\n  ws181.value AS value_hoch,\n  ws182.value AS value_niedrig\nFROM ws181 \nFULL JOIN ws182 \nON ws181.date = ws182.date\nORDER BY date\n;\nSELECT * FROM waermestrom_nulls LIMIT 20;\n\n\n\n\n\n\n\n\n\ndate\nvalue_hoch\nvalue_niedrig\n\n\n\n\n0\n2016-11-28 00:21:00\n547.0\nNaN\n\n\n1\n2016-11-28 15:23:00\nNaN\n484.0\n\n\n2\n2017-11-25 15:20:00\n3347.0\nNaN\n\n\n3\n2017-11-25 15:23:00\nNaN\n2677.0\n\n\n4\n2018-11-23 15:19:00\n5747.0\nNaN\n\n\n5\n2018-11-23 15:22:00\nNaN\n4893.0\n\n\n6\n2019-11-29 14:18:00\n7914.0\nNaN\n\n\n7\n2019-11-29 14:22:00\nNaN\n7224.0\n\n\n8\n2020-11-30 07:07:00\n9764.0\n9456.0\n\n\n9\n2020-11-30 17:36:00\n9773.0\n9456.0\n\n\n10\n2020-12-01 00:00:00\n9779.0\n9464.0\n\n\n11\n2020-12-01 07:18:00\n9782.0\nNaN\n\n\n12\n2020-12-01 07:19:00\nNaN\n9470.0\n\n\n13\n2020-12-01 14:23:00\n9792.0\n9470.0\n\n\n14\n2020-12-01 20:21:00\n9798.0\nNaN\n\n\n15\n2020-12-01 20:22:00\nNaN\n9470.0\n\n\n16\n2020-12-02 00:01:00\n9800.0\n9473.0\n\n\n17\n2020-12-02 07:29:00\n9802.0\n9476.0\n\n\n18\n2020-12-02 12:48:00\n9810.0\n9476.0\n\n\n19\n2020-12-03 02:38:00\n9821.0\n9480.0\n\n\n\n\n\n\n\n\nYeah, those are the cases: 2020-12-01 07:18:00 has value hoch, but no niedrig 2020-12-01 07:19:00 has value niedrig, but no hoch\nSo now we want to fill in those gaps using the value of the same column, that has the closest date. So it’s tricky, because it cannot simply be a fill-down, or fill-up. Because in one case, the correct value would be one position up, and in other case one position down. Here’s one approach (it just assumes there are no consecutive nulls for the value columns; please also note that it takes advantage of DuckDB’s flexible SQL syntax -otherwhise it would have been even longer, with a bunch of CTEs-)\n\n%%sql\n\nCREATE OR REPLACE TABLE waermestrom_nonulls AS\nSELECT\n  date,\n  value_hoch, value_niedrig, \n  -- calculate minutes diff with previous and next date, to see which is closer\n  -- note the use of a default value for lag/lead, substracting and adding one day\n  -- for lag and lead respectively, to avoid NULLs in the first and las rows\n  date_sub('minute', lag(date, 1, date - INTERVAL 1 DAY) over(order by date), date) AS minutes_lag,\n  date_sub('minute', date, lead(date, 1, date + INTERVAL 1 DAY) over(order by date)) AS minutes_lead,\n  -- and we want to replace null values column, with the value from closest date\n  CASE\n    WHEN value_hoch IS NULL AND minutes_lag &lt;= minutes_lead \n    THEN lag(value_hoch) over(order by date)\n    WHEN value_hoch IS NULL AND minutes_lag &gt; minutes_lead \n    THEN lead(value_hoch) over(order by date)\n    ELSE value_hoch\n  END AS value_hoch_fix,\n  CASE\n    WHEN value_niedrig IS NULL AND minutes_lag &lt;= minutes_lead \n    THEN lag(value_niedrig) over(order by date)\n    WHEN value_niedrig IS NULL AND minutes_lag &gt; minutes_lead \n    THEN lead(value_niedrig) over(order by date)\n    ELSE value_niedrig\n  END AS value_niedrig_fix,\n  value_hoch_fix + value_niedrig_fix AS value\nFROM waermestrom_nulls \nORDER BY date\n;\nSELECT * FROM waermestrom_nonulls ORDER BY date;\n\n\n\n\n\n\n\n\n\ndate\nvalue_hoch\nvalue_niedrig\nminutes_lag\nminutes_lead\nvalue_hoch_fix\nvalue_niedrig_fix\nvalue\n\n\n\n\n0\n2016-11-28 00:21:00\n547.0\nNaN\n1440\n902\n547\n484\n1031\n\n\n1\n2016-11-28 15:23:00\nNaN\n484.0\n902\n521277\n547\n484\n1031\n\n\n2\n2017-11-25 15:20:00\n3347.0\nNaN\n521277\n3\n3347\n2677\n6024\n\n\n3\n2017-11-25 15:23:00\nNaN\n2677.0\n3\n522716\n3347\n2677\n6024\n\n\n4\n2018-11-23 15:19:00\n5747.0\nNaN\n522716\n3\n5747\n4893\n10640\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1130\n2024-05-26 09:39:00\n2362.0\n71.0\n121\n586\n2362\n71\n2433\n\n\n1131\n2024-05-26 19:25:00\n2362.0\n73.0\n586\n272\n2362\n73\n2435\n\n\n1132\n2024-05-26 23:57:00\n2362.0\n75.0\n272\n395\n2362\n75\n2437\n\n\n1133\n2024-05-27 06:32:00\n2362.0\n75.0\n395\n164\n2362\n75\n2437\n\n\n1134\n2024-05-27 09:16:00\n2363.0\n75.0\n164\n1440\n2363\n75\n2438\n\n\n\n\n1135 rows × 8 columns\n\n\n\n\nGood, now we just need to calculate the consumption and create the main table.\n\n%%sql \n\nCREATE OR REPLACE TABLE waermestrom AS\nSELECT \n  date,\n  value,\n  value_hoch_fix AS value_hoch,\n  value_niedrig_fix AS value_niedrig,\n  minutes_lag AS minutes,\n  -- add default values to lag(), to prevent null in the first row\n  -- use 11kwh less than the first value which is approximately the avg consumption per day\n  -- and would be equivalent to the minutes in the first row, that we set with the default\n  -- of one day in the previous query \n  value - lag(value, 1, value-11) over(order by date) AS consumption,\n  1.0 * consumption / minutes_lag AS cm,\n  24.0 * 60.0 * consumption / minutes_lag AS consumption_day_equivalent,\n  -- now calculate consumption per tariff\n  value_hoch_fix - lag(value_hoch_fix, 1, value_hoch_fix-11) over(order by date) AS consumption_hoch,\n  value_niedrig_fix - lag(value_niedrig_fix, 1, value_niedrig_fix-11) over(order by date) AS consumption_niedrig,\n  1.0 * consumption_hoch / minutes_lag AS cm_hoch,\n  1.0 * consumption_niedrig / minutes_lag AS cm_niedrig\nFROM waermestrom_nonulls \nWHERE minutes &gt; 1 --get rid of the artificially short periods\n;\nSELECT * FROM waermestrom ORDER BY date;\n\n\n\n\n\n\n\n\n\ndate\nvalue\nvalue_hoch\nvalue_niedrig\nminutes\nconsumption\ncm\nconsumption_day_equivalent\nconsumption_hoch\nconsumption_niedrig\ncm_hoch\ncm_niedrig\n\n\n\n\n0\n2016-11-28 00:21:00\n1031\n547\n484\n1440\n11\n0.007639\n11.000000\n11\n11\n0.007639\n0.007639\n\n\n1\n2016-11-28 15:23:00\n1031\n547\n484\n902\n0\n0.000000\n0.000000\n0\n0\n0.000000\n0.000000\n\n\n2\n2017-11-25 15:20:00\n6024\n3347\n2677\n521277\n4993\n0.009578\n13.792897\n2800\n2193\n0.005371\n0.004207\n\n\n3\n2017-11-25 15:23:00\n6024\n3347\n2677\n3\n0\n0.000000\n0.000000\n0\n0\n0.000000\n0.000000\n\n\n4\n2018-11-23 15:19:00\n10640\n5747\n4893\n522716\n4616\n0.008831\n12.716351\n2400\n2216\n0.004591\n0.004239\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1032\n2024-05-26 09:39:00\n2433\n2362\n71\n121\n1\n0.008264\n11.900826\n0\n1\n0.000000\n0.008264\n\n\n1033\n2024-05-26 19:25:00\n2435\n2362\n73\n586\n2\n0.003413\n4.914676\n0\n2\n0.000000\n0.003413\n\n\n1034\n2024-05-26 23:57:00\n2437\n2362\n75\n272\n2\n0.007353\n10.588235\n0\n2\n0.000000\n0.007353\n\n\n1035\n2024-05-27 06:32:00\n2437\n2362\n75\n395\n0\n0.000000\n0.000000\n0\n0\n0.000000\n0.000000\n\n\n1036\n2024-05-27 09:16:00\n2438\n2363\n75\n164\n1\n0.006098\n8.780488\n1\n0\n0.006098\n0.000000\n\n\n\n\n1037 rows × 12 columns\n\n\n\n\n\n15483-15472\n16695-16662\n\n33\n\n\n\n\nVisualize the data\n\n%%sql \nwaermestrom &lt;&lt; SELECT * FROM waermestrom;\n\nAgain, very noisy data, with substantial variation in the consumption day equivalent and there is 1.5 years without data. But here you kinda already see the seasonal pattern of higher consumption in winter time.\n\nimport plotly.express as px\nfig = px.line(waermestrom, x='date', y=\"consumption_day_equivalent\")\nfig.show()\n\n                                                \n\n\nThe minutes show a similar pattern, but with a bunch of very low values (probably 1), that should be due to the combination of the two meters when they do not fall in exactly the same minute.\n\nimport plotly.express as px\nfig = px.histogram(waermestrom.query(\"minutes &lt; 10000\"), x=\"minutes\", marginal=\"box\")\nfig.show()\n\n                                                \n\n\nThe consumption day equivalent varies also substantially, and it is of course higher than the normal strom.\n\nimport plotly.express as px\nfig = px.histogram(waermestrom.query(\"minutes &lt; 10000\"), x=\"consumption_day_equivalent\", marginal=\"box\")\nfig.show()\n\n                                                \n\n\nHere the pattern of minutes and consumption is not so marked as in the normal strom.\n\nfrom matplotlib import pyplot\npyplot.scatter(\n    waermestrom.query(\"minutes &lt; 10000\")[\"minutes\"], \n    waermestrom.query(\"minutes &lt; 10000\")[\"consumption_day_equivalent\"]\n)\n\n\n\n\n\n\n\n\n\nimport plotly.express as px\nfig = px.scatter(\n    data_frame=waermestrom.query(\"minutes &lt; 10000\"), \n    x=\"minutes\", \n    y=\"consumption_day_equivalent\", hover_data=['date'],\n    marginal_x=\"histogram\", \n    marginal_y=\"histogram\"\n)\nfig.show()\n\n                                                \n\n\n\n\nConsumption by hour\nSo, again let’s take the inefficient but straightforward way. First expand in minutes to the whole range.\n\n%%sql\n\nCREATE OR REPLACE TABLE waermestrom_minute_nulls AS\nWITH minutes_table AS (\n  SELECT UNNEST(generate_series(ts[1], ts[2], interval 1 minute)) as minute\n  FROM (VALUES (\n    [(SELECT MIN(date) FROM waermestrom), (SELECT MAX(DATE) FROM waermestrom)]\n  )) t(ts)\n)\nSELECT * \nFROM minutes_table\nLEFT JOIN waermestrom\nON minutes_table.minute = waermestrom.date\n;\nSELECT * FROM waermestrom_minute_nulls ORDER BY minute LIMIT 10;\n\n\n\n\n\n\n\n\n\nminute\ndate\nvalue\nvalue_hoch\nvalue_niedrig\nminutes\nconsumption\ncm\nconsumption_day_equivalent\nconsumption_hoch\nconsumption_niedrig\ncm_hoch\ncm_niedrig\n\n\n\n\n0\n2016-11-28 00:21:00\n2016-11-28 00:21:00\n1031.0\n547.0\n484.0\n1440.0\n11.0\n0.007639\n11.0\n11.0\n11.0\n0.007639\n0.007639\n\n\n1\n2016-11-28 00:22:00\nNaT\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n2016-11-28 00:23:00\nNaT\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\n2016-11-28 00:24:00\nNaT\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\n2016-11-28 00:25:00\nNaT\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n5\n2016-11-28 00:26:00\nNaT\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n6\n2016-11-28 00:27:00\nNaT\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n7\n2016-11-28 00:28:00\nNaT\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n8\n2016-11-28 00:29:00\nNaT\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n9\n2016-11-28 00:30:00\nNaT\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\nAnd fill in the NULLS\n\n%%sql\n\nCREATE OR REPLACE TABLE waermestrom_minute AS\nSELECT\n  minute,\n  date,\n  value,\n  value_hoch,\n  value_niedrig,\n  minutes,\n  consumption,\n  FIRST_VALUE(cm IGNORE NULLS) OVER(\n    ORDER BY minute ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING \n  ) AS cm,\n  FIRST_VALUE(cm_hoch IGNORE NULLS) OVER(\n    ORDER BY minute ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING \n  ) AS cm_hoch,\n  FIRST_VALUE(cm_niedrig IGNORE NULLS) OVER(\n    ORDER BY minute ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING \n  ) AS cm_niedrig\nFROM waermestrom_minute_nulls t1\nORDER BY t1.minute\n;\nSELECT * FROM waermestrom_minute ORDER BY minute LIMIT 100;\n\n\n\n\n\n\n\n\n\n\n\n\nminute\ndate\nvalue\nvalue_hoch\nvalue_niedrig\nminutes\nconsumption\ncm\ncm_hoch\ncm_niedrig\n\n\n\n\n0\n2016-11-28 00:21:00\n2016-11-28 00:21:00\n1031.0\n547.0\n484.0\n1440.0\n11.0\n0.007639\n0.007639\n0.007639\n\n\n1\n2016-11-28 00:22:00\nNaT\nNaN\nNaN\nNaN\nNaN\nNaN\n0.000000\n0.000000\n0.000000\n\n\n2\n2016-11-28 00:23:00\nNaT\nNaN\nNaN\nNaN\nNaN\nNaN\n0.000000\n0.000000\n0.000000\n\n\n3\n2016-11-28 00:24:00\nNaT\nNaN\nNaN\nNaN\nNaN\nNaN\n0.000000\n0.000000\n0.000000\n\n\n4\n2016-11-28 00:25:00\nNaT\nNaN\nNaN\nNaN\nNaN\nNaN\n0.000000\n0.000000\n0.000000\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n95\n2016-11-28 01:56:00\nNaT\nNaN\nNaN\nNaN\nNaN\nNaN\n0.000000\n0.000000\n0.000000\n\n\n96\n2016-11-28 01:57:00\nNaT\nNaN\nNaN\nNaN\nNaN\nNaN\n0.000000\n0.000000\n0.000000\n\n\n97\n2016-11-28 01:58:00\nNaT\nNaN\nNaN\nNaN\nNaN\nNaN\n0.000000\n0.000000\n0.000000\n\n\n98\n2016-11-28 01:59:00\nNaT\nNaN\nNaN\nNaN\nNaN\nNaN\n0.000000\n0.000000\n0.000000\n\n\n99\n2016-11-28 02:00:00\nNaT\nNaN\nNaN\nNaN\nNaN\nNaN\n0.000000\n0.000000\n0.000000\n\n\n\n\n100 rows × 10 columns\n\n\n\n\nAnd now we can simply aggregate per day and hour, and the average will be correct, as all the rows have comparable units (consumption for one minute, with equal weight).\n\n%%sql\n\nconsumption_hour_avg &lt;&lt; SELECT \n  COUNT(*) AS cnt,\n  hour(minute) AS hour, \n  AVG(cm)*60*24 AS cmy\nFROM waermestrom_minute\nGROUP BY hour(minute)\n;\n\n\nimport plotly.express as px\nfig = px.line(consumption_hour_avg, y='cmy', x='hour')\nfig.show()\n\n                                                \n\n\n\n%%sql\n\nconsumption_hour_avg &lt;&lt; SELECT \n  hour(minute) AS hour, \n  1.0*AVG(cm)*60*24 AS cmy\nFROM waermestrom_minute\nWHERE minute &lt;= '2021-05-25' OR minute &gt;= '2022-11-30'\nGROUP BY hour(minute)\n;\n\n\nimport plotly.express as px\nfig = px.bar(consumption_hour_avg, y='cmy', x='hour')\nfig.show()\n\n                                                \n\n\n\n\nTraces plot\n\n%%sql\n\n\nSELECT\n  hour(minute) AS hour, \n  AVG(1.0 * 60 * 24 * cm) AS cm\nFROM waermestrom_minute\nWHERE minute &lt;= '2021-05-25' OR minute &gt;= '2022-11-30'\nGROUP BY hour(minute)\n;\n\n\n\n\n\n\n\n\n\nhour\ncm\n\n\n\n\n0\n0\n13.045296\n\n\n1\n1\n12.977597\n\n\n2\n2\n12.967582\n\n\n3\n3\n12.972058\n\n\n4\n4\n12.959446\n\n\n5\n5\n12.944655\n\n\n6\n6\n-0.657822\n\n\n7\n7\n-9.097893\n\n\n8\n8\n-9.192271\n\n\n9\n9\n-9.219974\n\n\n10\n10\n-9.321712\n\n\n11\n11\n-9.364090\n\n\n12\n12\n-9.422645\n\n\n13\n13\n-9.437484\n\n\n14\n14\n-9.444046\n\n\n15\n15\n-9.443632\n\n\n16\n16\n-9.460965\n\n\n17\n17\n-9.440750\n\n\n18\n18\n-9.394454\n\n\n19\n19\n-9.280046\n\n\n20\n20\n-9.072323\n\n\n21\n21\n-8.864177\n\n\n22\n22\n0.374380\n\n\n23\n23\n13.192526\n\n\n\n\n\n\n\n\n\n%%sql\n\nSELECT \n  minute,\n  date,\n  1.0 * 60 * 24 * cm AS cm,\n  AVG(1.0 * 60 * 24 * cm) OVER(\n    ORDER BY minute ROWS BETWEEN 240 PRECEDING AND CURRENT ROW\n  ) AS cmma\nFROM waermestrom_minute\nWHERE minute &gt; '2022-11-30'\n;\n\n\n\n\n\n\n\n\n\nminute\ndate\ncm\ncmma\n\n\n\n\n0\n2022-11-30 00:01:00\nNaT\n11.974301\n11.974301\n\n\n1\n2022-11-30 00:02:00\nNaT\n11.974301\n11.974301\n\n\n2\n2022-11-30 00:03:00\nNaT\n11.974301\n11.974301\n\n\n3\n2022-11-30 00:04:00\nNaT\n11.974301\n11.974301\n\n\n4\n2022-11-30 00:05:00\nNaT\n11.974301\n11.974301\n\n\n...\n...\n...\n...\n...\n\n\n783911\n2024-05-27 09:12:00\nNaT\n8.780488\n5.829369\n\n\n783912\n2024-05-27 09:13:00\nNaT\n8.780488\n5.865803\n\n\n783913\n2024-05-27 09:14:00\nNaT\n8.780488\n5.902237\n\n\n783914\n2024-05-27 09:15:00\nNaT\n8.780488\n5.938670\n\n\n783915\n2024-05-27 09:16:00\n2024-05-27 09:16:00\n8.780488\n5.975104\n\n\n\n\n783916 rows × 4 columns\n\n\n\n\n\n%%sql\n\nCREATE OR REPLACE TABLE toy AS\nWITH hourly_average AS (\n  SELECT\n    hour(minute) AS hour, \n    AVG(1.0 * 60 * 24 * cm) AS cmha\n  FROM waermestrom_minute\n  --This was originally here, because we wanted to see the hourly variation\n  --and keeping this long period without measurements just smoothed things\n  --but for waermestrom, it has a misleading implication: since we have \n  --measurements mostly in the wintertime, the average without this period\n  --is high, reflecting the higher energy consumption during winter\n  --WHERE minute &lt;= '2021-05-25' OR minute &gt;= '2022-11-30'\n  GROUP BY hour(minute)\n),\nlast_measurements AS (\n  SELECT \n    minute,\n    date,\n    1.0 * 60 * 24 * cm AS cm,\n    AVG(1.0 * 60 * 24 * cm) OVER(\n      ORDER BY minute ROWS BETWEEN 60*4 PRECEDING AND CURRENT ROW\n    ) AS cmma\n  FROM waermestrom_minute\n  WHERE minute &gt;= '2022-11-30'\n)\nSELECT *, CASE WHEN date IS NOT NULL THEN cm ELSE NULL END AS cmdate\nFROM last_measurements\nLEFT JOIN hourly_average\nON hour(last_measurements.minute) = hourly_average.hour\n;\n\n\n\n\n\n\n\n\n\nSuccess\n\n\n\n\n\n\n\n\n\n\n\n%sql toy &lt;&lt; SELECT * FROM toy;\n\n\nimport plotly.graph_objects as go\nfig = px.area(toy, x='minute', y='cmha')\nfig.add_trace(go.Scatter(\n  x=toy['minute'], y=toy['cmma'], mode='lines', showlegend=False\n))\nfig.add_trace(go.Scatter(\n  x=toy['date'], y=toy['cmdate'], mode='markers', showlegend=False\n))\n\n# Add range slider\nfig.update_layout(\n    xaxis=dict(\n        rangeselector=dict(\n            buttons=list([\n                dict(count=1,\n                     label=\"1m\",\n                     step=\"month\",\n                     stepmode=\"backward\"),\n                dict(count=7,\n                     label=\"7d\",\n                     step=\"day\",\n                     stepmode=\"backward\"),\n                dict(count=15,\n                     label=\"15d\",\n                     step=\"day\",\n                     stepmode=\"backward\"),\n                dict(step=\"all\")\n            ])\n        ),\n        rangeslider=dict(\n            visible=True\n        ),\n        type=\"date\"\n    )\n)\n\nfig.show()\n\nTODO: fix the range, which range?\nLet’s see the rate hoch tarif vs. niedrig\nniedrig_fraction &lt;&lt; SELECT date AS day, AVG(cm_niedrig/cm) AS niedrig_fraction, AVG(cm_niedrig)/AVG(cm) AS niedrig_fraction2, AVG(niedrig_fraction) OVER( ORDER BY minute ROWS BETWEEN 60 * 24 * 7 PRECEDING AND CURRENT ROW ) AS niedrig_fraction_ma FROM waermestrom_minute GROUP BY date ORDER BY date ;\nimport plotly.express as px fig = px.bar(niedrig_fraction, x=‘day’, y=‘niedrig_fraction2’) fig.show()",
    "crumbs": [
      "Dashboard",
      "Process",
      "Wärmestrom"
    ]
  },
  {
    "objectID": "Process/00_strom_data_src.html",
    "href": "Process/00_strom_data_src.html",
    "title": "Strom Data Source",
    "section": "",
    "text": "%%sql\nCREATE OR REPLACE VIEW fail AS\nSELECT * FROM sqlite_scan('{{latest_file}}', 'meter');\nSELECT * FROM fail;\n\n\nMeters table\n\n%%sql\nSELECT \n    _id,\n    decode(name)::STRING,\n    decode(units)::STRING,\n    decode(comment)::STRING,\n    decode(vsf)::STRING,\n    decode(tsf)::STRING,\n    decode(cost)::STRING,\n    decode(fcost)::STRING,\n    decode(invert)::STRING,\n    decode(vmsetup)::STRING,\n    decode(type)::STRING,\n    decode(currency)::STRING,\n    decode(scaling)::STRING,\n    decode(phyunits)::STRING,\n    decode(bidir)::STRING,\n    decode(prod)::STRING\nFROM sqlite_scan('{{latest_file}}', 'meter')\n;\n\n\n\nStrom data\n\n%%sql \n\nSELECT \n  meterid, \n  -- Blob Functions, because most columns get read as blob\n  -- https://duckdb.org/docs/sql/functions/blob\n  decode(date)::DATETIME AS date, \n  decode(value)::INT AS value,\n  decode(comment)::STRING AS comment,\n  decode(color)::STRING AS color,\n  decode(cost)::STRING AS cost,\n  decode(vcor)::STRING AS vcor,\n  decode(ccor)::STRING AS ccor,\n  decode(first)::STRING AS first\nFROM sqlite_scan('{{latest_file}}', 'reading') \nWHERE meterid = 1\n;\n\n\n%%sql \n\ntoy &lt;&lt; \nWITH strom_sqlite AS (\n  SELECT \n    meterid, \n    -- Blob Functions, because most columns get read as blob\n    -- https://duckdb.org/docs/sql/functions/blob\n    decode(date)::DATETIME AS date, \n    decode(value)::INT AS value,\n    decode(first)::INT AS first\n  FROM sqlite_scan('{{latest_file}}', 'reading') \n  WHERE meterid = 1\n)\nSELECT *,\ndate_sub('minute', lag(date, 1, '2020-11-30 00:00:00') over(order by date), date) AS minutes, \nvalue * (1/(1-first)) - lag(value, 1, 12160) over(order by date) AS consumption,\n1.0 * consumption / minutes AS cm,\n24.0 * 60.0 * consumption / minutes AS consumption_day_equivalent\nFROM strom_sqlite\nORDER BY date\n;\n\n\n\nStrom data\n\n%%sql \n\nATTACH '{{latest_file}}' AS ecas (TYPE SQLITE);\nUSE ecas;\n\n\n%%sql \n\nSELECT * FROM reading;\n\n\n%%sql \n\n\nUPDATE reading\nSET meterid = 1\nWHERE meterid = 16;\n\n\n%%sql \n\n\nUPDATE reading\nSET meterid = 3\nWHERE meterid = 17;\n\n\n%%sql \n\n\nUPDATE reading\nSET meterid = 2\nWHERE meterid = 18;\n\n\n%%sql\n\nCREATE OR REPLACE TABLE normalstrom_minute_nulls AS\nWITH minutes_table AS (\n    SELECT UNNEST(generate_series(ts[1], ts[2], interval 1 minute)) AS minute\n    FROM (VALUES (\n    [(SELECT MIN(date) FROM normalstrom), (SELECT MAX(DATE) FROM normalstrom)]\n    )) t(ts)\n)\nSELECT * \nFROM minutes_table\nLEFT JOIN normalstrom\nON minutes_table.minute = normalstrom.date\n;\n\nSELECT * FROM normalstrom_minute_nulls ORDER BY minute;\n\n\n%%sql\n\nCREATE OR REPLACE TABLE normalstrom_minute AS\nSELECT\n    minute,\n    date,\n    value,\n    minutes,\n    consumption,\n    FIRST_VALUE(cm IGNORE NULLS) OVER(\n    ORDER BY minute ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING \n    ) AS cm\nFROM normalstrom_minute_nulls t1\nORDER BY t1.minute\n;\n\n\n%%sql\n\nSELECT * FROM strom_per_day WHERE date &gt;= '2021-01-01' AND date &lt;= '2021-12-31';\n\nSELECT MAX(date) FROM strom_per_day;",
    "crumbs": [
      "Dashboard",
      "Process",
      "Strom Data Source"
    ]
  },
  {
    "objectID": "Process/01_strom_process.html",
    "href": "Process/01_strom_process.html",
    "title": "Strom",
    "section": "",
    "text": "%%sql\nCREATE OR REPLACE VIEW fail AS\nSELECT * FROM sqlite_scan('{{latest_file}}', 'meter');\nSELECT * FROM fail;\n\n\n\n\n\n\n\n\n\n_id\nname\nunits\ncomment\nvsf\ntsf\ncost\nfcost\ninvert\nvmsetup\ntype\ncurrency\nscaling\nphyunits\nbidir\nprod\n\n\n\n\n0\n1\n[83, 116, 114, 111, 109]\n[]\n[]\n[53, 48]\n[53, 48]\n[]\n[]\n[48]\nNaN\n[48]\n[36]\n[49, 46, 48]\n[]\n[48]\n[48]\n\n\n1\n2\n[87, 195, 164, 114, 109, 101, 115, 116, 114, 1...\n[]\n[]\n[53, 48]\n[53, 48]\n[]\n[]\n[48]\nNaN\n[48]\n[36]\n[49, 46, 48]\n[]\n[48]\n[48]\n\n\n2\n3\n[87, 195, 164, 114, 109, 101, 115, 116, 114, 1...\n[]\n[]\n[53, 48]\n[53, 48]\n[]\n[]\n[48]\nNaN\n[48]\n[36]\n[49, 46, 48]\n[]\n[48]\n[48]\n\n\n3\n4\n[87, 97, 115, 115, 101, 114]\n[]\n[]\n[53, 48]\n[53, 48]\n[]\n[]\n[48]\nNaN\n[48]\n[36]\n[49, 46, 48]\n[]\n[48]\n[48]\n\n\n4\n5\n[76, 195, 188, 102, 116, 117, 110, 103, 115, 9...\n[]\n[]\n[53, 48]\n[53, 48]\n[]\n[]\n[48]\nNaN\n[48]\n[36]\n[49, 46, 48]\n[]\n[48]\n[48]\n\n\n5\n8\n[79, 102, 102, 101, 110]\n[]\n[]\n[53, 48]\n[53, 48]\n[]\n[]\n[48]\nNaN\n[48]\n[36]\n[49, 46, 48]\n[]\n[48]\n[48]\n\n\n6\n9\n[82, 101, 115, 101, 116]\n[]\n[]\n[53, 48]\n[53, 48]\n[]\n[]\n[48]\nNaN\n[48]\n[36]\n[49, 46, 48]\n[]\n[48]\n[48]\n\n\n7\n10\n[78, 97, 99, 104, 108, 97, 100, 101, 110]\n[]\n[]\n[53, 48]\n[53, 48]\n[]\n[]\n[48]\nNaN\n[48]\n[36]\n[49, 46, 48]\n[]\n[48]\n[48]\n\n\n8\n11\n[84, 101, 109, 112]\n[]\n[]\n[53, 48]\n[53, 48]\n[]\n[]\n[48]\nNaN\n[48]\n[36]\n[49, 46, 48]\n[]\n[48]\n[48]\n\n\n9\n12\n[87, 195, 164, 114, 109, 101, 114]\n[]\n[]\n[53, 48]\n[53, 48]\n[]\n[]\n[48]\nNaN\n[48]\n[36]\n[49, 46, 48]\n[]\n[48]\n[48]\n\n\n10\n14\n[80, 111, 111, 108, 32, 68, 101, 99, 107, 101]\n[]\n[]\n[53, 48]\n[53, 48]\n[]\n[]\n[48]\nNaN\n[48]\n[226, 130, 172]\n[49, 46, 48]\n[]\n[48]\n[48]\n\n\n11\n15\n[83, 99, 104, 105, 101, 195, 159, 99, 97, 110,...\n[]\n[]\n[53, 48]\n[53, 48]\n[]\n[]\n[48]\nNaN\n[48]\n[226, 130, 172]\n[49, 46, 48]\n[]\n[48]\n[48]\n\n\n\n\n\n\n\n\n\nGet strom data\nBy querying directly the SQLite table and filtering by meter id.\n\n%%sql \n\nCREATE OR REPLACE TABLE strom AS \nWITH strom_sqlite AS (\n  SELECT \n    meterid, \n    -- Blob Functions, because most columns get read as blob\n    -- https://duckdb.org/docs/sql/functions/blob\n    decode(date)::DATETIME AS date, \n    decode(value)::INT AS value\n  FROM sqlite_scan('{{latest_file}}', 'reading') \n  WHERE meterid = 1\n)\nSELECT \n  *,\n  -- add default values to lag(), to prevent null in the first row\n  date_sub('minute', lag(date, 1, '2020-11-30 00:00:00') over(order by date), date) AS minutes, \n  -- add default values to lag(), to prevent null in the first row\n  value - lag(value, 1, 12160) over(order by date) AS consumption,\n  1.0 * consumption / minutes AS cm,\n  24.0 * 60.0 * consumption / minutes AS consumption_day_equivalent\nFROM strom_sqlite\nORDER BY date\n;\n\n\n\n\n\n\n\n\n\nSuccess\n\n\n\n\n\n\n\n\n\n\n\n\nVisualize the data\n\n%%sql \nstrom &lt;&lt; SELECT * FROM strom;\n\nOf course noisy data, with substantial variation in the consumption day equivalent and there is 1.5 years without data.\n\nimport plotly.express as px\nfig = px.line(strom, x='date', y=\"consumption_day_equivalent\")\nfig.show()\n\n                                                \n\n\n\n#import pandas as pd\n#from pandas_profiling import ProfileReport\n\n#EDA using pandas-profiling\n#profile = ProfileReport(strom, explorative=True)\n#profile.to_file(\"output.html\")\n\nWith the exception of the long period without data, the number of minutes shows there are measurements from a few hours away, to a few days away. Most measurements are between 7 and 16 hours apart. That’s worrisome, as the periods are relatively long. In addition, the measurements are scattered and do not follow a systematic pattern.\n\nimport plotly.express as px\nfig = px.histogram(strom.query(\"minutes &lt; 10000\"), x=\"minutes\", marginal=\"box\")\nfig.show()\n\n                                                \n\n\nThe consumption day equivalent varies also substantially. Median 8.8, which is consistent with the long-run consumption (equivalent to python 8.8*365 per year.). The distribution has a long right tail, with very high consumptions, presumably, associated to very short measurements periods.\n\nimport plotly.express as px\nfig = px.histogram(strom.query(\"minutes &lt; 10000\"), x=\"consumption_day_equivalent\", marginal=\"box\")\nfig.show()\n\n                                                \n\n\nWell, yeah, as expected, short measurement periods (few minutes) are associated with higher variability, and with the highest and lowest consumptions.\n\nfrom matplotlib import pyplot\npyplot.scatter(\n    strom.query(\"minutes &lt; 10000\")[\"minutes\"], \n    strom.query(\"minutes &lt; 10000\")[\"consumption_day_equivalent\"]\n)\n\n\n\n\n\n\n\n\n\nimport plotly.express as px\nfig = px.scatter(\n    data_frame=strom.query(\"minutes &lt; 10000\"), \n    x=\"minutes\", \n    y=\"consumption_day_equivalent\", hover_data=['date'],\n    marginal_x=\"histogram\", \n    marginal_y=\"histogram\"\n)\nfig.show()\n\n                                                \n\n\n\n\nConsumption by hour\nLet’s try to see what hours have the highest consumption. That’s tricky given this messy data. One approach is to just interpolate between data points and assume a constant consumption. That’s of course not realistic (specially during the day), but it would get us closer.\n\n%%sql\nSELECT MIN(date), MAX(DATE) FROM strom;\n\n\n\n\n\n\n\n\n\nmin(date)\nmax(DATE)\n\n\n\n\n0\n2016-11-28 16:16:00\n2024-05-27 09:15:00\n\n\n\n\n\n\n\n\nThis is pretty inefficient, as it will create a table with as many rows as minutes there are. So more than a million, and then left join the actual data to that huge table. We end up with a table with a bunch of nulls, and only observations where there are actual measurements. But let’s move on; this is quick-and-dirty.\n\n%%sql\n\nCREATE OR REPLACE TABLE strom_minute_nulls AS\nWITH minutes_table AS (\n  SELECT UNNEST(generate_series(ts[1], ts[2], interval 1 minute)) as minute\n  FROM (VALUES (\n    [(SELECT MIN(date) FROM strom), (SELECT MAX(DATE) FROM strom)]\n  )) t(ts)\n)\nSELECT * \nFROM minutes_table\nLEFT JOIN strom\nON minutes_table.minute = strom.date\n;\nSELECT * FROM strom_minute_nulls ORDER BY minute LIMIT 100;\n\n\n\n\n\n\n\n\n\nminute\nmeterid\ndate\nvalue\nminutes\nconsumption\ncm\nconsumption_day_equivalent\n\n\n\n\n0\n2016-11-28 16:16:00\n1.0\n2016-11-28 16:16:00\n659.0\n-2105744.0\n-11501.0\n0.005462\n7.864888\n\n\n1\n2016-11-28 16:17:00\nNaN\nNaT\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n2016-11-28 16:18:00\nNaN\nNaT\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\n2016-11-28 16:19:00\nNaN\nNaT\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\n2016-11-28 16:20:00\nNaN\nNaT\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n95\n2016-11-28 17:51:00\nNaN\nNaT\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n96\n2016-11-28 17:52:00\nNaN\nNaT\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n97\n2016-11-28 17:53:00\nNaN\nNaT\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n98\n2016-11-28 17:54:00\nNaN\nNaT\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n99\n2016-11-28 17:55:00\nNaN\nNaT\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n100 rows × 8 columns\n\n\n\n\nAnd now we just interpolate the consumption per minute, filling the nulls with the next non-null value (i.e. the consumption is constant in all the measurement period -all the minutes between one measurement and the other-). TODO: this uses a correlated subquery. Look for a better solution https://dba.stackexchange.com/questions/279039/how-to-get-the-last-non-null-value-that-came-before-the-current-row\n\n%%sql\n\nCREATE OR REPLACE TABLE strom_minute AS\nSELECT\n  minute,\n  date,\n  value,\n  minutes,\n  consumption,\n  CASE \n    WHEN cm IS NULL THEN\n      (SELECT cm \n      FROM strom_minute_nulls t2 \n      WHERE t2.minute &gt; t1.minute and cm is not null \n      ORDER BY minute\n      LIMIT 1)\n    else cm \n  END AS cm\nFROM strom_minute_nulls t1\nORDER BY t1.minute\n;\nSELECT * FROM strom_minute ORDER BY minute LIMIT 100;\n\nIt turns out DuckDB already implements ignore nulls, so we can rewrite the query above, to run much more efficiently, like this\n\n%%sql\n\nCREATE OR REPLACE TABLE strom_minute AS\nSELECT\n  minute,\n  date,\n  value,\n  minutes,\n  consumption,\n  FIRST_VALUE(cm IGNORE NULLS) OVER(\n    ORDER BY minute ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING \n  ) AS cm\nFROM strom_minute_nulls t1\nORDER BY t1.minute\n;\nSELECT * FROM strom_minute ORDER BY minute LIMIT 100;\n\n\n\n\n\n\n\n\n\n\n\n\nminute\ndate\nvalue\nminutes\nconsumption\ncm\n\n\n\n\n0\n2016-11-28 16:16:00\n2016-11-28 16:16:00\n659.0\n-2105744.0\n-11501.0\n0.005462\n\n\n1\n2016-11-28 16:17:00\nNaT\nNaN\nNaN\nNaN\n0.003876\n\n\n2\n2016-11-28 16:18:00\nNaT\nNaN\nNaN\nNaN\n0.003876\n\n\n3\n2016-11-28 16:19:00\nNaT\nNaN\nNaN\nNaN\n0.003876\n\n\n4\n2016-11-28 16:20:00\nNaT\nNaN\nNaN\nNaN\n0.003876\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n95\n2016-11-28 17:51:00\nNaT\nNaN\nNaN\nNaN\n0.003876\n\n\n96\n2016-11-28 17:52:00\nNaT\nNaN\nNaN\nNaN\n0.003876\n\n\n97\n2016-11-28 17:53:00\nNaT\nNaN\nNaN\nNaN\n0.003876\n\n\n98\n2016-11-28 17:54:00\nNaT\nNaN\nNaN\nNaN\n0.003876\n\n\n99\n2016-11-28 17:55:00\nNaT\nNaN\nNaN\nNaN\n0.003876\n\n\n\n\n100 rows × 6 columns\n\n\n\n\n\n%%sql\n\ntoy &lt;&lt; SELECT * FROM strom_minute ORDER BY minute LIMIT 7000;\n\nNow we can simply aggregate per day and hour, and the average will be correct, as all the rows have comparable units (consumption for one minute, with equal weight).\n\n%%sql\n\nconsumption_hour_avg &lt;&lt; SELECT \n  COUNT(*) AS cnt,\n  hour(minute) AS hour, \n  AVG(cm)*60*24*365 AS consumption\nFROM strom_minute\nGROUP BY hour(minute)\n;\n\n\nimport plotly.express as px\nfig = px.bar(consumption_hour_avg, y='consumption', x='hour')\nfig.show()\n\n                                                \n\n\nOk, good enough. But this includes a very long period without measurements, which would have the effect to smooth everything. Let’s take that chunk out to see how it looks.\n\n%%sql\n\nconsumption_hour_avg &lt;&lt; SELECT \n  hour(minute) AS hour, \n  AVG(cm)*60*24*365 AS consumption\nFROM strom_minute\nWHERE minute &lt;= '2021-05-25' OR minute &gt;= '2022-11-30'\nGROUP BY hour(minute)\n;\n\nThis looks more accurate. It still should have some smoothing going on, giving that there are still long-ish periods without measurements (a few days) and the non-systematic measurement pattern, than frequently spans more than one hour.\n\nimport plotly.express as px\nfig = px.bar(consumption_hour_avg, y='consumption', x='hour')\nfig.show()\n\n                                                \n\n\n\n%%sql\n\nselect * from consumption_hour_avg;\n\n\n\n\n\n\n\n\n\nhour\nconsumption\n\n\n\n\n0\n0\n2784.652421\n\n\n1\n1\n2735.823782\n\n\n2\n2\n2728.385751\n\n\n3\n3\n2727.778571\n\n\n4\n4\n2729.075276\n\n\n5\n5\n2741.844147\n\n\n6\n6\n-761.473604\n\n\n7\n7\n-2864.012973\n\n\n8\n8\n-2734.180209\n\n\n9\n9\n-2631.605569\n\n\n10\n10\n-2579.628068\n\n\n11\n11\n-2551.550426\n\n\n12\n12\n-2543.407614\n\n\n13\n13\n-2539.326264\n\n\n14\n14\n-2533.465860\n\n\n15\n15\n-2535.593274\n\n\n16\n16\n-2534.213682\n\n\n17\n17\n-2536.161544\n\n\n18\n18\n-2549.959383\n\n\n19\n19\n-2583.290036\n\n\n20\n20\n-2676.761289\n\n\n21\n21\n-2767.945648\n\n\n22\n22\n-437.644800\n\n\n23\n23\n2857.016762\n\n\n\n\n\n\n\n\n\n\nTraces plot\n\n%%sql\n\nCREATE OR REPLACE TABLE toy AS\nWITH hourly_average AS (\n  SELECT\n    hour(minute) AS hour, \n    AVG(1.0 * 60 * 24 * cm) AS cmha\n  FROM strom_minute\n  -- This was originally here, because we wanted to see the hourly variation\n  -- and keeping this long period without measurements just smoothed things\n  -- but for waermestrom, it has a misleading implication: since we have \n  -- measurements mostly in the wintertime, the average without this period\n  -- is high, reflecting the higher energy consumption during winter\n  WHERE minute &lt;= '2021-05-25' OR minute &gt;= '2022-11-30'\n  GROUP BY hour(minute)\n),\nlast_measurements AS (\n  SELECT \n    minute,\n    date,\n    1.0 * 60 * 24 * cm AS cm,\n    AVG(1.0 * 60 * 24 * cm) OVER(\n      ORDER BY minute ROWS BETWEEN 60*4 PRECEDING AND CURRENT ROW\n    ) AS cmma\n  FROM strom_minute\n  WHERE minute &gt;= '2022-11-30'\n)\nSELECT *, CASE WHEN date IS NOT NULL THEN cm ELSE NULL END AS cmdate\nFROM last_measurements\nLEFT JOIN hourly_average\nON hour(last_measurements.minute) = hourly_average.hour\n;\n\n\n\n\n\n\n\n\n\nSuccess\n\n\n\n\n\n\n\n\n\n\n\n%%sql \n\ntoy &lt;&lt; SELECT * FROM toy;\n\n\nimport plotly.graph_objects as go\nfig = px.area(toy, x='minute', y='cmha')\nfig.add_trace(go.Scatter(\n  x=toy['minute'], y=toy['cmma'], mode='lines', showlegend=False\n))\nfig.add_trace(go.Scatter(\n  x=toy['date'], y=toy['cmdate'], mode='markers', showlegend=False\n))\n\n# Add range slider\nfig.update_layout(\n    xaxis=dict(\n        rangeselector=dict(\n            buttons=list([\n                dict(count=1,\n                     label=\"1m\",\n                     step=\"month\",\n                     stepmode=\"backward\"),\n                dict(count=7,\n                     label=\"7d\",\n                     step=\"day\",\n                     stepmode=\"backward\"),\n                dict(count=15,\n                     label=\"15d\",\n                     step=\"day\",\n                     stepmode=\"backward\"),\n                dict(step=\"all\")\n            ])\n        ),\n        rangeslider=dict(\n            visible=True\n        ),\n        type=\"date\"\n    )\n)\n\nfig.show()\n\nTODO: fix the range\n\n%%sql\n\n--SELECT * FROM strom_minute LIMIT 10;\nSELECT COUNT(*) FROM strom_minute;\n\n\n\n\n\n\n\n\n\ncount_star()\n\n\n\n\n0\n3940860\n\n\n\n\n\n\n\n\n%sql toy &lt;&lt; SELECT * FROM strom_minute WHERE year(minute) &gt;= 2022 AND month(minute) &gt; 11;\n%%sql\ntoy &lt;&lt; SELECT *, ‘H’||hour(minute) AS hour FROM strom_minute WHERE minute &lt;= ‘2021-05-25’ OR minute &gt;= ‘2022-11-30’ ;\npx.line(toy, x=‘minute’, y=‘cm’)\npx.histogram(toy, x=‘cm’)\n%%sql SELECT COUNT(*) FROM strom WHERE date IS NOT NULL ;\nSELECT COUNT(*) FROM strom_minute WHERE date IS NOT NULL ;\nSELECT * FROM strom_minute LIMIT 10 OFFSET 1000;\nimport pandas as pd\nminute = pd.date_range( start=min(strom_df[‘date’]), end=max(strom_df[‘date’]), freq=‘min’ ) minute_df = pd.DataFrame(dict(date = minute)) minute_df = minute_df.merge(strom_df, on=‘date’, how=‘left’) minute_df[‘day’] = minute_df[‘date’].dt.date minute_df[‘hour’] = minute_df[‘date’].dt.hour minute_df[‘minute’] = minute_df[‘date’].dt.minute\nhour_df = minute_df.groupby([‘day’, ‘hour’]).agg({‘value’: [‘max’]})\nhour_df = minute_df.groupby([‘day’, ‘hour’]).agg({‘value’: [‘max’], ‘minutes’: ‘sum’}) hour_df = minute_df.groupby([‘day’, ‘hour’]).agg({‘value’: [‘max’], ‘minutes’: ‘sum’})\nfig = px.scatter(hour_df, x=‘index’, y=‘consumption_per_day’) fig.show()\nhttps://www.rstudio.com/blog/6-productivity-hacks-for-quarto/#write-verbatim-code-chunks-with-echo-fenced\nhttps://quarto.org/docs/computations/execution-options.html",
    "crumbs": [
      "Dashboard",
      "Process",
      "Strom"
    ]
  },
  {
    "objectID": "Process/03_dwd_data.html",
    "href": "Process/03_dwd_data.html",
    "title": "DWD Daten",
    "section": "",
    "text": "Deutscher Wetterdienst\nhttps://www.dwd.de/DE/leistungen/cdc/cdc_ueberblick-klimadaten.html\n\n\nstundliche Auflösung Wetter Stationen\nladen wir die Stationslisten herunter mit allen Klimastationen, die Lufttemperatur - stündliche Auflösung haben.\n\nimport epyfun\nstations_file = \"interim/climate/latest/TU_Stundenwerte_Beschreibung_Stationen.txt\"\n\nepyfun.web.download_file(\"https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/air_temperature/historical/TU_Stundenwerte_Beschreibung_Stationen.txt\", stations_file)\n\nepyfun.fs.convert_to_utf8(stations_file)\n\n'ISO-8859-1'\n\n\nZeige die Stationen kurz auf einer Karte an, wobei sich Brannenburg in der Mitte befindet, so dass es leicht ersichtlich ist, welche Stationen näher liegen.\n\nimport pandas as pd\n\ndf = pd.read_fwf(\n    stations_file,\n    colspecs=\"infer\",\n    skiprows=3,\n    names=[\n        \"Stations_id\",\n        \"von_datum\",\n        \"bis_datum\",\n        \"Stationshoehe\",\n        \"geoBreite\",\n        \"geoLaenge\",\n        \"Stationsname\",\n        \"Bundesland\",\n    ],\n)\n\n\nimport folium\nfrom folium.plugins import MarkerCluster\nimport pandas as pd\n\nbrannenburg_coords = [47.7424, 12.1041]  # source wikipedia\nmy_map = folium.Map(location=brannenburg_coords, zoom_start=10)\n\nfor index, row in df.iterrows():\n    folium.Marker(\n        [row[\"geoBreite\"], row[\"geoLaenge\"]],\n        popup=f\"\"\"\n            {row[\"Stationsname\"]} ({str(row[\"Stations_id\"])})\n            &lt;br/&gt;\n            {str(row[\"Stationshoehe\"])} m2\n            &lt;br/&gt;\n            Von: {str(row[\"von_datum\"])}\n            &lt;br/&gt;\n            Bis: {str(row[\"bis_datum\"])}\n        \"\"\",\n    ).add_to(my_map)\n\nmy_map\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nAlso, wir sollten entweder Kiefersfelden-Gach (3679) oder Rosenheim (4261) verwenden. Wendelstein (5467) ist ebenfalls näher, aber höher (832 m²). Daher wäre es möglicherweise besser, Kiefersfelden oder Rosenheim zu wählen, da deren Höhen (518 m² bzw. 442 m²) viel ähnlicher sind als die Höhe von Brannenburg (509 m² laut Wikipedia).\nDann schreiben wir eine kleine Funktion, die nach Dateien für solche Stationen sucht und sie herunterlädt. (Eine Funktion, da wir dies mindestens zweimal machen müssen, weil es Dateien mit historischen Daten und Dateien mit aktuellen Daten gibt.)\n\n\nfns to support download data\n\nimport requests\nfrom bs4 import BeautifulSoup\n\n\ndef download_climate_data(listing_url):\n    # Fetch HTML content\n    response = requests.get(listing_url)\n    html_content = response.text\n\n    # Parse HTML content\n    soup = BeautifulSoup(html_content, \"html.parser\")\n\n    # Extract links that contains the stations ids, with the codes\n    # for the stations of kiefersfelden or rosenheim\n    target_links = [\n        a[\"href\"]\n        for a in soup.find_all(\"a\", href=True)\n        if \"_03679_\" in a[\"href\"] or \"_04261_\" in a[\"href\"]\n    ]\n\n    downloaded_files = []\n    for link in target_links:\n        print(\"Downloading: \", link)\n        epyfun.web.download_file(\n            listing_url + \"/\" + link, \"interim/climate/latest/\" + link\n        )\n        downloaded_files.append(\"interim/climate/latest/\" + link)\n\n    return downloaded_files\n\n\nimport pandas as pd\nimport zipfile\nimport io\n\ndef read_csv_from_zip(zip_file_path):\n    with zipfile.ZipFile(zip_file_path, 'r') as zip_file:\n        # Get the list of file names in the zip file\n        file_names = zip_file.namelist()\n\n        # Filter the file names that start with the specified prefix\n        target_files = [file_name for file_name in file_names if file_name.startswith(\"produkt\")]\n\n        print(target_files)\n        if not target_files:\n            raise Exception(\"No files start with produkt\")\n        if len(target_files) &gt; 1:\n            raise Exception(\"There are more files that start with produkt. Double check\")\n        \n        target_file_name = target_files[0]\n\n        # Read the CSV file directly from the zip archive using pandas\n        with zip_file.open(target_file_name) as file_in_zip:\n            df = pd.read_csv(io.TextIOWrapper(file_in_zip), sep=';')  # Adjust the separator based on the file format\n\n        return df\n\n\ndef bind_rows_files(all_files):\n    # all_files = [\"interim/climate/latest/stundenwerte_TU_04261_akt.zip\",\n    # \"./interim/climate/latest/stundenwerte_TU_04261_20060301_20221231_hist.zip\"]\n    #read_csv_from_zip(\"interim/climate/latest/stundenwerte_TU_04261_akt.zip\")\n    all_dfs = [read_csv_from_zip(file) for file in all_files]\n    result_df = pd.concat(all_dfs, ignore_index=True)\n    result_df['MESS_DATUM'] = pd.to_datetime(result_df['MESS_DATUM'], format='%Y%m%d%H')\n\n    return result_df\n\n\n\nLufttemperatur - stündliche Auflösung\n\nhistorical_files = download_climate_data(\"https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/air_temperature/historical/\")\n\nrecent_files = download_climate_data(\"https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/air_temperature/recent/\")\n\nair = bind_rows_files(historical_files + recent_files)\naircols = [\"TT_TU\", \"RF_TU\"]\n\nDownloading:  stundenwerte_TU_03679_20061101_20231231_hist.zip\nDownloading:  stundenwerte_TU_04261_20060301_20231231_hist.zip\nDownloading:  stundenwerte_TU_03679_akt.zip\nDownloading:  stundenwerte_TU_04261_akt.zip\n['produkt_tu_stunde_20061101_20231231_03679.txt']\n['produkt_tu_stunde_20060301_20231231_04261.txt']\n['produkt_tu_stunde_20221125_20240527_03679.txt']\n['produkt_tu_stunde_20221125_20240412_04261.txt']\n\n\nEs gibt komische Werte hier. Es scheint, dass -999 als Missing Value-Indikator verwendet wird (für RF_TU, relative Feuchte).\n\nimport numpy as np\nair.replace(-999, np.nan, inplace=True)\n\n\nimport plotly.express as px\n\n# Filter rows where the date is greater than 2020\nfiltered_df = air[air['MESS_DATUM'] &gt; '2020-01-01']\n\nfig = px.line(\n    filtered_df,\n    x=\"MESS_DATUM\",\n    y=aircols,\n    labels={\"value\": \"Values\"},\n    line_shape=\"linear\"\n)\nfig.show()\n\nepyfun.splom(filtered_df, aircols + [\"MESS_DATUM\"]).show()\n\n                                                \n\n\n                                                \n\n\n\n\nTaupunkttemperatur - stündliche Auflösung\n\nhistorical_files = download_climate_data(\"https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/dew_point/historical/\")\n\nrecent_files = download_climate_data(\"https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/dew_point/recent/\")\n\ndew = bind_rows_files(historical_files + recent_files)\ndewcols = [\"  TT\", \"  TD\"]\n\nDownloading:  stundenwerte_TD_03679_20061101_20231231_hist.zip\nDownloading:  stundenwerte_TD_04261_20060301_20231231_hist.zip\nDownloading:  stundenwerte_TD_03679_akt.zip\nDownloading:  stundenwerte_TD_04261_akt.zip\n['produkt_td_stunde_20061101_20231231_03679.txt']\n['produkt_td_stunde_20060301_20231231_04261.txt']\n['produkt_td_stunde_20221125_20240527_03679.txt']\n['produkt_td_stunde_20221125_20240412_04261.txt']\n\n\nEs gibt komische Werte hier. Es scheint, dass -999 als Missing Value-Indikator verwendet wird (für RF_TU, relative Feuchte).\n\nimport numpy as np\ndew.replace(-999, np.nan, inplace=True)\n\n\nimport plotly.express as px\n\n# Filter rows where the date is greater than 2020\nfiltered_df = dew[dew['MESS_DATUM'] &gt; '2020-01-01']\n\nfig = px.line(\n    filtered_df,\n    x=\"MESS_DATUM\",\n    y=dewcols,\n    labels={\"value\": \"Values\"},\n    line_shape=\"linear\"\n)\nfig.show()\n\nepyfun.splom(filtered_df, dewcols + [\"MESS_DATUM\"]).show()\n\n                                                \n\n\n                                                \n\n\n\n\nMoisture - stündliche Auflösung\n\nhistorical_files = download_climate_data(\"https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/moisture/historical/\")\n\nrecent_files = download_climate_data(\"https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/moisture/recent/\")\n\nmoist = bind_rows_files(historical_files + recent_files)\nmoistcols = ['VP_STD', 'TF_STD', 'P_STD', 'TT_STD', 'RF_STD', 'TD_STD']\n\nDownloading:  stundenwerte_TF_03679_20061101_20231231_hist.zip\nDownloading:  stundenwerte_TF_04261_20060301_20231231_hist.zip\nDownloading:  stundenwerte_TF_03679_akt.zip\nDownloading:  stundenwerte_TF_04261_akt.zip\n['produkt_tf_stunde_20061101_20231231_03679.txt']\n['produkt_tf_stunde_20060301_20231231_04261.txt']\n['produkt_tf_stunde_20221125_20240527_03679.txt']\n['produkt_tf_stunde_20221125_20240412_04261.txt']\n\n\n\nimport plotly.express as px\n\n# Filter rows where the date is greater than 2020\nfiltered_df = moist[moist['MESS_DATUM'] &gt; '2020-01-01']\n\nfig = px.line(\n    filtered_df,\n    x=\"MESS_DATUM\",\n    y=moistcols,\n    labels={\"value\": \"Values\"},\n    line_shape=\"linear\"\n)\nfig.show()\n\nepyfun.splom(filtered_df, moistcols + [\"MESS_DATUM\"]).show()\n\n                                                \n\n\n                                                \n\n\n\n\nPrecipitaci - stündliche Auflösung\n\nhistorical_files = download_climate_data(\"https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/precipitation/historical/\")\n\nrecent_files = download_climate_data(\"https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/precipitation/recent/\")\n\nprecip = bind_rows_files(historical_files + recent_files)\nprecipcols = ['  R1', 'RS_IND', 'WRTR']\n\nDownloading:  stundenwerte_RR_03679_20061101_20231231_hist.zip\nDownloading:  stundenwerte_RR_04261_20060301_20231231_hist.zip\nDownloading:  stundenwerte_RR_03679_akt.zip\nDownloading:  stundenwerte_RR_04261_akt.zip\n['produkt_rr_stunde_20061101_20231231_03679.txt']\n['produkt_rr_stunde_20060301_20231231_04261.txt']\n['produkt_rr_stunde_20221125_20240527_03679.txt']\n['produkt_rr_stunde_20221125_20240413_04261.txt']\n\n\nEs gibt komische Werte hier. Es scheint, dass -999 als Missing Value-Indikator verwendet wird.\n\nimport numpy as np\nprecip.replace(-999, np.nan, inplace=True)\n\n\nimport plotly.express as px\n\n# Filter rows where the date is greater than 2020\nfiltered_df = precip[precip['MESS_DATUM'] &gt; '2020-01-01']\n\nfig = px.line(\n    filtered_df,\n    x=\"MESS_DATUM\",\n    y=precipcols,\n    labels={\"value\": \"Values\"},\n    line_shape=\"linear\"\n)\nfig.show()\n\nepyfun.splom(filtered_df, precipcols + [\"MESS_DATUM\"]).show()\n\n                                                \n\n\n                                                \n\n\n\n\njoin in a single table and mach es auf täglicheauslösung\n\noncols = [\"MESS_DATUM\", \"STATIONS_ID\"]\nagg_df = air.merge(dew[oncols + dewcols], on=oncols)\nagg_df = agg_df.merge(moist[oncols + moistcols], on=oncols)\nagg_df = agg_df.merge(precip[oncols + precipcols], on=oncols)\n\n\ndatacols = aircols + dewcols + moistcols + precipcols\nagg_df[\"date\"] = agg_df[\"MESS_DATUM\"].dt.normalize()\nagg_df = (\n    agg_df.groupby(\"date\")\n    .agg({key: [\"min\", \"mean\", \"max\"] for key in datacols})\n    .reset_index()\n)\nagg_df = epyfun.pandas.clean_names(agg_df)\n\n\n#epyfun.splom(agg_df).show(\"browser\")\n\n\n\nmerge everything and save the file\n\n# agg_df.to_parquet('./interim/climate/climate_daily.parquet', index=False)",
    "crumbs": [
      "Dashboard",
      "Process",
      "DWD Daten"
    ]
  },
  {
    "objectID": "Setup/01_quarto.html",
    "href": "Setup/01_quarto.html",
    "title": "A. Quarto Web Site",
    "section": "",
    "text": "This is a Quarto website that will contain all the strom project.\nIt uses “Auto Generation” to populate the contents of the web site, so al lthe qmd files will be included. You just need to arrange them in folders so they will be arranged in the navigation accordingly.\nThe website is a sub-directory of the main project. We want everything relative to the main project. Therefore, when rendering/previewing the website, you should set the working directory. For example, using Quarto’s CLI:\nquarto preview /OneDrive/All/R/strom/quarto --execute-dir /OneDrive/All/R/strom\nor, relative to the current directory in the shell, simply:\nquarto render 'quarto' --execute-dir '.'\nOtherwise, Quarto would use the website sub-directory as the working directory to run the code. And we do not want that!\nHere’s a workaround that seems to work to control the order of execution of the different files in the project https://github.com/quarto-dev/quarto-cli/discussions/6944\nquarto render ‘quarto’ –execute-dir ‘.’ ; ..html",
    "crumbs": [
      "Dashboard",
      "Setup",
      "A. Quarto Web Site"
    ]
  },
  {
    "objectID": "06_crossvalidation.html",
    "href": "06_crossvalidation.html",
    "title": "Cross-validation",
    "section": "",
    "text": "Let’s run the models using a scikit-learn pipeline, so we can bundle variable selection, transformations and model estimation in one pipeline, that will be stored in a pin as a vetiver model",
    "crumbs": [
      "Dashboard",
      "Cross-validation"
    ]
  },
  {
    "objectID": "06_crossvalidation.html#plotting-cross-validated-predictions",
    "href": "06_crossvalidation.html#plotting-cross-validated-predictions",
    "title": "Cross-validation",
    "section": "Plotting Cross-Validated predictions",
    "text": "Plotting Cross-Validated predictions\nhttps://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_predict.html#sphx-glr-auto-examples-model-selection-plot-cv-predict-py\nbasically use cross_val_predict to lump together the predictions of all different models fitted in for the cross-validation",
    "crumbs": [
      "Dashboard",
      "Cross-validation"
    ]
  },
  {
    "objectID": "06_crossvalidation.html#to-dos",
    "href": "06_crossvalidation.html#to-dos",
    "title": "Cross-validation",
    "section": "to-dos",
    "text": "to-dos\n\nIt needs a polinomial with the temperature (second order perhaps)\ncheck if the association with relative humidity, if it is just an artifact of the correlation with temperature, or if there might something meaningful there going on\nand bring other climatic data, rainfall and snowfall might be relevant\ndepite the much better fit, there are still some -although minor- heteroscedaticity, mostly associated to the lowest temperatures where the variance of strom use is higher\nthere might be also a seasonal pattern, because we are not accounting for, for example, weekends and so on\nfinally, there is still some evidence of autocorrelation. Which kinda makes sense: it is not the same to have one day with a very low temperature compared to a whole week with very low temperatures: while the former could be wintered with the heat already produced, as long as there is not much energy lose, the latter would certainly need much more work from the wärmepumpe to keep fighting the cold. In that sense, it seems unrealistic to just assume that the increase energy consumption of several days of extreme cold is just the sum of the daily increases. So if the one-day-increase due to extreme cold is 10, five consecutve days would probably result in an increase greater than 10*5=50, probably much greater. But let’s explore such hipothesis in the data and try to accomodate that in the model using lags, or moving average of the last few days.\ninfluence plot and leverage plots should be interactive with tooltips with all the data associated to the points. Cook’s distance perhaps as well.\nrun some PCA, with min, max mean and so on and think about a Cumulative Explained Variance plot https://archive.is/X1wrZ#selection-1059.8-1059.37\nand use cross validation\nperhaps experiment with feature selection\nand consider log-transforming the target, after all, the distribution of it is right-skewed with relatively long right tail",
    "crumbs": [
      "Dashboard",
      "Cross-validation"
    ]
  },
  {
    "objectID": "07_playground.html",
    "href": "07_playground.html",
    "title": "playground",
    "section": "",
    "text": "So let’s start from the pipeline with the best model cross-validated before\nI am new!\n\n\ncheck coeficient variability / stability using cross validation",
    "crumbs": [
      "Dashboard",
      "playground"
    ]
  },
  {
    "objectID": "07_playground.html#coefficients",
    "href": "07_playground.html#coefficients",
    "title": "playground",
    "section": "",
    "text": "check coeficient variability / stability using cross validation",
    "crumbs": [
      "Dashboard",
      "playground"
    ]
  },
  {
    "objectID": "07_playground.html#coefficients-1",
    "href": "07_playground.html#coefficients-1",
    "title": "playground",
    "section": "coefficients",
    "text": "coefficients\ncheck coeficient variability / stability using cross validation",
    "crumbs": [
      "Dashboard",
      "playground"
    ]
  },
  {
    "objectID": "01_strom_results.html",
    "href": "01_strom_results.html",
    "title": "Strom",
    "section": "",
    "text": "Stromverbrauch im Durchschnitt\n\n\n\n\n\n\n\n\n\n\nMin\nMax\nUse\nFirst\nLast\nMins\nUse/Day\nUse/Year\nYearly Exp\n\n\n\n\n0\n7\n23127\n23120\n2016-11-28 16:16:00\n2024-05-27 09:15:00\n3940859\n8.448107\n3083.559194\n1200.73795\n\n\n\n\n\n\n\n\n\n\nStrom consumption per day\n\n\n                                                \n\n\nSo, an normalen Tagen beträgt der Stromverbrauch etwa 10-11 kWh. An Urlaubstagen liegt er bei 2,85 kWh. Also, man kann sagen, es sind 3 kWh. Das entspricht wahrscheinlich hauptsächlich dem Stromverbrauch des Kühlschranks, …, und was noch? Keine Ahnung. Aber gut, 10 - 11 kWh bedeuten bei aktuellen 0,38 Cent pro kWh, dass der Stromverbrauch etwa 4 Euro pro Tag beträgt. Im Urlaub, bei 2,85-3 kWh zu 38 Cent pro kWh, sind es ungefähr 1 Euro pro Tag. Das heißt, 10 Tage Urlaub sparen etwa 30 Euro (3 Euro pro Tag).\n\n\n\n\n\n\n\n\n\n\nmeterid\ndate\nvalue\nfirst\nminutes\nconsumption\ncm\n\n\n\n\n0\n1\n2024-05-27 09:15:00\n170\n0\n163\n1.0\n0.006135\n\n\n1\n1\n2024-05-27 06:32:00\n169\n0\n395\n0.0\n0.000000\n\n\n2\n1\n2024-05-26 23:57:00\n169\n0\n272\n3.0\n0.011029\n\n\n3\n1\n2024-05-26 19:25:00\n166\n0\n587\n7.0\n0.011925\n\n\n4\n1\n2024-05-26 09:38:00\n159\n0\n120\n1.0\n0.008333\n\n\n5\n1\n2024-05-26 07:38:00\n158\n0\n945\n5.0\n0.005291\n\n\n6\n1\n2024-05-25 15:53:00\n153\n0\n1109\n9.0\n0.008115\n\n\n7\n1\n2024-05-24 21:24:00\n144\n0\n863\n9.0\n0.010429\n\n\n8\n1\n2024-05-24 07:01:00\n135\n0\n638\n1.0\n0.001567\n\n\n9\n1\n2024-05-23 20:23:00\n134\n0\n1468\n9.0\n0.006131\n\n\n\n\n\n\n\n\n\n\nConsumption per week",
    "crumbs": [
      "Dashboard",
      "Strom"
    ]
  },
  {
    "objectID": "02_waermestrom_results.html",
    "href": "02_waermestrom_results.html",
    "title": "Wärmestrom",
    "section": "",
    "text": "Wärmestrom average consumption\n\n\n\n\n\n\n\n\n\n\nmin(\"value\")\nmax(\"value\")\ncmtot\nmin(date)\nmax(date)\nmintot\navgcm\navgcy\navgyearexpend\n\n\n\n\n0\n1031\n34849\n33818\n2016-11-28 00:21:00\n2024-05-27 09:16:00\n3941815\n12.354187\n4509.278289\n1117.850088\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntarif\nmin(\"value\")\nmax(\"value\")\ncmtot\nfrac\nmin(date)\nmax(date)\nmintot\navgcm\navgcy\navgyearexpend\n\n\n\n\n0\nvalue\n1031\n34849\n33818\n1.000000\n2016-11-28 00:21:00\n2024-05-27 09:16:00\n3941815\n12.354187\n4509.278289\n1117.850088\n\n\n1\nvalue_hoch\n547\n16616\n16069\n0.475161\n2016-11-28 00:21:00\n2024-05-27 09:16:00\n3941815\n5.870230\n2142.633888\n531.158941\n\n\n2\nvalue_niedrig\n1\n18233\n18232\n0.539121\n2016-11-28 00:21:00\n2024-05-27 09:16:00\n3941815\n6.660404\n2431.047424\n602.656656\n\n\n\n\n\n\n\n\n\n\nWärmestrom consumption per day\n\n\n                                                \n\n\nWärmestrom zeigt natürlich das saisonale Muster. Es gibt einen hohen und volatilen Verbrauch in der Heizperiode und einen niedrigeren, weniger volatilen Verbrauch in den wärmeren Monaten. Offensichtlich beträgt der normale Verbrauch ohne Heizung etwa 3-4 kWh. Das ist wahrscheinlich nur für Warmwasser. Der Verbrauch an Urlaubstagen beträgt 2,69 kWh. Ist das einfach, um das Wasser warm zu halten?\nDer Verbrauch in der Heizperiode liegt zwischen 12 und bis zu 50 kWh. Vermutlich stark abhängig von der Außentemperatur.\nOverlay temperature Was ist am 25-26 Juli passiert?, warum im sommer solcher hoher verbreauch?\n\n\nHoch und Niedrig tarif\nPreis unterschied zwischen Hoch- und Niedrigtarif ist nicht so groß. 1 o 2 cent. Zub Beispiel, für 2024, es wird 27,63 fürs Hochtarif und 26,29 fürs Niedrigtarif.\nat durchschnitliche Verbrauch von 12 Kwh pro Tag, das Kostet 3.31 Euro in der Hochtarif und 3.15 Euro in der Niedrigtarif. Die Jährliche durschschnit Verbrauch von 4,400 Kwh heist 1,215 Euro vs. 1157 Euro pro jahr. ALso, wenig al 60 Euro Unterschied im Jahr, wenn alle Verbrauch in einen Tarif wäre.\nAlso, es ist nich viel relevant.\nTrotzdem, schau ma moi wir war der verlauf von denen. But let’s aggregate at weekly level; the daily plot just basically shows that the weekends its only niedrig.",
    "crumbs": [
      "Dashboard",
      "Wärmestrom"
    ]
  },
  {
    "objectID": "03_periods.html",
    "href": "03_periods.html",
    "title": "Periods",
    "section": "",
    "text": "Success\n\n\n\n\n\n\n\n\n\n\n\n\n                                                \n\n\n\n\n                                                \n\n\n\n\n\n\n\n\n\n\n\n\nname\nbegin\nfin\ncd\n\n\n\n\n0\n20 grad\n2023-10-27 22:49:00\n2023-10-30 22:49:00\n8.300808\n\n\n1\n18 grad\n2023-10-24 22:49:00\n2023-10-27 22:49:00\n6.151376\n\n\n\n\n\n\n\n\nwe have to brind the temp data and other climatic variables here",
    "crumbs": [
      "Dashboard",
      "Periods"
    ]
  }
]